■ 밑바닥부터 시작하는 딥러닝 책 목차

	1장. 이 책을 보기 위한 파이썬 기본문법


	2장. 퍼셉트론


	3장. 저자가 만들어온 가중치를 이용해서 3층 신경망 구성


	4장. 수치미분을 이용한 2층 신경망 구현


	5장. 오차역전파를 이용한 2층 신경망 구현
		1. 계산 그래프가 왜 필요한지 ?
		2. 덧셈, 곱셈 계산 그래프
		3. Relu, Sigmoid, Affine계층의 계산 그래프

	   "행렬은 여러개의 연산을 한번에 표기하고 처리할 수 있는 수학적 툴"
	   행렬의 각 요소가 수행하는 반복적인 계산을하나의 표기법으로 처리하고 연산할 수 있는 방식을
	   제공하는 것이 선형대수의 역할이다.
	
	6장. 신경망을 학습을 높이기 위한 방법들
		- 가중치 초기화
		- 여러가지 경사감소법
		- 배치 정규화


	7장. CNN을 이용한 3층 신경망 구현







★ 입력값을 1차원으로 하면 안되고 2차원으로 하는 이유는 ?

	import numpy as np
	
	x1 = np.array([[1, 2]])
	print(x1.ndim) # 2
	
	x2 = np.array([1, 2])
	print(x2.ndim) # 1


	1차원
	import numpy as np
	x = np.array([1, 2])
	w = np.array([[1, 3, 5], [2, 4, 6]])
	print(np.dot(x,w))
	#결과값이 1차원이 나옴


	#선생님이 준 코드 (돌려봐라)
	
	import numpy as np
	
	class Affine:
	    def __init__(self, w, b):
	        self.w = w
	        self.b = b
	        self.x = None
	        self.dw = None
	        self.db = None
	
	    def forward(self, x):
	        self.x = x
	        out = np.dot(x, self.w) + self.b
	        return out
	
	    def backward(self, dout):
	        dx = np.dot(dout, self.w.T)
	        self.dw = np.dot(self.x.T, dout)
	        self.db = np.sum(dout, axis=0)
	        return dx
	
	
	# x = np.array([1, 2])   # 오류남 
	x = np.array([[1, 2]])
	w = np.array([[1, 3, 5], [2, 4, 6]])
	b = np.array([[1, 1, 1]])
	dout = np.array([[1, 2, 3]])
	
	affine = Affine(w, b)
	print("forward : ", affine.forward(x))
	print("dx : ", affine.backward(dout))
	print("dw : ", affine.dw)
	print("db : ", affine.db)
	#
	# x = np.array([[1,2]])
	# x2 = np.array([1,2])
	# print (x.shape)  (1,2)
	# print (x2.shape) (2,)




문제 179. 위에서 만든 SoftmaxWithLoss클래스를 객체화 시켜서 아래의 x (입력값), t(target value)를 
	  입력해서 순전파의 오차율을 출력하고 역전파도 출력하시오 !

	t = np.array([0,0,1,0,0,0,0,0,0,0])   # 숫자2
	
	x1 = np.array([0.01,0.01,0.01,0.01,0.01,0.01,0.05,0.3,0.1,0.5])
	x2 = np.array([0.01,0.01,0.9,0.01,0.01,0.01,0.01,0.01,0.01,0.02])
	
	# 클래스 객체화
	swl1 = SoftmaxWithLoss()
	swl2 = SoftmaxWithLoss()
	
	#순전파
	out1 = swl1.forward(x1, t)
	out2 = swl2.forward(x2, t)
	print(out1)
	print(out2)
	
	2.40727986639
	1.5475681948
	
	
	#역전파
	dx1 = swl1.backward()
	dx2 = swl2.backward()
	print(dx1)
	print(dx2)
	
	[ 0.00900598  0.00900598 -0.09099402  0.00900598  0.00900598  0.00900598
	  0.00937352  0.01203584  0.00985412  0.01470061]
	[ 0.0087373   0.0087373  -0.07872354  0.0087373   0.0087373   0.0087373
	  0.0087373   0.0087373   0.0087373   0.00882511]




문제 180. 아래의 x값 9개를 t과 비교해서 오차율이 아래와 같이 출력되게 하시오 ! 
	  ( SoftmaxWithLoss클래스를 이용해서)

	t = [0,0,1,0,0,0,0,0,0,0]    # 숫자2 
	
	x1 = [0.1,0.05,0.1,0.0,0.05,0.1,0.0,0.1,0.0,0.0]  
	x2 = [0.1,0.05,0.2,0.0,0.05,0.1,0.0,0.6,0.0,0.0] 
	x3 = [0.0,0.05,0.3,0.0,0.05,0.1,0.0,0.6,0.0,0.0] 
	x4 = [0.0,0.05,0.4,0.0,0.05,0.0,0.0,0.5,0.0,0.0] 
	x5 = [0.0,0.05,0.5,0.0,0.05,0.0,0.0,0.4,0.0,0.0] 
	x6 = [0.0,0.05,0.6,0.0,0.05,0.0,0.0,0.3,0.0,0.0] 
	x7 = [0.0,0.05,0.7,0.0,0.05,0.0,0.0,0.2,0.0,0.0] 
	x8 = [0.0,0.1,0.8,0.0,0.1,0.0,0.0,0.2,0.0,0.0] 
	x9 = [0.0,0.05,0.9,0.0,0.05,0.0,0.0,0.0,0.0,0.0] 
	
	for i in range(1, 10):
	    swl = SoftmaxWithLoss()
	    print('x%d의 오차 : '%i, swl.forward(eval('x'+str(i)), t))




문제 181. 어제 마지막 문제였던 문제 229번 3층 신경망의 마지막 층으로 SoftmaxWithLoss 클래스를 추가하시오 !
	
	import numpy as np
	
	x = np.array([[1,2]])
	t = np.array([0, 0, 1])
	
	w1 = np.array([[1, 3, 5], [2, 4, 6]])
	b1 = np.array([[1, 2, 3]])
	
	w2 = np.array([[1, 4], [2, 5], [3, 6]])
	b2 = np.array([[1, 2]])
	
	w3 = np.array([[5, 6, 7], [2, 3, 4]])
	b3 = np.array([[1, 1, 1]])
	
	#각 층 객체화
	affine1 = Affine(w1, b1)
	relu1 = Relu()
	affine2 = Affine(w2, b2)
	relu2 = Relu()
	affine3 = Affine(w3, b3)
	swl = SoftmaxWithLoss()
	
	# 순전파
	out1 = affine1.forward(x)
	out1_hat = relu1.forward(out1)
	out2 = affine2.forward(out1_hat)
	out2_hat = relu2.forward(out2)
	out3 = affine3.forward(out2_hat)
	result = swl.forward(out3, t)
	print(result)

	-9.99999950584e-08




문제 182. 이번에는 문제 181번 코드의 역전파 코드를 구현하시오 ! dout = 1로 한다.

	import numpy as np
	
	x = np.array([[1,2]])
	t = np.array([0, 0, 1])
	
	w1 = np.array([[1, 3, 5], [2, 4, 6]])
	b1 = np.array([[1, 2, 3]])
	
	w2 = np.array([[1, 4], [2, 5], [3, 6]])
	b2 = np.array([[1, 2]])
	
	w3 = np.array([[5, 6, 7], [2, 3, 4]])
	b3 = np.array([[1, 1, 1]])
	
	#각 층 객체화
	affine1 = Affine(w1, b1)
	relu1 = Relu()
	affine2 = Affine(w2, b2)
	relu2 = Relu()
	affine3 = Affine(w3, b3)
	swl = SoftmaxWithLoss()
	
	# 순전파
	out1 = affine1.forward(x)
	out1_hat = relu1.forward(out1)
	out2 = affine2.forward(out1_hat)
	out2_hat = relu2.forward(out2)
	out3 = affine3.forward(out2_hat)
	result = swl.forward(out3, t)
	# print(result)
	
	# 역전파
		
	dout3 = swl.backward()
	dout2_hat = affine3.backward(dout3)
	dout2 = relu2.backward(dout2_hat)
	dout1_hat = affine2.backward(dout2)
	dout1 = relu1.backward(dout1_hat)
	dx = affine1.backward(dout1)
	dw1 = affine1.dW
	db1 = affine1.db
	print(dx)
	print(dw1)
	print(db1)
	
	[[  8.76920959e-131   1.13151091e-130]]
	[[  5.65755457e-132   8.48633186e-132   1.13151091e-131]
	 [  1.13151091e-131   1.69726637e-131   2.26302183e-131]]
	[  5.65755457e-132   8.48633186e-132   1.13151091e-131]







★ OrderDict() 딕셔너리의 이해

	orderdict는 그냥 Dictionary와는 다르게 입력된 데이터 뿐만 아니라 입력된 순서까지 같아야 동일한
	것으로 판단한다.

	예제 :
		# 일반 딕셔너리 예제

		print('dict : ')
		d1 = {}
		d1['a'], d1['b'], d1['c'], d1['d'], d1['e'] = 'A', 'B', 'C', 'D', 'E'

		
		d2 = {}
		d2['e'], d2['d'], d2['c'], d2['b'], d2['a'] = 'E', 'D', 'C', 'B', 'A'
		
		print(d1) # {'a': 'A', 'b': 'B', 'c': 'C', 'd': 'D', 'e': 'E'}
		print(d2) # {'e': 'E', 'd': 'D', 'c': 'C', 'b': 'B', 'a': 'A'}
		
		print(d1 == d2) # True
		
		

	# OrderDict() 를 테스트
	
	import collections
	
		print('dict : ')
		d1 = collections.OrderedDict()
		d1['a'], d1['b'], d1['c'], d1['d'], d1['e'] = 'A', 'B', 'C', 'D', 'E'
		
		d2 = collections.OrderedDict()
		d2['e'], d2['d'], d2['c'], d2['b'], d2['a'] = 'E', 'D', 'C', 'B', 'A'
		
		print(d1) # OrderedDict([('a', 'A'), ('b', 'B'), ('c', 'C'), ('d', 'D'), ('e', 'E')])
		print(d2) # OrderedDict([('e', 'E'), ('d', 'D'), ('c', 'C'), ('b', 'B'), ('a', 'A')])
		
		print(d1 == d2) # False

		OrderedDict함수로 만든 Dictionary는 순서까지 같아야 같은 데이터로 인식한다.
		순전파 순서의 반대로 역전파가 되어야 하기 때문에 OrderedDict함수를 사용해야 한다.

		순전파 : 입력값 --> Affine1층 --> Relu1 --> Affine2층 --> Relu --> Affine3층
			 --> Lastlayer --> 오차

		역전파 : 1 --> Lastlayer --> Affine3층 --> Relu --> Affine2층 --> Relu --> Affine1층

		※ 순전파의 순서를 반대로(reverse)해서 역전파 될 수 있도록 OrderedDict함수를 신경망
		   코드에 사용해야 한다.




문제 183. OrderedDict함수를 이용해서 answp 182번 코드의 순전파 코드를 구현하시오 ! (참고 p182)

	import numpy as np
	import collections
	
	x = np.array([[1,2]])
	t = np.array([[0, 0, 1]])
	
	w1 = np.array([[1, 3, 5], [2, 4, 6]])
	b1 = np.array([[1, 2, 3]])
	
	w2 = np.array([[1, 4], [2, 5], [3, 6]])
	b2 = np.array([[1, 2]])
	
	w3 = np.array([[5, 6, 7], [2, 3, 4]])
	b3 = np.array([[1, 1, 1]])
	
	#각 층 객체화
	layers = collections.OrderedDict()
	layers['affine1'] = Affine(w1, b1)
	layers['relu1'] = Relu()
	layers['affine2'] = Affine(w2, b2)
	layers['relu2'] = Relu()
	layers['affine3'] = Affine(w3, b3)
	
	
	for layer in layers.values():
	    x = layer.forward(x)
	
	    
	swl = SoftmaxWithLoss()
	
	print(swl.forward(x, t))
	
	-9.99999950584e-08




문제 184. 위의 3층 신경망의 역전파 코드를 구현하시오 !

	import numpy as np
	import collections
	
	x = np.array([[1,2]])
	t = np.array([[0, 0, 1]])
	
	w1 = np.array([[1, 3, 5], [2, 4, 6]])
	b1 = np.array([[1, 2, 3]])
	
	w2 = np.array([[1, 4], [2, 5], [3, 6]])
	b2 = np.array([[1, 2]])
	
	w3 = np.array([[5, 6, 7], [2, 3, 4]])
	b3 = np.array([[1, 1, 1]])
	
	#각 층 객체화
	layers = collections.OrderedDict()
	layers['affine1'] = Affine(w1, b1)
	layers['relu1'] = Relu()
	layers['affine2'] = Affine(w2, b2)
	layers['relu2'] = Relu()
	layers['affine3'] = Affine(w3, b3)
	
	# 순전파
	for layer in layers.values():
	    x = layer.forward(x)
	
	swl = SoftmaxWithLoss()
	out = swl.forward(x, t)
	
	# 역전파
	dout = 1
	dout = swl.backward(dout)
	
	layer = list(layers.values())
	layer.reverse()
	for i in layer:
	    dout = i.backward(dout)
	
	print(dout)
	print(layers['affine1'].dW)
	print(layers['affine1'].db)
	
	[[  2.63076288e-130   3.39453274e-130]]
	[[  1.69726637e-131   2.54589956e-131   3.39453274e-131]
	 [  3.39453274e-131   5.09179911e-131   6.78906549e-131]]
	[  1.69726637e-131   2.54589956e-131   3.39453274e-131]





★ 오차 역전파를 이용한 2층 신경망 ( p 182 ~ p 183 )의 코드


======================================================================================
저자코드
--------------------------------------------------------------------------------------
# coding: utf-8

import sys, os
sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정
import numpy as np
from common.layers import *
from common.gradient import numerical_gradient
from collections import OrderedDict
import matplotlib.pyplot as plt
from dataset.mnist import load_mnist


class TwoLayerNet:
    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):

        # 가중치 초기화
        self.params = {}
        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)
        self.params['b1'] = np.zeros(hidden_size)
        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)
        self.params['b2'] = np.zeros(output_size)


        # 계층 생성

        self.layers = OrderedDict()
        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])
#         self.layers['sigmoid'] = Sigmoid()
        self.layers['Relu'] = Relu()
        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])
        self.lastLayer = SoftmaxWithLoss()


    def predict(self, x):
        for layer in self.layers.values():
            x = layer.forward(x)

        return x


    # x : 입력 데이터, t : 정답 레이블

    def loss(self, x, t):
        y = self.predict(x)
        return self.lastLayer.forward(y, t)


    def accuracy(self, x, t):
        y = self.predict(x)
        y = np.argmax(y, axis=1)
        if t.ndim != 1: t = np.argmax(t, axis=1)
        accuracy = np.sum(y == t) / float(x.shape[0])
        return accuracy


    # x : 입력 데이터, t : 정답 레이블

    def numerical_gradient(self, x, t):
        loss_W = lambda W: self.loss(x, t)
        grads = {}
        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])
        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])
        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])
        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])
        return grads


    def gradient(self, x, t):
        # forward
        self.loss(x, t)
        # backward
        dout = 1
        dout = self.lastLayer.backward(dout)
        layers = list(self.layers.values())
        layers.reverse()

        for layer in layers:
            dout = layer.backward(dout)


        # 결과 저장

        grads = {}
        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db
        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db

        return grads

# 데이터 읽기

(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)

network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)


# 하이퍼파라미터

iters_num = 10000  # 반복 횟수를 적절히 설정한다.
train_size = x_train.shape[0] # 60000 개
batch_size = 100  # 미니배치 크기
learning_rate = 0.1

train_loss_list = []
train_acc_list = []
test_acc_list = []



# 1에폭당 반복 수

iter_per_epoch = max(train_size / batch_size, 1)



for i in range(iters_num): # 10000

    # 미니배치 획득  # 랜덤으로 100개씩 뽑아서 10000번을 수행하니까 백만번
    batch_mask = np.random.choice(train_size, batch_size) # 100개 씩 뽑아서 10000번 백만번
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]

    # 기울기 계산

    #grad = network.numerical_gradient(x_batch, t_batch)
    grad = network.gradient(x_batch, t_batch)

    # 매개변수 갱신

    for key in ('W1', 'b1', 'W2', 'b2'):
        network.params[key] -= learning_rate * grad[key]

    # 학습 경과 기록

    loss = network.loss(x_batch, t_batch)
    train_loss_list.append(loss) # cost 가 점점 줄어드는것을 보려고

    # 1에폭당 정확도 계산 # 여기는 훈련이 아니라 1에폭 되었을때 정확도만 체크

    if i % iter_per_epoch == 0: # 600 번마다 정확도 쌓는다.

        train_acc = network.accuracy(x_train, t_train)

        test_acc = network.accuracy(x_test, t_test)

        train_acc_list.append(train_acc) # 10000/600 개  16개 # 정확도가 점점 올라감

        test_acc_list.append(test_acc)  # 10000/600 개 16개 # 정확도가 점점 올라감

        print("train acc, test acc | " + str(train_acc) + ", " + str(test_acc))



# 그래프 그리기

markers = {'train': 'o', 'test': 's'}
x = np.arange(len(train_acc_list))
plt.plot(x, train_acc_list, label='train acc')
plt.plot(x, test_acc_list, label='test acc', linestyle='--')
plt.xlabel("epochs")
plt.ylabel("accuracy")
plt.ylim(0, 1.0)
plt.legend(loc='lower right')
plt.show()
======================================================================================




문제 185. 위의 2층 신경망을 활성화 함수로 relu를 사용하고 있고 최종 정확도는 아래와 같다.
	  그런데 현업에서는 sigmoid함수보다 relu함수를 더 선호한다고 하는데 그 이유를 테스트로 확인해보시오 !

	1. Relu의 최종 정확도
	 train acc, test acc | 0.976566666667, 0.9693
	
	2. Sigmoid의 최종 정확도
	 train acc, test acc | 0.947233333333, 0.9474




문제 186. (오늘의 마지막 문제)
	
	2층 신경망 vs 3층 신경망의 정확도 차이는 발생하는가?
	층을 많이 쌓으면 쌓을수록 정확도가 높아지는가?
	
	2층 신경망 (Relu 사용)
	정확도 : train acc, test acc | 0.976566666667, 0.9693
	
	3층 신경망 (Relu사용)
	정확도 : train acc, test acc | 0.911116666667, 0.9123



##### 3층 신경망 코드 #####
# coding: utf-8

import sys, os
sys.path.append(os.pardir)
import numpy as np
from common.layers import *
from common.gradient import numerical_gradient
from collections import OrderedDict
import matplotlib.pyplot as plt
from dataset.mnist import load_mnist


class TwoLayerNet:
    def __init__(self, input_size, hidden_size1, hidden_size2, output_size1, output_size2, weight_init_std=0.01):
        self.params = {}
        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size1)
        self.params['b1'] = np.zeros(hidden_size1)
        self.params['W2'] = weight_init_std * np.random.randn(hidden_size1, output_size1)
        self.params['b2'] = np.zeros(output_size1)
        self.params['W3'] = weight_init_std * np.random.randn(hidden_size2, output_size2)
        self.params['b3'] = np.zeros(output_size2)
        self.layers = OrderedDict()
        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])
#         self.layers['sigmoid'] = Sigmoid()
        self.layers['Relu1'] = Relu()
        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])
        self.layers['Relu2'] = Relu()
        self.layers['Affine3'] = Affine(self.params['W3'], self.params['b3'])
        self.lastLayer = SoftmaxWithLoss()

    def predict(self, x):
        for layer in self.layers.values():
            x = layer.forward(x)
        return x

    def loss(self, x, t):
        y = self.predict(x)
        return self.lastLayer.forward(y, t)

    def accuracy(self, x, t):
        y = self.predict(x)
        y = np.argmax(y, axis=1)
        if t.ndim != 1: t = np.argmax(t, axis=1)
        accuracy = np.sum(y == t) / float(x.shape[0])
        return accuracy

    def numerical_gradient(self, x, t):
        loss_W = lambda W: self.loss(x, t)
        grads = {}
        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])
        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])
        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])
        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])
        return grads

    def gradient(self, x, t):
        # forward
        self.loss(x, t)
        # backward
        dout = 1
        dout = self.lastLayer.backward(dout)
        layers = list(self.layers.values())
        layers.reverse()
        for layer in layers:
            dout = layer.backward(dout)
        grads = {}
        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db
        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db
        return grads


# 데이터 읽기
(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)
network = TwoLayerNet(input_size=784, hidden_size1=50, hidden_size2=100, output_size1 = 100,  output_size2=10)
iters_num = 10000
train_size = x_train.shape[0] 
batch_size = 100  
learning_rate = 0.1
train_loss_list = []
train_acc_list = []
test_acc_list = []
iter_per_epoch = max(train_size / batch_size, 1)
for i in range(iters_num):
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]
    #grad = network.numerical_gradient(x_batch, t_batch)
    grad = network.gradient(x_batch, t_batch)
    for key in ('W1', 'b1', 'W2', 'b2'):
        network.params[key] -= learning_rate * grad[key]
    loss = network.loss(x_batch, t_batch)
    train_loss_list.append(loss) 
    if i % iter_per_epoch == 0: 
        train_acc = network.accuracy(x_train, t_train)
        test_acc = network.accuracy(x_test, t_test)
        train_acc_list.append(train_acc) 
        test_acc_list.append(test_acc)  
        print("train acc, test acc | " + str(train_acc) + ", " + str(test_acc))
markers = {'train': 'o', 'test': 's'}
x = np.arange(len(train_acc_list))
plt.plot(x, train_acc_list, label='train acc')
plt.plot(x, test_acc_list, label='test acc', linestyle='--')
plt.xlabel("epochs")
plt.ylabel("accuracy")
plt.ylim(0, 1.0)
plt.legend(loc='lower right')
plt.show()




























