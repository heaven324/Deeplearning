최종 포트폴리오 주제 선정하기 위해 도움되는 자료

1. 케글 데이터

2. object detection






■ 어제 배운 내용을 리뷰

1. 최단 거리 알고리즘

2. 4장. 수치미분을 이용한 신경망 학습

	- 3층 신경망

	입력층  -->  은닉1층  -->  은닉2층 --> 출력층
	  0층          1층           2층         3층
		W1		W2	    W3

	- 2층 신경망(직접 신경망을 학습시킴)

	입력층 -->  은닉층  --> 출력층
	  0층        1층          2층
		W1	     W2







■ 수치미분을 이용한 2층 신경망 구현 코드 설명

	* 신경망에 들어간 함수들

		1. softmax 함수 --> 입력값과 가중치의 총합을 받아 0~1 사이의 확률벡터를 출력하는 함수

		2. sigmoid 함수 --> 입력값과 가중치의 총합을 받아 0~1 사이의 확률값을 출력하는 함수

		3. predict 함수 --> 입력층 -> 은닉층 -> 출력층을 구현한 함수

		4. cross_entropy_error 함수 --> 예측값과 라벨(target)을 입력 받아 오차를 출력하는 함수

		5. loss 함수 --> 출력층 -> 오차함수를 구현한 함수

		6. accuracy 함수 --> 예측값과 라벨(target) 값을 받아 정확도를 출력하는 함수

		7. numerical_gradient 함수 --> 비용함수와 가중치를 입력받아 해당 가중치의 기울기를 
					       출력하는 함수

		8. simpleNet 클래스를 객체화 시켜서 훈련 데이터를 100장씩 입력해서 학습시키는 코드





문제 150. 위의 train_loss_list 에 들어간 오차들을 그래프로 시각화 하면 어떻게 되겠는가?

	그림 4-_-7




문제 151. 위의 신경망의 정확도를 시각화 하면 어떻게 되겠는가 ?

	그림 4-_-8




문제 152. 위의 정확도 그래프를 실제로 파이썬 코드로 시각화 하시오 !

	코드 참고 (수치미분_오차역전파 코드를 그래프까지 그리는 코드.txt)





문제 153. 위의 코드를 수정해서 그림 4-11 에 해당하는 오차 그래프를 그리시오 ! 
	  ( 오차가 점점 줄어드는 그래프를 그리시오 )

	# 그래프 그리기
	
	markers = {'train': 'o', 'test': 's'}
	x = np.arange(len(train_loss_list))
	plt.plot(x, train_loss_list, label='loss')
	plt.xlabel("iteration")
	plt.ylabel("loss")
	plt.ylim(0, 1.0)
	plt.legend(loc='lower right')
	plt.show()






★ 힘들게 학습시켜 만들어 놓은 가중치를 파일로 생성하는 방법

		" pickle 을 이용하면 된다 !!! "


* pickle 모듈을 이용하는 예제

	import pickle

	list = ['a','b','c']

	with open('d:\\list.pkl', 'wb') as f:
	    pickle.dump(list,f)



	- pickle 파일을 파이썬으로 로드하는 예제
	
		with open ('d:\\list.pkl','rb') as f:
		    data = pickle.load(f)
		
		print(data)



	- 이미 저자가 만들어 놓은 가중치와 바이어스로 신경망을 구현하는 코드

		# 가중치와 bias 값을 가져오는 함수	
	
		def init_network():
		    with open('sample_weight.pkl', 'rb') as f:
		        network = pickle.load(f)
		    return network
		
		3층 신경망의 가중치와 바이어스의 pickle 파일
		
		(W1, W2, W3, b1, b2, b3)





문제 154. 방금 학습시킨 2층 신경망의 가중치와 바이어스를 two_layer_w.pkl 이라는 pickle 파일로 
	  생성하시오 !

	import pickle
	
	with open('d:\\two_layer_w.pkl', 'wb') as f:
	    pickle.dump(network.params,f)
	
		※ 설명 : 지금 생성한 pickle 파일은 2층 신경망의 가중치와 바이어스
			  (W1,W2,b1,b2) 의 값을 저장한 파일이다.





문제 155. 지난번 저자가 만들어온 가중치와 바이어스의 pickle 파일을 로드해서 만든 3층 신경망에 nist 숫자 
	  데이터 한장을 넣어서 인공지능이 잘 맞추는지 확인하는 코드를 다시 돌려보시오 !

	import sys, os
	sys.path.append(os.pardir)
	
	import numpy as np
	from dataset.mnist import load_mnist
	from PIL import Image
	import  pickle
	import  numpy  as np
	
	# 신경망 함수들
	def sigmoid(num):
	    rst = (1 / (1 + np.exp(-num)))
	    return (rst)

	def identity_function(x):
	    return x
	
	def softmax(a):
	    c = np.max(a)
	    minus = a - c
	    exp_a = np.exp(minus)
	    sum_exp_a = np.sum(exp_a)
	    y = exp_a / sum_exp_a
	    return y
	
	def  init_network():
	    with open("d:\\two_layer_w.pkl",'rb')  as  f:
	        network = pickle.load(f)
	    return  network
	
	
	def  predict(network, x):
	    W1, W2, W3 = network['W1'], network['W2'], network['W3']
	    b1, b2, b3 = network['b1'], network['b2'], network['b3']
	    a1 = np.dot(x,W1) + b1
	    z1 = sigmoid(a1)
	    a2 = np.dot(z1,W2) + b2
	    z2 = sigmoid(a2)
	    a3 = np.dot(z2,W3) + b3
	    y = softmax(a3)
	    return  y
	
	def  get_data():
	    (x_train, t_train) , (x_test, t_test) = \
	    load_mnist(normalize=True, flatten=True, one_hot_label=False)
	    return  x_test, t_test
	
	x, t = get_data()
	network = init_network()
	y = predict(network,x[0])
	print(np.argmax(y))





문제 156. 위의 코드를 수정해서 지금 방금 만든 two_layer_w.pkl 파일을 로드해서 x[0] 숫자를 잘 
	  맞추는지 확인하시오 !

	저자가 만든 신경망	vs 	우리가 만든 신경망
	
		3층 신경망		2층 신경망
	





문제 157. 지금 pickle 파일 로드해서 만든 2층 신경망에 선혜가 만든 필기체 그림을 입력해서 숫자 4를 잘 
	  맞추는지 확인하시오 !
	  (점심시간 문제)

	import sys, os
	sys.path.append(os.pardir)
	
	import numpy as np
	from dataset.mnist import load_mnist
	from PIL import Image
	import  pickle
	import  numpy  as np

	# 신경망 함수들
	def sigmoid(num):
	    rst = (1 / (1 + np.exp(-num)))
	    return (rst)
	
	def identity_function(x):
	    return x
	
	def softmax(a):
	    c = np.max(a)
	    minus = a - c
	    exp_a = np.exp(minus)
	    sum_exp_a = np.sum(exp_a)
	    y = exp_a / sum_exp_a
	    return y
	
	def  init_network():
	    with open("d:\\two_layer_w.pkl",'rb')  as  f:
	        network = pickle.load(f)
	    return  network
	
	def  predict(network, x):
	    W1, W2= network['W1'], network['W2']
	    b1, b2= network['b1'], network['b2']
	    a1 = np.dot(x,W1) + b1
	    z1 = sigmoid(a1)
	    a2 = np.dot(z1,W2) + b2
	    y = softmax(a2)
	    return  y
	
	def  get_data():
	    (x_train, t_train) , (x_test, t_test) = \
	    load_mnist(normalize=True, flatten=True, one_hot_label=False)
	    return  x_test, t_test
	
	import cv2
	j = 'D:\\5pic.jpg'
	import numpy as np
	import matplotlib.pyplot as plt
	import matplotlib.image as mpimg
	
	def rgb2gray(rgb):
	    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])
	
	img = mpimg.imread(j)
	gray = rgb2gray(img)
	a = np.array(gray)
	pic= a.flatten()
	print(pic.shape)
	
	x, t = get_data()
	network = init_network()
	y = predict(network,pic)
	
	print(np.argmax(y))
















■ 5장. 오차 역전파를 이용한 3층 신경망 학습 시키는 방법 

	4장에서는 순전파를 배웠고 또 가중치를 갱신하기 위한 기울기를 구하기 위해 수치미분을 배웠다

	"순전파 + 가중치 갱신 (수치미분을 이용해서 ) "


※ 역전파 ? 

	신경망 학습 처리에서 최소화 되는 함수의 경사를 효율적으로 계산하기 위한 방법으로 "오류 역전파"
	가 있다. 

	- 함수의 경사 (기울기) 를 계산하는 방법 ?

		1. 수치미분 <--------- 너무 성능이 느렸다.
		2. 오류 역전파
	
	- 순전파 : 입력층 -----> 은닉층 ------> 출력층
	- 역전파 : 입력층 <----- 은닉층 <------ 출력층 

		여기서 역전파를 시키는 것이 뭐냐면 ? 오류(오차) 입니다.

		출력층 부터 차례대로 역방향으로 따라 올라가 각 층에 있는 노드의 오차를 계산할 수 있다.

		각 노드의 오차를 계산하면 그 오차를 사용해서 함수의 경사(기울기)를 계산할 수 있다.

		" 즉 전파된 오차를 이용해서 가중치를 조정한다. "

				 ↓

			" 오류(오차) 역전파 "





★ 계산 그래프 ( p 148 )

	" 순전파와 역전파에 계산 과정을 그래프로 나타내는 방법 "

	계산 그래프의 장점이 무엇인가?
		국소적 계산을 할 수 있다.
		국소적 계산이란 전체에 어떤 일이 벌어지든 상관없이 자신과 관련된 정보만으로 다음 결과를
		출력할 수 있다는 것이다.

		그림 5-_-1

		4000원이라는 숫자가 어떻게 계산 되었느냐와는 상관없이 사과가 어떻게 200원이 되었는가만
		신경쓰면 된다는 것이 국소적 계산이다.




☆ 왜 계산그래프로 문제를 해결하는가?

	전체가 아무리 복잡해도 각 노드에서 단순한 계산에 집중하여 문제를 단순화 시킬 수 있다.



☆ 실제로 계산그래프를 사용하는 가장 큰 이유는?

	역전파를 통해서 미분을 효율적으로 계산할 수 있는 점에 있다.
				↓
	사과 값이 '아주 조금' 올랐을 때 '지불금액'이 얼마나 증가하는지 알고 싶은 것이다.

			∂지불금액 / ∂사과값

	지불 금액을 사과값으로 편미분하면 알 수 있다.
				↓ 계산 그래프 역전파를 이용하면 되는데
	사과 값이 1원이 오르면 최종금액은 2.2이 오른다.




문제 158. 곱셈 계층을 파이썬으로 구현하시오 ! ( p 161)

	class MulLayer:
	    def __init__(self):
	        self.x = None
	        self.y = None
	    
	    def forward(self, x, y):
	        self.x = x
	        self.y = y
	        out = x * y
	        return out
	    
	    def backward(self, out):
	        dx = dout * self.y
	        dy = dout * self.x
	        return dx, dy




문제 159. 위에서 만든 곱셈 클래스를 객체화 시켜서 사과 가격의 총 가격을 구하시오 !

	apple =100
	apple_num = 2
	tax = 1.1
	
	#계층
	mul_apple_layer = MulLayer()
	mul_tax_layer = MulLayer()
	
	#순전파
	apple_price = mul_apple_layer.forward(apple, apple_num)
	price = mul_tax_layer.forward(apple_price, tax)
	
	print(price)
	
	220.00000000000003




문제 160. 덧셈 계층을 파이썬으로 구현하시오 ! (클래스 생성) (p 163)

	class AddLayer:
	    def __init__(self):
	        pass
	    
	    def forward(self, x, y):
	        out = x + y
	        return out
	    
	    def backward(self, dout):
	        dx = dout * 1  # 덧셈노드는 상류값을 여과없이 하류로 흘려보냄
	        dy = dout * 1
	        return dx, dy




문제 161. 위에서 만든 곱셈 클래스와 덧셈 클래스를 이용해서 책 149쪽의 그림 5-3의 신경망을 구현하시오 !

	apple = 100
	apple_num = 2
	orange = 150
	orange_num = 3
	tax = 1.1
	
	mul_apple_layer = MulLayer()
	mul_orange_layer = MulLayer()
	add_total_layer = AddLayer()
	mul_tax_layer = MulLayer()
	
	apple_price = mul_apple_layer.forward(apple, apple_num)
	orange_price = mul_orange_layer.forward(orange, orange_num)
	total_layer = add_total_layer.forward(apple_price, orange_price)
	price = mul_tax_layer.forward(total_layer, tax)
	print(price)




문제 162. 이번에는 순전파로 출력한 과일 가격의 총합 신경망의 역전파를 구현하시오 !

	dprice = 1
	dtotal_layer, dtax = mul_tax_layer.backward(dprice)
	dapple_price, dorange_price = add_total_layer.backward(dtotal_layer)
	dapple, dapple_num = mul_apple_layer.backward(dapple_price)
	dorange, dorange_num = mul_orange_layer.backward(dorange_price)
	print(dapple, dapple_num, dorange, dorange_num, dtax)







★ 활성화 함수 계층 구현하기

	1. Relu 계층을 위한 클래스 생성

		" 0보다 큰 값이 입력되면 그 값을 그대로 출력하고 0이거나 0보다 작은 값이 입력되면 0을
		  출력하는 함수 "

		- 순전파 함수
		- 역전파 함수

	* Relu클래스를 이해하려면 알아야 하는 파이썬 문법 2가지
	
		1. copy의 의미
		2. x[x <= 0]의 의미


		1. copy 테스트

			from copy import copy
			
			a = [1, 2, 3]
			b = copy(a)
			
			print(b)
			a[1] = 6
			print(a)
			print(b)
					
			[1, 2, 3]
			[1, 6, 3]
			[1, 2, 3]


		2. x[x <= 0] 테스트

			import numpy as np

			x = np.array([[1.0, -0.5], [-2.0, 3.0]])
			print(x)
			mask = (x <= 0)
			print(mask)

			[[ 1.  -0.5]
			 [-2.   3. ]]
			[[False  True]
			 [ True False]]

			out = x.copy()
			out[mask] = 0
			print(out)
			
			[[ 1.  0.]
			 [ 0.  3.]]




문제 163. 책 166페이지의 Relu클래스를 생성하시오 !

	from copy import copy
	
	class Relu:
	    def __init__(self):
	        self.mask = None
	        
	    def forward(self, x):
	        self.mask = (x <= 0)
	        out = x.copy()
	        out[self.mask] = 0
	        return out
	    
	    def backward(self, dout):
	        dout[self.mask] = 0
	        dx = dout
	        return dx
	    
	import numpy as np
	x = np.array([[1.0, -0.5], [-2.0, 3.0]])
	relu = Relu()
	print(relu.forward(x))
	
	[[ 1.  0.]
	 [ 0.  3.]]




































