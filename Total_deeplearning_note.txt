■ 1장. 이 책을 보기 위해 필요한 파이썬 기본 문법
	- numpy 란 ?						문제 1 ~ 4
	- 넘파이의 산술연산					문제 5 ~ 7
	- numpy의 브로드 캐스트 기능				문제 8 ~ 12
	- matplotlib 사용 법					문제 13 ~ 25

■ 2장 퍼셉트론 (Perceptron)					문제 26 ~ 33
	- 신경망의 종류 2가지
	- 퍼셉트론 게이트 4가지				문제 34 ~ 39
	- 단층 신경망과 다층 신경망의 차이			문제 40 ~ 42

■ 3장. 신경망
	- 활성화 함수 ?
	- 활성화 함수의 종류 3가지
	- 시그모이드 함수의 유래
	- Relu 함수 ( Rectified Linear Unit )		문제 48 ~ 49
	- 다차원 배열의 계산 (p77)				
	- 행렬의 내적						문제 50 ~ 56
	- 신경망 내적						문제 57 ~ 60
	- 출력층 함수 (p90)					문제 61 ~ 63
	- 함수를 변수에 담아서 사용하기			문제 65 ~ 66
	- 신경망 안에 들어가는 함수들			문제 67 ~ 68
	- 출력층 함수 소프트 맥스 함수			문제 69 ~ 76
	- 출력층의 뉴런 수 구하기	
	- mnist(손글씨 필기체) 데이터 로드			문제 77 ~ 86
	- 저자의 신경망 코드					문제 87
	- 정확도를 이해를 위한 파이썬 기본 문법		문제 88 ~ 94
	- 배치처리 (p 102)
	- 배치 이해를 위한 파이썬 코드연습			문제 95 ~ 106

■ 4장. 신경망 학습
	- 오차 함수
	- 평균제곱 오차 함수 (p 112) 			문제 107 ~ 108
	- 교차 엔트로피 오차함수				문제 109 ~ 111
	- 미니배치 (mini batch) 학습 (p 115)		문제 112 ~ 115
	- 미니 배치 처리에 맞게 교차 엔트로피 함수 구성	문제 116
	- 수치 미분 ( p121 ) 					문제 117 ~ 124
	- 편미분 (p 125)					문제 125
	- 경사하강법						문제 130 ~ 133
	- 위에서 배운 방법으로 3층 신경망 학습법 구현	문제 134 ~ 139
	- accuracy (정확도) 구하는 함수의 이해		문제 147
	- numerical_gradient 함수 생성
	- 비용함수 생성 방법 					문제 148 ~ 149
	- 만들어 놓은 가중치 파일로 생성하는 방법		문제 154 ~ 157

■ 5장. 오차 역전파를 이용한 3층 신경망 학습 시키는 방법 
	- 계산 그래프 ( p 148 )				문제 158 ~ 162
	- 활성화 함수 계층 구현하기				문제 163 ~ 164
	- sigmiod 함수를 파이썬으로 구현하기		문제 165 ~ 166
	- Affine 계층 ( p 170 )				문제 167 ~ 172
	- Affine 계층의 계산 그래프				문제 173 ~ 177
	- OrderDict() 딕셔너리의 이해			문제 183 ~ 184
	- 오차 역전파를 이용한 2층 신경망의 코드		문제 185 ~ 186

■ 6장. 신경망의 정확도를 높이기 위한 여러가지 방법들 소개
	- 1. SGD ( Stochastic Gradient Descent )	
	- 2. momentum(운동량)					문제 188
	- 3. Adagrade 경사 감소법				문제 189
	- 4. Adam 경사 감소법 				문제 190
	- 가중치 초기화 값 설정				문제 191
	- 오버피팅을 억제하는 방법 2가지			문제 192
	- 배치 정규화 (p210)					문제 193 ~ 194
	- 배치 정규화란 ?
	- 배치 정규화 공식
	- 배치 정규화의 장점 (p 210 )			문제 195
	- 배치 정규화의 계산 그래프
	- 배치 정규화의 역전파				문제 196

■ 7장. CNN (Convolution Neural Network)
	- 기존에 구현했던 완전 연결 계층의 문제점		문제 197 ~ 203
	- Padding ( p 232 )					문제 204 ~ 207
	- 3차원 합성곱 ( p 235 )				문제 208 ~ 228
	- 블록으로 생각하기 ( p 237 )			문제 229
	- 배치처리 ( p 239 )
	- im2col 함수의 역할					문제 230 ~ 232
	- 필터 RGB행렬을 2차원으로 푸는 방법		문제 233 ~ 240
	- convolution 클래스내에서 일어나는 일		문제 241 ~ 244
	- 풀링(pooling) 층의 역할				문제 245 ~ 254
	- CNN구현하기 (p 250)					문제 255

■ 문제 모음

■ 딥러닝 수업

	인공지능의 눈 ?  cnn

		사진속의 사람, 동물 등을 구별할 수 있다.

		다음 카카오 로드뷰에서 사람 얼굴이나 차번호등은 개인정보 보호법상 반드시 모자이크 처리를
		해야하는데 이것을 일일히 사람이 할 수 없다.
		그래서 컴퓨터에게 모자이크 처리를 하라고 시켜야 한다.

		딥러닝 기술로 할 수 있다.


		제조업에서 만드는 제품들의 불량 여부를 확인하는 직업인 품질관리사의 롤을 컴퓨터가 대신할
		수 있다.
		관련 영상 : https://www.youtube.com/watch?v=cYDDowrozNI


	인공지능의 입과 귀 ?  rnn

		변호사가 되기 위해 봐야할 법률 책 ----> 컴퓨터에 넣고 학습

		판례 ------> 컴퓨터 ------> 판결

		인공지능 변호사




■ 딥러닝 수업 목차

	1장. 이 책을 보기 위해 필요한 파이썬 기본 문법

	2장. 퍼셉트론 ( 신경망의 하나의 세포를 컴퓨터로 구현 )

	3장. 인공 신경망 ( 신경망의 활성화 함수, 3층 신경망 구현 )

	4장. 인공 신경망 학습 시킴 ( 손실 (오차) 함수, 수치 미분, 경사하강법, 학습 알고리즘 구현)

	5장. 오차 역전파 ( 계산 그래프, 연쇄법칙, 역전파 )

	6장. 신경망을 학습시키는 여러 기술들 소개
	     (경사 하강법의 종류, 배치 정규화, 드롭아웃)

	7장. CNN ( 합성곱 신경망 )
		사진을 신경망에 입력해서 이 사진이 어떤 사진인지 컴퓨터가 알아 맞히게 하는 방법을 구현

	8장. 딥러닝의 역사
		딥러닝 --> 텐서플로우를 이용해서 신경망을 구현

		목표 : "이미지를 분류할 수 있는 신경망을 구현"
			구체적인 예 : 정상 폐사진 vs 폐결절 사진을 구분

		제조업에서 만드는 제품들의 불량 여부 확인

			정상 이파리 vs 질병에 걸린 이파리
■ 1장. 이 책을 보기 위해 필요한 파이썬 기본 문법


★ numpy 란 ?
문제 1 ~ 4
	- python언어에서 기본적으로 지원하지 않는 배열(array) 혹은 행렬(matrix)의 계산을 손쉽게 해주는
	  라이브러리

	- 머신러닝에서 많이 사용하는 선형대수학에 관련된 수식들을 python에서 쉽게 프로그래밍할 수 있게
	  해준다.





★ 넘파이의 산술연산 (p 37)
문제 5 ~ 7
	import numpy as np
	x = np.array( [1, 2, 3] )
	y = np.array( [2, 4, 6] )

	print( x + y )
	print( x * y )
	print( x / y )

	[3 6 9]
	[ 2  8 18]
	[ 0.5  0.5  0.5]


★ numpy의 브로드 캐스트 기능
문제 8 ~ 12
		참고 (1.5.5_브로드캐스트_그림1_1.png)

	넘파이 배열은 원소별 계산뿐 아니라 넘파이 배열과 수치 하나(스칼라 값)의 조합으로 된 산술연산도
	수행할 수 있습니다. 이 경우 스칼라 값과의 계산이 넘파이 배열의 원소별로 한 번씩 수행 됩니다.
	이 기능을 브로드캐스트라고 합니다. (p 37)




★ matplotlib 사용 법 (p 41)
문제 13 ~ 25
	딥러닝 실험에서는 그래프 그리기와 데이터 시각화가 중요하다.

	matplotlib는 그래프를 그리기 위한 라이브러리이다.

	matplotlib를 이용하면 그래프를 그리기가 쉬워진다.


	예제1 :
		import matplotlib.pyplot as plt
		plt.plot( [ 1,2,3,4,5,6,7,8,9,8,7,6,5,4,3,2,1,0])
		plt.show()

		


	예제2 :
	
		import matplotlib.pyplot as plt
		import numpy as np
		
		t = np.arange(0, 12, 0.01)
		print(t)
		
		plt.plot(t)
		plt.show()

		[  0.00000000e+00   1.00000000e-02   2.00000000e-02 ...,   1.19700000e+01
		   1.19800000e+01   1.19900000e+01]
		
■ 2장 퍼셉트론 (Perceptron)
문제 26 ~ 33
	인공지능 --> 머신러닝 --> 딥러닝

	• 머신러닝 ( 컴퓨터가 스스로 학습 하는 알고리즘 ) 의 종류 3가지

		   1. 지도 학습 : 정답이 있는 상태에서 학습
		   딥러닝
		   예 : 개 사진 10000장, 고양이 사진 10000장을 계속 컴퓨터에게 보여주면서 이 사진은 개다
		        이 사진은 고양이다 라고 계속 알려주는 것이다.

		   2. 비지도 학습 : 정답이 없는 상태에서 학습

		   3. 강화 학습 : 보상을 통해서 학습 데이터를 만들며 학습


		지도 학습과 강화 학습은 케이크의 크림과 체리에 해당하고 비지도 학습은 케이크의 빵에 해당한다.
		
		겉보기에 화려한 부분은 강화학습인데 실제로
		
		인공지능 --> 머신러닝 --> 딥러닝
		                  ↓
		                 퍼셉트론
               
	퍼셉트론 ? 인간의 뇌세포중 하나를 컴퓨터로 구현해 봄 ( 뉴런 )
	1957년에 프랑크 로젠 블라트가 퍼셉트론 알고리즘을 고안을 했다.
	사람의 뇌의 동작을 전기 스위치인 온/오프로 흉내 낼 수 있다는 이론을 증명을 했다.
	간단히 말하면 인간의 신경세포 하나를 흉내를 냈는데
	

	고등학교 생물시간 배운 3가지 용어?

	   1. 자극 (stimulus)
	   2. 반응 (response)
	   3. 역치 (threshold)

		" 특정 자극이 있다면 그 자극이 어느 역치 이상이어야지만 세포가 반응한다 "

		예 : 짜게 먹는 사람은 자기가 평소에 먹는 만큼 음식이 짜지 않으면 싱겁다고 느낀다.
		   ( 역치이하의 자극은 무시)

		     싱겁게 먹는 사람은 짜게 먹기 시작하면 오랜 시간 지나면 예전에 먹던 싱거운 음식에
		     만족하지 못한다.(역치가 올라감)

	   • 뉴런의 개수
	   
	      1. 사람 : 850억개
	      2. 고양이 : 10억개
	      3. 쥐 : 7천5백만개
	      4. 바퀴벌레 : 몇백만개
	      5. 하루살이 : 지금 현재까지 나온 최첨단 인공지능의 뉴런수 보다 많다.
   
      퍼셉트론 ?





★ 신경망의 종류 2가지

	1. 단층 신경망 			선형 분류

		입력층 -------> 출력층
		 0층		  1층

							  반복
		1957년 로젠블라트 ┬────▶ 뇌 : 신호 -----> 암기
		                  │
			           │				         가중치
		                  └────▶ 인공신경망 : 신호 ┬────▶ 1 (흐른다)
				         	                     └────▶ 0 (안흐른다)


		퍼셉트론 ?
			사람의 뇌 세포 하나를 단층 신경망으로 구현
			n개의 이진수가 하나의 뉴런을 통과해서 가중의 합이 0보다 크면 활성화 되는 가장
			간단한 신경망




	2. 다층 신경망			비선형 분류

		얕은 신경망 : 입력층 -----> 은닉층 -----> 출력층
			       0층	     1층	   2층

		깊은 신경망 : 입력층 -----> 은닉층들 -----> 출력층
			       0층	       1층	     2층
				(Deep learning)





★ 퍼셉트론 게이트 4가지
문제 34 ~ 39
	1. And  게이트
		x1	x2	y
		0	0	0
		0	1	0		
		1	0	0
		1	1	1

	2. Or   게이트
		x1	x2	y
		0	0	0
		0	1	1
		1	0	1
		1	1	1


	3. NAnd 게이트 (Not And)

		x1	x2	y
		0	0	1
		0	1	1
		1	0	1
		1	1	0
		

	4. XOr  게이트 (Exclusive Or라는 뜻으로 둘중에 하나만 1이 될 때 1이 된다)

		x1	x2	y
		0	0	0
		0	1	1
		1	0	1
		1	1	0



	입력신호의 연결강도가 가중치인데 가중치의 값이 클수록 "강한 신호"이다
	
	입력신호가 뉴런에 보내질때는 각각의 고유한 가중치 곱해진다.
	
	w1 * x1 + w2 * x2 <= 임계값(θ) ----> 0 (신호가 안흐른다)
	w1 * x1 + w2 * x2 >  임계값(θ) ----> 0 (신호가 흐른다)
	
	뉴런에서 보내온 신호의 총합이 정해진 한계(임계값)을 넘어설 때만 1을 출력한다.
	
	퍼셉트론은 n개의 이진수가 하나의 뉴런을 통과해서 가중의 합이 0보다 크면 활성화 되는 가장 간단한
	신경망이다.
	
	퍼셉트론을 학습시키는 방법은 간단한데, 보통 목표치를 정해주고 현재 계산한 값이 목표치와 다르면 그
	만큼의 오차를 다시 퍼셉트론에 반영해서 오차를 줄여나가는 방법이다.		



★ 퍼셉트론 함수 4가지
	1. AND  게이트 함수 : 단층
	2. OR   게이트 함수 : 단층
	3. NAND 게이트 함수 (Not AND): 단층
	4. XOR  게이트 함수 (eXclusive OR): 다층





★ 단층 신경망과 다층 신경망의 차이
문제 40 ~ 42
	- 1958년 로젠 블래트가 퍼셉트론을 제안을 했다.
	- 1959년 민스키가 기존 퍼셉트론의 문제점을 지적했는데 xor분류를 못한다는 문제점을 지적하고
	  인공지능의 겨울기가 시작되었다.
	  즉, xor 게이트는 단층 신경망으로는 구현이 안되는 것이었다.
	      xor 게이트는 다층 신경망으로 구현해야 하는 것이었다.


	1. XOR 게이트 진리 연산표

	xor	x1	x2	t
		0	1	0
		1	0	1
		0	1	1
		1	1	0


	2. 중간층을 넣어서

		입력층(0층)		    은닉1층(1층)	     출력층(2층)

	xor	x1	x2	|	or 결과		NAND 결과  =	AND결과
	----------------------- | -------------------------------- = ----------
		0	0	|	   0		   1	   =	   0
		0	1	|	   1		   1	   =	   1
		1	0	|	   1		   1	   =	   1
		1	1	|	   1		   0	   =	   0

	




■ 3장. 신경망

	* 퍼셉트론과 신경망의 차이점 ?

	    1. 퍼셉트론 ?
		

	편향 ?
	뉴런이 얼마나 쉽게 활성화 되는지를 제어한다.
	편향이 없다면 모든 뉴련이 원점을 통과하는 결과가 발생.



★ 활성화 함수 ?

	입력신호의 총합을 출력신호로 변환하는 함수





★ 활성화 함수의 종류 3가지

	1. Step함수
		극명하게 두가지 케이스로 나뉘어 주는 함수
		극명하게 둘로 나뉜다면 애매한 내용들을 표현하기가 어렵다
		

	2. Sigmoid 함수
		step함수를 보완해 애매한 내용들을 0~1사이의 실수로 표현하는 함수
		
		

	3. Relu 함수	(현업에서 가장 많이 사용)
		입력이 0을 넘으면 그 입력을 그대로 출력		표현을 강하게 약하게 할 수 있다.
		입력이 0이하면 0을 출력				아닌건 모두 0으로
		
		





★ 시그모이드 함수의 유래

	통계학에서는 성공확률이 실패확률보다 얼마나 클지를 나타내는 오즈비율이라는 값이 있다.

	Odds = 성공/실패 = p/(1-p)
	이 그래프는 p의 값이 1에 가까우면 오즈값이 급격히 커져버리는 현상이 일어나 p/(1-p)에 로그를 취한게
	로짓함수 이다 log(p/(1-p))
	





★ Relu 함수 ( Rectified Linear Unit )
		 정류된
문제 48 ~ 49
	Relu는 입력이 0을 넘으면 그 입력을 그대로 출력하고 0 이하면 0을 출력하는 함수

	신경망 학습이 잘되어서 현업에서 주로 사용하는 함수





★ 다차원 배열의 계산 (p77)

	넘파이로 행렬을 만들고 차원수 확인과 몇행 몇열인지 확인하는 방법

	예 : 아래의 3행 2열의 행렬을 만드시오 !

		1 2
		3 4
		5 6

		import numpy as np
		b = np.array( [[1, 2], [3, 4], [5, 6]] )
		print(b)
		
		print( np.ndim(b))
		print( b.shape)




★ 행렬의 내적
문제 50 ~ 56
	


★ 신경망 내적 (p82)
문제 57 ~ 60


★ 출력층 함수 (p90)
문제 61 ~ 63
	"출력층의 함수는 그동안 흘러왔던 확률들의 숫자를 취합해서 결론을 내줘야 하는 함수 !"

	신경망으로 구현하고자 하는 문제가

		1. 회귀면 ? 항등함수
			예 : 독립변수 ( 콘크리트 재료 : 자갈 200kg, 시멘트 20포대....)
			     종속변수 ( 콘크리트 강도 )

		2. 분류면 ? 소프트 맥스 함수
			예 : 정상 폐사진  vs  폐결절 사진




	* 3층 신경망의 가중치인 w1, w2, w3를 하나로 모아서 심플하게 코드를 작성하려면 ?
	
		"딕셔너리를 활용하면 된다"
		import numpy as np
		
		network = {}
		
		network['w1'] = np.array([[1, 2], [3 ,4], [5, 6]])
		network['w2'] = np.array([[3, 5, 7], [4, 6, 8]])
		network['w3'] = np.array([[4, 6], [5, 7]])
		print(network.keys())
		print(network.values())

	dict_keys(['w1', 'w2', 'w3'])
	dict_values([array([[1, 2],
	       [3, 4],
	       [5, 6]]), array([[3, 5, 7],
	       [4, 6, 8]]), array([[4, 6],
	       [5, 7]])])



★ 함수를 변수에 담아서 사용하기
문제 65 ~ 66
	예제1 :
		def other_method(a):
		    print(a)
		    
		val = other_method  # 함수를 val이라는 변수에 담는다.
		other_method(123)
		val(123)
★ 신경망 안에 들어가는 함수들
문제 67 ~ 68
	1. 가중치를 만드는 함수 : init_network()
	2. 순전파 시키는 함수 : forward()
	3. 역전파 시키는 함수 : backward()
	4. 활성화 함수들 : sigmoid(), relu(), step_func()
	5. 출력층 함수들 : identity_func(), softmax()
	6. 오차 함수들 : 평균오차함수(회귀분석), 교차엔트포리 함수(분류문제)




★ 출력층 함수 소프트 맥스 함수
문제 69 ~ 76
	"분류를 위한 출력층 함수인데 0 ~ 1 사이의 숫자를 출력하는 함수"

	"순전파로 흘러왔던 확률(숫자)를 가지고 결론을 내주는 함수"

	결론이 뭐냐면 ? 개와 고양이 분류라면 개인지 고양이인지 결론을 내준다.

	정상폐, 폐결절, 폐결핵, 중 3개중 어떤 사진인지 결론을 내야 한다면 이 3개중 한가지로 결론을 내려주는
	함수
		

	소프트 맥스 함수 수학식을 컴퓨터 코드로 구현할때 주의사항 ?

		소프트 맥스 함수는 지수 함수를 사용하는데 이 지수함수 라는것이 쉽게 아주 큰 값을 내뱉는다.
		e (스위스의 수학자 오일러가 발견한 무리수)의 10승은 20,000이 넘고 e의 100승은 숫자가
		40개가 넘고 e의 1000승은 무한대를 뜻하는 inf가 출력되어 돌아오므로 컴퓨터로 계산을 할 수
		없다.


★ 출력층의 뉴런 수 구하기
							    출력층의 뉴런수 ?
	1. 분류하고자 하는 사진 개, 고양이 분류        ------>  2개
	2. 분류하고자 하는 사진 정상폐, 폐결절, 폐결핵 --------> 3개
	3. mnist 데이터 (필기체 숫자 0 ~ 9 까지의 사진) ---->  10개




★ mnist(손글씨 필기체) 데이터를 파이썬으로 로드하는 방법
문제 77 ~ 86
	1. 책 소스코드와 데이터를 다운로드 받는다. (dataset.zip)

	2. dataset이라는 폴더를 워킹 디렉토리에 가져다 둔다.
	   (실행하는 소스가 있는 디렉토리)

	3. 아래의 파이썬 코드를 실행해서 필기체 데이터 하나를 시각화 한다.

		# coding: utf-8
		
		import sys, os
		
		sys.path.append(os.pardir)  # 부모디렉토리의 파일을 가져올수
		                            # 있도록 설정
		
		import numpy as np
		from dataset.mnist import load_mnist
		from PIL import Image
		
		
		def img_show(img): # 이미지를 출력하는 함수
		    pil_img = Image.fromarray(np.uint8(img))
		    # 파이썬 이미지 객체로 변환
		    pil_img.show()  # pil_img 객체의 show 라는 메소드를 실행
		
		(x_train, t_train), (x_test, t_test) = load_mnist(flatten=True, normalize=False)
		
		
		img = x_train[0]
		label = t_train[0]
		print(label)  # 5
		
		print(img.shape)  # (784,)
		img = img.reshape(28, 28)
		
		img_show(img)



☆ 필기체 데이터 60000장을 충분히 학습시키고 만들어낸 가중치 값을 (저자가 미리 생성함)을 init_network함수로
   가져오는 방법

	import pickle
	def init_network():
	    with open('sample_weight.pkl', 'rb') as f:
	        network = pickle.load(f)
	    return network
	
	network = init_network()
	print(network['W1'].shape)
	print(network['W2'].shape)
	print(network['W3'].shape)
	print(network['b1'].shape)
	print(network['b2'].shape)
	print(network['b3'].shape)

	(784, 50)
	(50, 100)
	(100, 10)
	(50,)
	(100,)
	(10,)



   * 훈련데이터의 행렬 모습이 60000x784인지 확인한다.

	(x_train, t_train), (x_test, t_test) = load_mnist(flatten=True, normalize=False)
	
	print(x_train.shape)

	(60000, 784)
★ 저자가 이미 최적화 해놓은 W1, W2, W3, b1, b2, b3를 가지고 만든 신경망 코드
문제 87
	import sys, os
	
	sys.path.append(os.pardir)
	
	import numpy as np
	from dataset.mnist import load_mnist
	from PIL import Image
	import  pickle
	import  numpy  as np
	
	# 신경망 함수들
	def sigmoid(num):
	    rst = (1 / (1 + np.exp(-num)))
	    return (rst)
	
	
	def identity_function(x):
	    return x
	
	
	def softmax(a):
	    c = np.max(a)
	    minus = a - c
	    exp_a = np.exp(minus)
	    sum_exp_a = np.sum(exp_a)
	    y = exp_a / sum_exp_a
	    return y
	
	def  init_network():
	    with open("sample_weight.pkl",'rb')  as  f:
	        network = pickle.load(f)
	    return  network
	
	
	def  predict(network, x):
	    W1, W2, W3 = network['W1'], network['W2'], network['W3']
	    b1, b2, b3 = network['b1'], network['b2'], network['b3']
	
	    a1 = np.dot(x,W1) + b1
	    z1 = sigmoid(a1)
	    a2 = np.dot(z1,W2) + b2
	    z2 = sigmoid(a2)
	    a3 = np.dot(z2,W3) + b3
	    y = softmax(a3)
	
	    return  y
	
	def  get_data():
	    (x_train, t_train) , (x_test, t_test) = \
	    load_mnist(normalize=True, flatten=True, one_hot_label=False)
	    return  x_test, t_test
	
	x, t = get_data()
	network = init_network()
	y = predict(network,x[0])
	print(y)


★ 정확도를 이해하기 위해 기본적으로 알아야할 파이썬 문법
	오차함수 ? 예측값과 실제값의 차이를 구하는 함수
문제 88 ~ 94





★ 배치처리 (p 102)

	"신경망에 데이터 입력시 배치(batch)로 입력하는 방법"

	"이미지를 한장씩 처리하는게 아니라 여러장을 한번에 처리"


	1. 이미지를 한장씩 처리한 신경망
		

	2. 이미지를 100장씩 처리한 신경망
		




★ 배치로 신경망을 학습시키기 위한 코드를 이해하기 위해 기본적으로 알아야 하는 파이썬 코드 연습
문제 95 ~ 106
■ 4장. 신경망 학습

	앞장에서는 저자가 만들어온 가중치값을 가지고 신경망을 구성했기 때문에 신경망을 학습시킬 필요가
	없었다.

		필기체 7   ──────▶ 신경망  ──────▶ 예측값 == 실제값
					    ↑			   ↓
			      저자가 만들어온 가중치값 셋팅	  정확도

	4장에서는 우리가 직접 신경망을 학습을 시킬 것인데 학습을 시키기 위해서 알아야 할 내용 ?

		1. 오차함수
		2. 가중치를 갱신하는 방법 ( 수치 미분 )
		3. 미니배치 (mini batch)




★ 오차 함수

	"예상값과 실제값과의 오차를 신경망에 역전파 시켜주기 위해서 필요한 함수"

	오차가 최소화될 때 까지 신경망을 학습시키기 위해서 필요한 함수

		1. 평균제곱 오차함수      : 회귀분석을 할 때 사용
		2. 교차 엔트로피 오차함수 : 분류문제를 풀 때 사용

					 	 출력층 함수  	,     오차함수
		회귀 문제를 해결할 때 ───▶     항등함수   	, 평균제곱 오차함수
		분류 문제를 해결할 때 ───▶ 소프트맥스 함수	, 교차 엔트로피 함수




★ 1. 평균제곱 오차 함수 (p 112)
문제 107 ~ 108



★ 2. 교차 엔트로피 오차함수
문제 109 ~ 111
	




★ 3. 미니배치 (mini batch) 학습 (p 115)
문제 112 ~ 115
	"훈련 데이터중에 일부만 골라서 학습하는 방법"
	"표본을 뽑아서 학습 시킨다."
		     ↓
	예 : 된장찌개의 맛을 확인하기 위해서 한수저만 간을 본다
					     (표본)

	복원추출이든 비 복원 추출이든 mnist의 경우 100장씩 배치처리한다면 100장씩 600번을 훈련시키면 그게
	1 epooch이다.
	
	여러번 에폭이 반복되면서 비용함수(오차함수)의 global minima로 찾아가게 된다.

	설명 :  복원추출?	한번 뽑은 것을 다시 뽑을 수 있는 추출

		비 복원 추출?   한번 뽑은 것을 다시 뽑을 수 없는 추출

		에폭 ? 		
			





★ 미니 배치 처리에 맞게끔 교차 엔트로피 함수를 구성하는 방법
문제 116
	- 미니 배치 처리하기 전 교차 엔트로피 함수 (1장씩 처리)

	y = np.array([0, 0, 0.9, 0, 0, 0, 0, 0, 0.1 ])
	t = np.array([0, 0, 1,   0, 0, 0, 0, 0, 0])
	
	def cross_entropy_error(x,t):
	    delta = 1e-7
	    return -np.sum(t*np.log(x+delta))
	
	print(cross_entropy_error(y,t))

	0.105360404547


–	100장씩






★ 수치 미분 ( p121 )
문제 117 ~ 124
	신경망 학습 시킬 때 미분을 왜 알아야 하는가 ?
	가중치를 갱신해주기 위해서 미분이 필요하다.
	가중치 = 가중치 - 기울기

	한 점에서의 속도를 구하려면 접선의 방정식을 구해야 하는데 구하는 방법이 바로 미분(기울기)
			lim  ( f(x+h) - f(x) ) / h
			h→0

	위의 미분 공식을 파이썬 함수로 구현해서 실행을 해본다.

	import numpy as np
	print(np.float32(1e-50))
	0.0
	계산이 안되므로 h를 10의 -4승의 값을 사용하면 좋은 결과가 나온다고 알려져있다.

	1. 미분 함수의 첫번째 개선
		def numerical_diff(f,x):
		    h = 1e-4 # 0.0001, 컴퓨터로 는 극한값을 구하기 어려우므로
		    return (f(x+h) - f(x)) / h

		↓  컴퓨터로는 미분을 하지 못하니 도함수를 구현해야 한다.

	2. 두번째 개선

		접선의 공식				할선의 공식

		lim  ( f(x+h) - f(x) ) / h		lim  ( f(x+h) - f(x-h) ) / (x+h) - (x-h)
		h→0					h→0
	
							lim  ( f(x+h) - f(x-h) ) / 2h
							h→0

		def numerical_diff(f,x):
		    h = 1e-4
		    return (f(x+h) - f(x-h)) / 2h



		※ 컴퓨터로는 진정한 미분을 구현할 수가 없기 때문에 오차가 발생한다.

			왜?
				1. 극한값을 구현하기가 어렵다(limit)
				2. 할선의 방정식의 기울기로 미분함수를 만들어야 하기 때문이다.


		지금까지 구현한 비용함수는 3차원에 해당하는 비용함수 였다.
		즉 W(가중치)를 하나만 두고 만든 함수였다.
		그런데, 신경망에 들어가는 가중치는 W(가중치)가 W1하나만이 아니라 W0도 있고 W2도 있고
		여러개의 W가 있을 수 있기 때문에 비용함수가 2차원을 넘어설 수 있다.

	2차원그래프가 아니라 3차원 그래프를 미분할 때 사용한 미분방법이 ?






★ 편미분 (p 125)
문제 125
	"변수가 2개 이상인 함수를 미분할 때 미분 대상 변수 외에 나머지 변수를 상수처럼 고정시켜 미분하는
	 것을 편미분이라고 한다"

	f(x0, x1) = x0**2 + x1**2
	f(w0, w1) = w0**2 + w1**2
	



★ 경사하강법
문제 130 ~ 133
	" 특정 가중치의 위치에서 기울기를 빼서 점차 global minima로 진행하는 학습방법 "

	경사 하강법 식 :

		가중치 = 가중치 - 러닝레이트 * 기울기

		w0 = w0 - 학습률* (∂비용함수 / ∂w0 )
		w1 = w1 - 학습률* (∂비용함수 / ∂w1 )
		학습률 ?  한번의 학습으로 얼마만큼 학습해야 할지 즉 매개변수 값을 얼마나 갱신하느냐를
			  정하는 것

		학습률 값은 0.01 이나 0.001 등 미리 특정 값으로 정해두어야 하는데, 일반적으로 이 값이 너무
		크거나 작으면 '좋은 장소'를 찾아갈 수 없다.

		신경망 학습에서는 보통 이 학습률을 변경하면서 올바르게 학습하고 있는지를 확인하면서
		진행된다.



★ 위에서 배운 3가지 방법으로 3층 신경망을 학습 시키는 방법 구현
문제 134 ~ 139
	1. 오차함수
	2. 미니배치
	3. 수치미분(편미분)


☆ 학습이 스스로 되는 3층 신경망 구현




★ accuracy (정확도) 구하는 함수의 이해
문제 147
	"예상한 숫자와 실제 숫자를 비교해서 정확도를 출력하는 함수"

	
	예제 :
		def accuracy(self, x, t):
		    y = self.predict(x) # 10개짜리 확률벡터 100개 출력
		                        # 100 x 10 행렬
		# - y 값의 행렬
		
		#  [ [ 0.1 0.1 0.05 0.4 0.05 0.1 0.05 0.05 0.05 0.05 0.00],  # 3
		#    [ 0.4 0.1 0.05 0.1 0.05 0.1 0.05 0.05 0.05 0.05 0.00],  # 0
		#                  :  100개
		#                  :
		#    [ 0.1 0.1 0.05 0.4 0.05 0.1 0.05 0.05 0.05 0.05 0.00] ]
		
		# - target 의 행렬 (one hot encoding)
		
		#             [ [ 0 0 1 0 0 0 0 0 0 0  ],
		#               [ 1 0 0 0 0 0 0 0 0 0  ],
		#                          :
		#                          :     100개
		#               [ 0 0 1 0 0 0 0 0 0 0  ] ]
		
		    y = np.argmax(y, axis=1) # [ 3 0 1 5 6 .......... 3 ]  # 100개
		    t = np.argmax(t, axis=1) # [ 2 0 2 1 3 ...........3 ]  # 100개
		    accuracy = np.sum(y == t) / float(x.shape[0])
		    return accuracy




★ numerical_gradient 함수 생성

	"비용함수와 가중치 또는 바이어스를 입력받아 기울기를 출력하는 함수"
	
	예제 :
		from common.gradient import numerical_gradient
		def numerical_differentiation(self,x,t):
		    loss_W = lambda w: self.loss(x,t)
		    grads = {}
		    grads['W1'] = numerical_gradient(loss_W, self.params['W1'])
		    grads['b1'] = numerical_gradient(loss_W, self.params['b1'])
		    grads['W2'] = numerical_gradient(loss_W, self.params['W2'])
		    grads['b2'] = numerical_gradient(loss_W, self.params['b1'])
		    return grads



★ 비용함수 생성 방법
문제 148 ~ 149
	net = simpleNet(input_size = 784, hidden_size = 50, output_size = 10)

	def f(W):
	    return metwork.loss(x, t)

	설명 : 135 페이지
		여기서 정의한 f(W) 함수의 W의 의미는 더미로 만든것 입니다.
		numerical_gradient(f,x) 내부에서 f(x)를 실행하는데 그와의 일관성을 위해서 f(W)를
		정의한 것입니다.
		lambda 표현식으로 한줄로 표현하면 아래와 같습니다.

		f = lambda W: network.loss(x, t)
		dW = numerical_gradient(f, net.W)
		↑
		W의 기울기




★ 힘들게 학습시켜 만들어 놓은 가중치를 파일로 생성하는 방법
문제 154 ~ 157
		" pickle 을 이용하면 된다 !!! "


* pickle 모듈을 이용하는 예제

	import pickle

	list = ['a','b','c']

	with open('d:\\list.pkl', 'wb') as f:
	    pickle.dump(list,f)


	- pickle 파일을 파이썬으로 로드하는 예제
	
		with open ('d:\\list.pkl','rb') as f:
		    data = pickle.load(f)
		
		print(data)


	- 이미 저자가 만들어 놓은 가중치와 바이어스로 신경망을 구현하는 코드

		# 가중치와 bias 값을 가져오는 함수	
	
		def init_network():
		    with open('sample_weight.pkl', 'rb') as f:
		        network = pickle.load(f)
		    return network
		
		3층 신경망의 가중치와 바이어스의 pickle 파일
		
		(W1, W2, W3, b1, b2, b3)
■ 5장. 오차 역전파를 이용한 3층 신경망 학습 시키는 방법

	4장에서는 순전파를 배웠고 또 가중치를 갱신하기 위한 기울기를 구하기 위해 수치미분을 배웠다

	"순전파 + 가중치 갱신 (수치미분을 이용해서 ) "


※ 역전파 ?

	신경망 학습 처리에서 최소화 되는 함수의 경사를 효율적으로 계산하기 위한 방법으로 "오류 역전파"
	가 있다.

	- 함수의 경사 (기울기) 를 계산하는 방법 ?

		1. 수치미분 <--------- 너무 성능이 느렸다.
		2. 오류 역전파
	
	- 순전파 : 입력층 -----> 은닉층 ------> 출력층
	- 역전파 : 입력층 <----- 은닉층 <------ 출력층

		여기서 역전파를 시키는 것이 뭐냐면 ? 오류(오차) 입니다.

		출력층 부터 차례대로 역방향으로 따라 올라가 각 층에 있는 노드의 오차를 계산할 수 있다.

		각 노드의 오차를 계산하면 그 오차를 사용해서 함수의 경사(기울기)를 계산할 수 있다.

		" 즉 전파된 오차를 이용해서 가중치를 조정한다. "

				 ↓

			" 오류(오차) 역전파 "




★ 계산 그래프 ( p 148 )
문제 158 ~ 162
	" 순전파와 역전파에 계산 과정을 그래프로 나타내는 방법 "

	계산 그래프의 장점이 무엇인가?
		국소적 계산을 할 수 있다.
		국소적 계산이란 전체에 어떤 일이 벌어지든 상관없이 자신과 관련된 정보만으로 다음 결과를
		출력할 수 있다는 것이다.
		4000원이라는 숫자가 어떻게 계산 되었느냐와는 상관없이 사과가 어떻게 200원이 되었는가만
		신경쓰면 된다는 것이 국소적 계산이다.




☆ 왜 계산그래프로 문제를 해결하는가?

	전체가 아무리 복잡해도 각 노드에서 단순한 계산에 집중하여 문제를 단순화 시킬 수 있다.



☆ 실제로 계산그래프를 사용하는 가장 큰 이유는?

	역전파를 통해서 미분을 효율적으로 계산할 수 있는 점에 있다.
				↓
	사과 값이 '아주 조금' 올랐을 때 '지불금액'이 얼마나 증가하는지 알고 싶은 것이다.

			∂지불금액 / ∂사과값

	지불 금액을 사과값으로 편미분하면 알 수 있다.
				↓ 계산 그래프 역전파를 이용하면 되는데
	사과 값이 1원이 오르면 최종금액은 2.2이 오른다.




★ 활성화 함수 계층 구현하기
문제 163 ~ 164
	1. Relu 계층을 위한 클래스 생성

		" 0보다 큰 값이 입력되면 그 값을 그대로 출력하고 0이거나 0보다 작은 값이 입력되면 0을
		  출력하는 함수 "

		- 순전파 함수
		- 역전파 함수

	* Relu클래스를 이해하려면 알아야 하는 파이썬 문법 2가지
	
		1. copy의 의미
		2. x[x <= 0]의 의미


		1. copy 테스트

			from copy import copy
			
			a = [1, 2, 3]
			b = copy(a)
			
			print(b)
			a[1] = 6
			print(a)
			print(b)
					
			[1, 2, 3]
			[1, 6, 3]
			[1, 2, 3]


		2. x[x <= 0] 테스트

			import numpy as np

			x = np.array([[1.0, -0.5], [-2.0, 3.0]])
			print(x)
			mask = (x <= 0)
			print(mask)

			[[ 1.  -0.5]
			 [-2.   3. ]]
			[[False  True]
			 [ True False]]

			out = x.copy()
			out[mask] = 0
			print(out)
			
			[[ 1.  0.]
			 [ 0.  3.]]




★ sigmiod 함수를 파이썬으로 구현하기
문제 165 ~ 166
	


★ Affine 계층 ( p 170 )
문제 167 ~ 172
	"신경망의 순전파 때 수행하는 행렬을 내적은 기하학에서는 어파인 변환이라고 합니다.
	 그래서 신경망에서 입력값과 가중치의 내적의 합에 바이어스를 더하는 그 층을 Affine계층이라고
	 해서 구현합니다. "

	"지금까지의 계산 그래프는 노드 사이에 '스칼라값'이 흘렀는데 이에 반해 이번에는 '행렬'이 흐르고
	 있어서 Affine계층 구현이 필요하다"




★ Affine 계층의 계산 그래프
문제 173 ~ 177



☆ 입력값을 1차원으로 하면 안되고 2차원으로 하는 이유는 ?

	import numpy as np
	
	x1 = np.array([[1, 2]])
	print(x1.ndim) # 2
	
	x2 = np.array([1, 2])
	print(x2.ndim) # 1


	1차원
	import numpy as np
	x = np.array([1, 2])
	w = np.array([[1, 3, 5], [2, 4, 6]])
	print(np.dot(x,w))
	#결과값이 1차원이 나옴


	#선생님이 준 코드 (돌려봐라)
	
	import numpy as np
	
	class Affine:
	    def __init__(self, w, b):
	        self.w = w
	        self.b = b
	        self.x = None
	        self.dw = None
	        self.db = None
	
	    def forward(self, x):
	        self.x = x
	        out = np.dot(x, self.w) + self.b
	        return out
	
	    def backward(self, dout):
	        dx = np.dot(dout, self.w.T)
	        self.dw = np.dot(self.x.T, dout)
	        self.db = np.sum(dout, axis=0)
	        return dx
	
	
	# x = np.array([1, 2])   # 오류남
	x = np.array([[1, 2]])
	w = np.array([[1, 3, 5], [2, 4, 6]])
	b = np.array([[1, 1, 1]])
	dout = np.array([[1, 2, 3]])
	
	affine = Affine(w, b)
	print("forward : ", affine.forward(x))
	print("dx : ", affine.backward(dout))
	print("dw : ", affine.dw)
	print("db : ", affine.db)
	#
	# x = np.array([[1,2]])
	# x2 = np.array([1,2])
	# print (x.shape)  (1,2)
	# print (x2.shape) (2,)



★ OrderDict() 딕셔너리의 이해
문제 183 ~ 184
	orderdict는 그냥 Dictionary와는 다르게 입력된 데이터 뿐만 아니라 입력된 순서까지 같아야 동일한
	것으로 판단한다.

	예제 :
		# 일반 딕셔너리 예제

		print('dict : ')
		d1 = {}
		d1['a'], d1['b'], d1['c'], d1['d'], d1['e'] = 'A', 'B', 'C', 'D', 'E'

		
		d2 = {}
		d2['e'], d2['d'], d2['c'], d2['b'], d2['a'] = 'E', 'D', 'C', 'B', 'A'
		
		print(d1) # {'a': 'A', 'b': 'B', 'c': 'C', 'd': 'D', 'e': 'E'}
		print(d2) # {'e': 'E', 'd': 'D', 'c': 'C', 'b': 'B', 'a': 'A'}
		
		print(d1 == d2) # True
		
		

	# OrderDict() 를 테스트
	
	import collections
	
		print('dict : ')
		d1 = collections.OrderedDict()
		d1['a'], d1['b'], d1['c'], d1['d'], d1['e'] = 'A', 'B', 'C', 'D', 'E'
		
		d2 = collections.OrderedDict()
		d2['e'], d2['d'], d2['c'], d2['b'], d2['a'] = 'E', 'D', 'C', 'B', 'A'
		
		print(d1) # OrderedDict([('a', 'A'), ('b', 'B'), ('c', 'C'), ('d', 'D'), ('e', 'E')])
		print(d2) # OrderedDict([('e', 'E'), ('d', 'D'), ('c', 'C'), ('b', 'B'), ('a', 'A')])
		
		print(d1 == d2) # False

		OrderedDict함수로 만든 Dictionary는 순서까지 같아야 같은 데이터로 인식한다.
		순전파 순서의 반대로 역전파가 되어야 하기 때문에 OrderedDict함수를 사용해야 한다.

		순전파 : 입력값 --> Affine1층 --> Relu1 --> Affine2층 --> Relu --> Affine3층
			 --> Lastlayer --> 오차

		역전파 : 1 --> Lastlayer --> Affine3층 --> Relu --> Affine2층 --> Relu --> Affine1층

		※ 순전파의 순서를 반대로(reverse)해서 역전파 될 수 있도록 OrderedDict함수를 신경망
		   코드에 사용해야 한다.




★ 오차 역전파를 이용한 2층 신경망 ( p 182 ~ p 183 )의 코드
문제 185 ~ 186

======================================================================================
저자코드
--------------------------------------------------------------------------------------
# coding: utf-8

import sys, os
sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정
import numpy as np
from common.layers import *
from common.gradient import numerical_gradient
from collections import OrderedDict
import matplotlib.pyplot as plt
from dataset.mnist import load_mnist


class TwoLayerNet:
    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):

        # 가중치 초기화
        self.params = {}
        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)
        self.params['b1'] = np.zeros(hidden_size)
        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)
        self.params['b2'] = np.zeros(output_size)


        # 계층 생성

        self.layers = OrderedDict()
        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])
#         self.layers['sigmoid'] = Sigmoid()
        self.layers['Relu'] = Relu()
        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])
        self.lastLayer = SoftmaxWithLoss()


    def predict(self, x):
        for layer in self.layers.values():
            x = layer.forward(x)

        return x


    # x : 입력 데이터, t : 정답 레이블

    def loss(self, x, t):
        y = self.predict(x)
        return self.lastLayer.forward(y, t)


    def accuracy(self, x, t):
        y = self.predict(x)
        y = np.argmax(y, axis=1)
        if t.ndim != 1: t = np.argmax(t, axis=1)
        accuracy = np.sum(y == t) / float(x.shape[0])
        return accuracy


    # x : 입력 데이터, t : 정답 레이블

    def numerical_gradient(self, x, t):
        loss_W = lambda W: self.loss(x, t)
        grads = {}
        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])
        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])
        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])
        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])
        return grads


    def gradient(self, x, t):
        # forward
        self.loss(x, t)
        # backward
        dout = 1
        dout = self.lastLayer.backward(dout)
        layers = list(self.layers.values())
        layers.reverse()

        for layer in layers:
            dout = layer.backward(dout)


        # 결과 저장

        grads = {}
        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db
        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db

        return grads

# 데이터 읽기

(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)

network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)


# 하이퍼파라미터

iters_num = 10000  # 반복 횟수를 적절히 설정한다.
train_size = x_train.shape[0] # 60000 개
batch_size = 100  # 미니배치 크기
learning_rate = 0.1

train_loss_list = []
train_acc_list = []
test_acc_list = []



# 1에폭당 반복 수

iter_per_epoch = max(train_size / batch_size, 1)



for i in range(iters_num): # 10000

    # 미니배치 획득  # 랜덤으로 100개씩 뽑아서 10000번을 수행하니까 백만번
    batch_mask = np.random.choice(train_size, batch_size) # 100개 씩 뽑아서 10000번 백만번
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]

    # 기울기 계산

    #grad = network.numerical_gradient(x_batch, t_batch)
    grad = network.gradient(x_batch, t_batch)

    # 매개변수 갱신

    for key in ('W1', 'b1', 'W2', 'b2'):
        network.params[key] -= learning_rate * grad[key]

    # 학습 경과 기록

    loss = network.loss(x_batch, t_batch)
    train_loss_list.append(loss) # cost 가 점점 줄어드는것을 보려고

    # 1에폭당 정확도 계산 # 여기는 훈련이 아니라 1에폭 되었을때 정확도만 체크

    if i % iter_per_epoch == 0: # 600 번마다 정확도 쌓는다.

        train_acc = network.accuracy(x_train, t_train)

        test_acc = network.accuracy(x_test, t_test)

        train_acc_list.append(train_acc) # 10000/600 개  16개 # 정확도가 점점 올라감

        test_acc_list.append(test_acc)  # 10000/600 개 16개 # 정확도가 점점 올라감

        print("train acc, test acc | " + str(train_acc) + ", " + str(test_acc))



# 그래프 그리기

markers = {'train': 'o', 'test': 's'}
x = np.arange(len(train_acc_list))
plt.plot(x, train_acc_list, label='train acc')
plt.plot(x, test_acc_list, label='test acc', linestyle='--')
plt.xlabel("epochs")
plt.ylabel("accuracy")
plt.ylim(0, 1.0)
plt.legend(loc='lower right')
plt.show()




##### 3층 신경망 코드 #####
# coding: utf-8

import sys, os
sys.path.append(os.pardir)
import numpy as np
from common.layers import *
from common.gradient import numerical_gradient
from collections import OrderedDict
import matplotlib.pyplot as plt
from dataset.mnist import load_mnist


class TwoLayerNet:
    def __init__(self, input_size, hidden_size1, hidden_size2, output_size1, output_size2, weight_init_std=0.01):
        self.params = {}
        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size1)
        self.params['b1'] = np.zeros(hidden_size1)
        self.params['W2'] = weight_init_std * np.random.randn(hidden_size1, output_size1)
        self.params['b2'] = np.zeros(output_size1)
        self.params['W3'] = weight_init_std * np.random.randn(hidden_size2, output_size2)
        self.params['b3'] = np.zeros(output_size2)
        self.layers = OrderedDict()
        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])
#         self.layers['sigmoid'] = Sigmoid()
        self.layers['Relu1'] = Relu()
        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])
        self.layers['Relu2'] = Relu()
        self.layers['Affine3'] = Affine(self.params['W3'], self.params['b3'])
        self.lastLayer = SoftmaxWithLoss()

    def predict(self, x):
        for layer in self.layers.values():
            x = layer.forward(x)
        return x

    def loss(self, x, t):
        y = self.predict(x)
        return self.lastLayer.forward(y, t)

    def accuracy(self, x, t):
        y = self.predict(x)
        y = np.argmax(y, axis=1)
        if t.ndim != 1: t = np.argmax(t, axis=1)
        accuracy = np.sum(y == t) / float(x.shape[0])
        return accuracy

    def numerical_gradient(self, x, t):
        loss_W = lambda W: self.loss(x, t)
        grads = {}
        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])
        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])
        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])
        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])
        return grads

    def gradient(self, x, t):
        # forward
        self.loss(x, t)
        # backward
        dout = 1
        dout = self.lastLayer.backward(dout)
        layers = list(self.layers.values())
        layers.reverse()
        for layer in layers:
            dout = layer.backward(dout)
        grads = {}
        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db
        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db
        return grads


# 데이터 읽기
(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)
network = TwoLayerNet(input_size=784, hidden_size1=50, hidden_size2=100, output_size1 = 100,  output_size2=10)
iters_num = 10000
train_size = x_train.shape[0]
batch_size = 100  
learning_rate = 0.1
train_loss_list = []
train_acc_list = []
test_acc_list = []
iter_per_epoch = max(train_size / batch_size, 1)
for i in range(iters_num):
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]
    #grad = network.numerical_gradient(x_batch, t_batch)
    grad = network.gradient(x_batch, t_batch)
    for key in ('W1', 'b1', 'W2', 'b2'):
        network.params[key] -= learning_rate * grad[key]
    loss = network.loss(x_batch, t_batch)
    train_loss_list.append(loss)
    if i % iter_per_epoch == 0:
        train_acc = network.accuracy(x_train, t_train)
        test_acc = network.accuracy(x_test, t_test)
        train_acc_list.append(train_acc)
        test_acc_list.append(test_acc)  
        print("train acc, test acc | " + str(train_acc) + ", " + str(test_acc))
markers = {'train': 'o', 'test': 's'}
x = np.arange(len(train_acc_list))
plt.plot(x, train_acc_list, label='train acc')
plt.plot(x, test_acc_list, label='test acc', linestyle='--')
plt.xlabel("epochs")
plt.ylabel("accuracy")
plt.ylim(0, 1.0)
plt.legend(loc='lower right')
plt.show()





	수치미분을 이용한 기울기 :
		- 3층 신경망을 학습 시켜야 한다고 하면 구해야 하는 기울기 ?
		  W1, W2, W3, b1, b2, b3에 대하여 각각 기울기를 구해야 한다.

		  W1 = W1 - 기울기 ( ∂오차함수 / ∂W1 )

		  W1만해도 벌써 784 x 100개의 W1이 있으므로 수치미분을 이용해서 기울기룰 구하려면 시간이
		  상당히 걸린다.


	오차 역전파를 이용한 기울기 :
		──▶ W1 ──▶ Relu1 ──▶ W2 ──▶ Relu2 ──▶ W3 ──▶ Softmax ──▶ Cost func
		       ↓	  ↓	      ↓	 ↓	     ↓		  ↓
		   backward    backward    backward   backward    backward     backward

			∂k    ∂m    ∂g    ∂y    ∂t    ∂E (오차함수)    ∂E
			── x ── x ── x ── x ── x ──		   = ──
			∂W1   ∂k    ∂m    ∂g    ∂y    ∂t		     ∂W1

		   (p 154) chain rule

		※ 위의 연쇄 법칙은 합성함수 미분과 같다.
		   합성함수 ? 두개의 함수를 이어붙여서 한개의 함수로 표현하는것
			x ──▶ f(x) ──▶ g(f(x)) ──▶ z(g(f(x))) ──▶ k(z(g(f(x))))
				  ↑	       ↑		↑		    ↑
				Relu1	      Relu2	      Softmax		 Loss Func

		k(z(g(f(x))))를 미분 ──▶ k(z(g(f(x))))´ * z(g(f(x)))´ * g(f(x))´ * f(x)´

			∂k      ∂k    ∂z    ∂g    ∂f
			──  =  ── x ── x ── x ──
			∂x      ∂z    ∂g    ∂f    ∂x






■ 6장. 신경망의 정확도를 높이기 위한 여러가지 방법들 소개

	* 목차
		1. 고급 경사 감소법들 	─▶ underfitting을 막기 위해
		2. 가중치 초기화값 설정 ─▶ underfitting을 막기 위해
		3. 배치 정규화 		─▶ underfitting을 막기 위해
		4. Dropout 기법 	─▶ overfitting을 막기위해

		5. 기타 방법 (이미지 한장을 회전, 밝기, 반전을 시켜서 여러개의 이미지를 만들어서 학습
			      시키는 방법)
			" 학습이 잘 되도록 데이터를 생성하는 방법 "




□ 1. 고급 경사 감소법들

	1. SGD
	2. Momentum
	3. Adagrade
	4. Adam





★ 1. SGD ( Stochastic Gradient Descent )
	      확률적    경사    감소법

	딥러닝 면접문제 :
		GD(Gradient Descent)의 단점은 무엇인가?
			1. 학습 데이터를 모두 입력해서 한걸음 이동하므로 시간이 많이 걸린다.
				그래서 이 문제를 해결하기 위해서 나온것이 ?
					SGD ( Stochastic Gradient Descent )
					        확률적    경사    감소법

			2. 기울기가 global minima 쪽으로 기울어진 방향이 아니기 때문에 global minima
			   쪽으로 가지 못하고 local minima에 빠지는 단점이 있다.
			그림 6-_-1
			그림 6-_-2

			   그림 6-2 처럼 global minima쪽으로 기울기 방향이 있는게 아니기 때문에
			   local minima에 빠지고 global minima쪽으로 가지 못하는 단점이 있다.

		SGD의 단점을 해결한 경사 감소법 ?




★ 2. momentum(운동량)
문제 188
	기존 SGD : 가중치     ◀─    가중치 - 러닝레이트 * 기울기

	momentum :   W	      ◀─    가중치 + 속도
		   가중치     ◀─    가중치 + 속도
		    속도      ◀─    0.9 * 속도 - 러닝레이트 * 기울기
				      ↑
				   마찰 계수
				      ↑
				물체가 아무런 힘을 받지 않을 때 서서히 하강시키는 역할을 한다.
				물리에서의 지면 마찰이나 공기저항에 해당한다.
				내리막 : 기울기 음수이면 속도가 증가
				오르막 : 기울기 양수이면 속도가 감소

			기울어진 방향으로 물체가 가속되는 관성의 원리를 적용 !

		※ 관성 ──▶ 정지해 있는 물체는 계속 정지해 있으려 하고 운동하는 물체는 계속
			       운동하려는 성질
			(예 : 100미터 달리기 결승점에서 갑자기 멈춰지지 않는 현상)





★ 3. Adagrade 경사 감소법
문제 189
	" Learning Rate(학습률)이 신경망 학습이 되면서 자체적으로 조정이 되는 경사 감소법"

	     

	SGD			   vs		Adagrade
			∂L				     ∂L     ∂L
	W  ◀─ W - n * ──			h  ◀─  h + ── ⊙ ──
			∂W				     ∂W     ∂W
						⊙ 는 행렬의 원소별 곱셈을 의미
								   1	 ∂L
						W  ◀─  W - η * ── x ──
								  √h    ∂W
						1. 처음에는 학습률이 크다가 조금씩 감소
						2. ⊙ 는 행렬의 원소별 곱셈을 의미하는데 h의 원소가
						   각각의 매개변수 원소변화량에 의해 결정된다.
								↓
						"각각의 매개변수 원소가 갱신되는 값이 다르다."
								↓
						"많이 움직인 원소일 수록 누적 h의 값이 크니까 갱신되는
						 정도가 그만큼 감소한다."




★ 4. Adam 경사 감소법
문제 190
	"Momentum의 장점 + Adagrade의 장점을 살린 경사 감소법"
	  (가속도)	      (각 매개변수마다 학습률 조절)

	최근에 딥러닝에 가장 많이 사용하는 최적화 방법

		


===================================================================================================
import sys, os
sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정
import numpy as np
from common.layers import *
from common.gradient import numerical_gradient
from collections import OrderedDict
import matplotlib.pyplot as plt
from dataset.mnist import load_mnist

class SGD:
    def __init__(self, lr = 0.01):
        self.lr = lr
        
    def update(self, params, grads):
        for key in params.keys():
            params[key] -= self.lr * grads[key]

class Momentum:
    def __init__(self, lr=0.01, momentum = 0.9):
        self.lr = lr
        self.momentum = momentum
        self.v = None
        
    def update(self, params, grads):
        if self.v is None:
            self.v = {}
            for key, val in params.items():
                self.v[key] = np.zeros_like(val)
        for key in params.keys():
            self.v[key] = self.momentum*self.v[key] - self.lr*grads[key]
            params[key] += self.v[key]

class Adagrade:
    def __init__(self, lr=0.01):
        self.lr = lr
        self.h = None
        
    def update(self, params, grads):
        if self.h is None:
            self.h = {}
            for key, val in params.items():
                self.h[key] = np.zeros_like(val)
        for key in params.keys():
            self.h[key] += grads[key] * grads[key]
            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)

class Adam:
    """Adam (http://arxiv.org/abs/1412.6980v8)"""
    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):
        self.lr = lr
        self.beta1 = beta1
        self.beta2 = beta2
        self.iter = 0
        self.m = None
        self.v = None
        
    def update(self, params, grads):
        if self.m is None:
            self.m, self.v = {}, {}
            for key, val in params.items():
                self.m[key] = np.zeros_like(val)
                self.v[key] = np.zeros_like(val)
        self.iter += 1
        lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)         
        for key in params.keys():
            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])
            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])
            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)


class TwoLayerNet:
    def __init__(self, input_size, hidden_size1, hidden_size2, output_size1, output_size2, weight_init_std=0.01):
        self.params = {}
        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size1)
        self.params['b1'] = np.zeros(hidden_size1)
        self.params['W2'] = weight_init_std * np.random.randn(hidden_size1, output_size1)
        self.params['b2'] = np.zeros(output_size1)
        self.params['W3'] = weight_init_std * np.random.randn(hidden_size2, output_size2)
        self.params['b3'] = np.zeros(output_size2)
        self.layers = OrderedDict()
        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])
        self.layers['Relu1'] = Relu()
        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])
        self.layers['Relu2'] = Relu()
        self.layers['Affine3'] = Affine(self.params['W3'], self.params['b3'])
        self.lastLayer = SoftmaxWithLoss()

    def predict(self, x):
        for layer in self.layers.values():
            x = layer.forward(x)
        return x

    def loss(self, x, t):
        y = self.predict(x)
        return self.lastLayer.forward(y, t)


    def accuracy(self, x, t):
        y = self.predict(x)
        y = np.argmax(y, axis=1)
        if t.ndim != 1: t = np.argmax(t, axis=1)
        accuracy = np.sum(y == t) / float(x.shape[0])
        return accuracy

    def numerical_gradient(self, x, t):
        loss_W = lambda W: self.loss(x, t)
        grads = {}
        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])
        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])
        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])
        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])
        grads['W3'] = numerical_gradient(loss_W, self.params['W3'])
        grads['b3'] = numerical_gradient(loss_W, self.params['b3'])
        return grads


    def gradient(self, x, t):
        self.loss(x, t)
        dout = 1
        dout = self.lastLayer.backward(dout)
        layers = list(self.layers.values())
        layers.reverse()
        for layer in layers:
            dout = layer.backward(dout)
        grads = {}
        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db
        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db
        grads['W3'], grads['b3'] = self.layers['Affine3'].dW, self.layers['Affine3'].db
        return grads



★ 2. 가중치 초기화 값 설정
문제 191
	"가중치 초기값을 적절히 설정하면 각 층의 활성화 값의 분포가 적당히 퍼지는 효과가 발생한다. 적당히
	 펴지게 되면 학습이 잘되고 정확도가 높아진다.


	1. 표준편차가 작을수록 data가 평균에 가깝게 분포
	   (시험문제 쉬우면 학생들 점수가 평균에 가깝다)
		구현 예 : 0.01 * np.random.randn(10, 100)
	
		그림 6-_-7



	2. 표준편차가 클수록 data가 더 많이 흩어져 있다.
	   (시험문제가 어려워지면 아주 잘하는 학생들과 아주 못하는 학생들로 나누어 진다.)	
		구현 예 : 1 * np.random.randn(10, 100)
	
		그림 6-_-8
	
		그림 6-13이 바람직한 가중치 초기화 값의 구성이다.
	
		바람직한 가중치 초기화값 구성을 위한 방법 2가지
	
			1. Xavier(사비에르) 초기값 선정
				표준편차가 √1/n 인 정규분포로 초기화 한다
				( n은 앞층의 노드수 )
				구현 예 : 1/np.sqrt(50) * np.random.randn(10, 100)

			2. He 초기값 선정
				표준편차가 √2/n 인 정규분포로 초기화 한다.
				( n은 앞층의 노드수 )
				구현 예 : np.sqrt(2/50) * np.random.randn(10, 100)




★ 오버피팅을 억제하는 방법 2가지
문제 192
	1. 드롭아웃 (dropout)
	2. 가중치 감소 (Weight Decay)



☆ 드롭아웃(dropout) (p 219)

	"오버피팅을 억제하기위해서 뉴런을 임의로 삭제하면서 학습시키는 방법"


	 * Dropout class 구현 코드  
	
		class Dropout:
		    """
		    http://arxiv.org/abs/1207.0580
		    """
		    def __init__(self, dropout_ratio=0.15):
		        self.dropout_ratio = dropout_ratio
		        self.mask = None
		
		    def forward(self, x, train_flg=True):
		        if train_flg:
		            self.mask = np.random.rand(*x.shape) > self.dropout_ratio
		            return x * self.mask
		        else:
		            return x * (1.0 - self.dropout_ratio)
		
		    def backward(self, dout):
		        return dout * self.mask
	
	   * dropout 클래스의 키가 되는 부분
	
	   self.mask = np.random.rand(*x.shape) > self.dropout_ratio
	                                                    ↓
	                                                   0.15  
		100% 중에 85%의 노드만 남겨두고 15%노드를 삭제하겠다.
		사진의 일부를 없애면서 학습을 시킨다.
		너무 많이 삭제하면 무슨 문제가 생길까?

			"정확도가 떨어진다."

--------------------------------------------------------------------------------------------
class TwoLayerNet:
    def __init__(self, input_size, hidden_size1, hidden_size2, output_size1, output_size2, \
                 weight_init_std=np.sqrt(2/50)):
        self.params = {}
        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size1)
        self.params['b1'] = np.zeros(hidden_size1)
        self.params['W2'] = weight_init_std * np.random.randn(hidden_size1, output_size1)
        self.params['b2'] = np.zeros(output_size1)
        self.params['W3'] = weight_init_std * np.random.randn(hidden_size2, output_size2)
        self.params['b3'] = np.zeros(output_size2)
        self.layers = OrderedDict()
        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])
        self.layers['Relu1'] = Relu()
        self.layers['dropout1'] = Dropout()
        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])
        self.layers['Relu2'] = Relu()
        self.layers['dropout1'] = Dropout()
        self.layers['Affine3'] = Affine(self.params['W3'], self.params['b3'])
        self.lastLayer = SoftmaxWithLoss()

    def predict(self, x):
        for layer in self.layers.values():
            x = layer.forward(x)
        return x

    def loss(self, x, t):
        y = self.predict(x)
        return self.lastLayer.forward(y, t)


    def accuracy(self, x, t):
        y = self.predict(x)
        y = np.argmax(y, axis=1)
        if t.ndim != 1: t = np.argmax(t, axis=1)
        accuracy = np.sum(y == t) / float(x.shape[0])
        return accuracy

    def numerical_gradient(self, x, t):
        loss_W = lambda W: self.loss(x, t)
        grads = {}
        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])
        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])
        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])
        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])
        grads['W3'] = numerical_gradient(loss_W, self.params['W3'])
        grads['b3'] = numerical_gradient(loss_W, self.params['b3'])
        return grads


    def gradient(self, x, t):
        self.loss(x, t)
        dout = 1
        dout = self.lastLayer.backward(dout)
        layers = list(self.layers.values())
        layers.reverse()
        for layer in layers:
            dout = layer.backward(dout)
        grads = {}
        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db
        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db
        grads['W3'], grads['b3'] = self.layers['Affine3'].dW, self.layers['Affine3'].db
        return grads
    

(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)
network = TwoLayerNet(input_size=784, hidden_size1=50, hidden_size2=100, output_size1 = 100, \
 output_size2=10)
iters_num = 10000
train_size = x_train.shape[0] # 60000 개
batch_size = 100
learning_rate = 0.1
train_loss_list = []
train_acc_list = []
test_acc_list = []

iter_per_epoch = max(train_size / batch_size, 1)

optimizer = Adam()

for i in range(iters_num):
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]
    grad = network.gradient(x_batch, t_batch)
    for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):
        grads = network.gradient(x_batch, t_batch)
        params = network.params
        optimizer.update(params,grads)
    loss = network.loss(x_batch, t_batch)
    train_loss_list.append(loss)
    if i % iter_per_epoch == 0:
        train_acc = network.accuracy(x_train, t_train)
        test_acc = network.accuracy(x_test, t_test)
        train_acc_list.append(train_acc)
        test_acc_list.append(test_acc)
        print("train acc, test acc | " + str(train_acc) + ", " + str(test_acc))

markers = {'train': 'o', 'test': 's'}
x = np.arange(len(train_acc_list))
plt.plot(x, train_acc_list, label='train acc')
plt.plot(x, test_acc_list, label='test acc', linestyle='--')
plt.xlabel("epochs")
plt.ylabel("accuracy")
plt.ylim(0, 1.0)
plt.legend(loc='lower right')
plt.show()

train acc, test acc | 0.985533333333, 0.9654
--------------------------------------------------------------------------------------------



	2. 가중치 감소 (Weight Decay)

		"학습 과정에서 큰 가중치에 대해서는 그에 상응하는 큰 패널티를 부여하여 오버피팅을
		 억제하는 방법"

	   예 : 고양이와 개사진을 구분하는 신경망

		입력층			은닉층
		  ○  ──────▶  귀가 있어요   	     ──────▶  가중치 출력
	고양이    ○  ──────▶  꼬리가 있어요 	     ──────▶  아주 큰 가중치 출력
		  ○  ──────▶  집게발을 가지고 있어요 ──────▶  가중치 출력
		  ○  ──────▶  장난스럽게 보여요      ──────▶  가중치 출력

		cf) 위와 같이 학습이 되면 꼬리가 없는 고양이 사진을 입력하면 고양이가 아니라고 판단을
		    한다.
			↓
			그래서 이를 해결하는 방법이 ?

		큰 패널티는 오차함수가 준다. 오차함수가 오차를 역전파 시킬 때 패널티를 부여하게끔
		하는데 오차함수 코드가 아래와 같다.






☆ 가중치 decay 코드 분석
                                    1
	 "모든 가중치의 각각의 손실 함수에 --- x 람다 x w^2  을 더한다.
	                                    2
	
	 1. 오차 함수의 코드에서 수정해야 할 부분
	
	  weight_decay += 0.5 * self.weight_decay_lambda * np.sum(W**2)
	
	  return  self.lastLayer.forward(y,t) + wight_decay  
	                      ↓
	                     오차
	
	 2. gradient  Descent 업데이트 과정에서 그동안 오차 역전파법에
	    따른 결과에 정규화 항을 미분한 값에 람다 * 가중치를 더한다.
	
	 코드예제:
	
	           기존 코드  : grad['W1'] = 미분값
	         변경된 코드  : grad['W1'] = 미분값 + 람다 * 현재의 가중치




★ 배치 정규화 (p210)
문제 193 ~ 194
	" batch normalization "
	  batch 단위로 가중치의 값을 정규화 하겠다.



    통계 분석에서 중요한 2가지

	1. 표준화 : 평균을 기준으로 얼마나 떨어져 있는지 나타내는 값
		    2개 이상의 대상이 단위가 다를때 대상 데이터를 같은 기준으로 볼 수 있게 해준다.

	예: 키와 몸무게 -> 키와 몸무게는 단위가 다른데 우리반 학생들의 키를 0~1
			   사이의 숫자로 변경하고 몸무게도 0~1사이의 숫자로 변경하면
			   키와 몸무게 간의 어떤 관계가 있는지 분석을 해볼 수 있다.

		177(키)		84(몸무게)

	수식 : (요소값 - 평균) / 표준편차



2. 정규화(Normalization)

	  정규화는 전체 구간을 0~100 으로 설정하여 데이터를 관찰하는 방법이다.
	
	  이 방법은 데이터 군에서 특정 데이터가 가지는 위치를 볼 때 사용한다.
	
	  예 : 우리반에서 내 몸무게 77 kg 이면 0~100 사이에 몇에 해당 되는지 파악이 된다.

	  수식 : (요소값 - 최소값) / (최대값 – 최소값)






★ 배치 정규화란 ?

	앞에서는 가중치 초기화 값을 적절히 설정하면 각 층의 활성화 값의 분포가 적당히 퍼지는 효과를 보았다.

	아주 공부를 잘하거나 아주 공부를 못하는 학생들이 있으면 별로 바람직하지 못해서 신경망 학습이
	잘 안된다.

	또 평균에 다 몰려있어도 시험문제가 너무 쉬운 것이기 때문에 바람직 하지 못하다.

	그래서 가중치 초기화 할때 Xavier 나 He 를 써서 가중치 값들을 0~1 사이의 숫자로 적절히 분포시키는
	작업을 했는데
	그런데 문제는 처음에는 데이터가 적절히 분포가 되었다가 점점 학습이 진행되면서 다시 데이터의 분포가
	평균에 몰리거나 아니면 아주 잘하거나 아주 못하는 현상이 발생하게 된다.

	즉 학습이 진행될수록 분포가 틀어지는 현상이 생긴다.

	그래서 배치 정규화를 통해서 각 층의 활성화 값이 적당히 분포되도록 강제로 조정할 필요가 있는데
	그게 배치 정규화이다.

	배치 정규화를 신경망에 적절히 삽입해주면 된다.
	배치 정규화는 활성화 함수 앞이나 뒤쪽에 입력할 수 있지만 대개는 활성화 함수 앞에 사용한다.






★ 배치 정규화 공식

	배치 정규화는 기본적으로 데이터 분포가 평균 0, 분산이 1 이 되도록 정규화한다.
	사이즈가 m 인 미니배치에 대하여 평균과 분산을 구한다.
	그리고 이 입력 데이터를 평균이 0, 분산이 1이 되도록 정규화를 한다.




★ 배치 정규화의 장점 (p 210 )
문제 195
	1. 학습을 빨리 진행할 수 있다.		---> underfitting 해결
	2. 초기값에 크게 의존하지 않아도 된다. ---> underfitting 해결
	3. 오버피팅을 억제한다. ---> overfitting 해결

	이 입력 데이터를 평균이 0, 분산이 1 이 되도록 정규화를 하는데 그런데 신경망은 비선형을 가지고
	있어야 표현력이 커져서 복잡한 함수를 표현할 수 있는데 위와 같이 평균이 0, 분산을 1로 고정시키는
	것은 활성화 함수의 비선형을 없애버릴 수 있다.

	그래서 아래와 같이 감마와 베타를 이용해서 새로운 값을 내놓는다.

	감마가 데이터의 확대(분포) 를 조정하는 것이고 베타가 데이터의 이동을 조정하는 것인데 학습이
	되면서 감마와 베타가 자동으로 신경망에 맞게 조정이 된다.




★ 배치 정규화의 계산 그래프

	μ(평균)
	σ(분산)






★ 배치 정규화의 역전파
문제 196
https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html
참고




■ 7장. CNN (Convolution Neural Network)
	         합성      곱    신경망

	합성 곱 신경망 ?

		" Convolution층과 pooling층을 포함하는 신경망 "


	기존 신경망과의 차이 ?
		- 기존 방법 : Affine ──▶ Relu
		- CNN 	    : Conv   ──▶ Relu ──▶ Pooling ──▶ 완전 연결 계층






★ 기존에 구현했던 완전 연결 계층의 문제점
문제 197 ~ 203
	"데이터의 형상이 무시된다"

	그림으로 설명 (카페 확인 : http://cafe.daum.net/oracleoracle/SY3L/175 )


	* CNN을 이용하지 않은 기존 층의 문제점
		필기체 ──▶ 28 x 28 = 784의 1차원 데이터로 변경해서 784개의 데이터를 첫 Affine계층에
			      입력한게 기존 방법이다.
					↓
			형상을 무시하고 무든 입력 데이터를 동등한 뉴런으로 취급하기 때문에 이미지가 갖는
			본질적인 패턴을 읽지 못한다.
					↓
			그래서 합성곱 계층이 필요하다.

		결국 원본 이미지에서 조금만 모양이 달라져도 같은 이미지로 인식하지 못하는 문제를 합성곱이
		해결해 줄 수 있다.

		어떻게 해결하는가?	원본이미지를 가지고 여러개의 feature map을 만들어서 완전연결계층에
					입력해서 분류를 한다.



	* 합성곱 예제 :
		  0  2  0  2  0
		  2  0  2  0  2        1  1  1
		  2  0  0  0  2    ◎  0  0  0
		  2  0  0  0  2        0  0  0  
		  0  2  2  2  0
		  0  0  2  0  0
		
		   하트 이미지        필터

	* 합성곱 연산을 컴퓨터로 구현하는 방법

		1 2 3 0
		0 1 2 3	     ◎    2 0 1    =   15  16
		3 0 1 2            0 1 2         6  15
		2 3 0 1            1 0 2

		입력 데이터 	   필터 	 결과
		(4, 4) 		   (3, 3) 	 (2, 2)

		순서 1 :
			1 2 3        2 0 1		2 0 3
			0 1 2	◎   0 1 2	=       0 1 4    =   15
			3 0 1	     1 0 2		3 0 2
	
		순서 2 :
			2 3 0        2 0 1		4 0 0
			1 2 3	◎   0 1 2	=       0 2 6    =   16
			0 1 2	     1 0 2		0 0 4
	
		순서 3 :
			0 1 2        2 0 1		0 0 2
			3 0 1	◎   0 1 2	=       0 0 2    =   6
			2 3 0	     1 0 2		2 0 0
	
		순서 4 :
			1 2 3        2 0 1		2 0 3
			0 1 2	◎   0 1 2	=       0 1 4    =   15
			3 0 1	     1 0 2		3 0 2




★ Padding ( p 232 )
문제 204 ~ 207
	그림
	
	"합성곱 연산을 수행하기 전에 입력 데이터 주변을 특정 값으로 채워 늘리는 것을 말한다."

	- 패딩이 필요한 이유 ?
		패딩을 하지 않을 경우 data의 공간크기는 합성곱 계층이 지날때 마다 작아지게 되므로 가장
		자리 정보들이 사라지게 되는 문제가 발생하기 때문에 패딩을 사용한다.

		4x4  ◎ 3x3 = 2x2

		conv ──▶ polling ──▶ conv ──▶ polling




■ 3차원 합성곱 ( p 235 )
문제 208 ~ 228
	이미지의 색깔이 보통은 흑백이 아니라 RGB 컬러이므로 RGB(Red, Green, Blue)컬러에 합성곱을 해야한다.
	




★ 블록으로 생각하기 ( p 237 )
문제 229
	3차원 합성곱 연산은 데이터와 필터를 직육면체 블럭이라고 생각하면 쉽다.
	블럭은 3차원 직육면체(채널, 높이, 너비)로 구성됨
			       C      H     W


	필터의 채널 (FC), 필터의 높이(FH), 필터의 너비(FW)
	설명 : 아이린 사진 한장(RGB) 를 RGB필터 1개로 합성곱해서 2차원 출력행렬(feature map) 1장을
	       출력한 그림

					↓

		위의 그림은 feature map이 한개가 나오고 있는데 실제로는 아이린 사진 한장에 대해서
		여러개의 feature map이 필요하다.
		그러면 여러개의 feature map을 출력하려면 어떻게 해야 하는가?

					↓

			     "Filter의 갯수를 늘린다."

					↓
				




★ 배치처리 ( p 239 )

	위의 그림은 이미지를 1장씩 넣어서 학습시키므로 학습속도가 느리다. 그래서 여러장의 이미지를
	한번에 입력해서 학습시키면 (mini batch)아래의 그림이 된다.
				↓
			

	아이린 사진 10장 ──▶  필터의 갯수 100개면 ──▶  (10, 100, 5, 5)
								   ↓
						아이린 사진 1장당 만들어지는 100개의 feature map이
						10개가 생성됨

	그러면 결국 합성곱 계층을 구현할 때 흘러가는 행렬이 4차원 행렬이 흘러가면서 연산이 될 것인데
	그러면 문제가 연산속도가 느리므로 행렬 계산을 할 때 행렬 연산을 빠르게 하려면 4차원이 아니라
	2차원으로 차원축소가 필요하다.
	그래서 필요한 함수 ?
		"im2col 함수"  ◀── p 234
		     ↓
		4차원 ──▶ 2차원




★  im2col 함수의 역할
문제 230 ~ 232



★ 필터 RGB행렬을 2차원으로 푸는 방법
문제 233 ~ 240
	1. 원본 이미지를 필터 사이즈에 맞게 2차원으로 변경한 행렬
		( 10, 3, 7, 7 ) ─────▶ ( 90, 75 )
				im2col 함수

	2. 4차원 필터 행렬을 2차원으로 변경
		(10, 3, 5, 5 ) ─────▶ (75, 10)
			reshape 함수의 -1을 이용
 ★ convolution 클래스내에서 일어나는 일

	  1. 원본이미지를 im2col로 2차원 행렬로 변경한다.
	  2. filter 를 reshape 의 -1 옵션을 이용해서  2차원 행렬로 변경한다.
	  3. 두개의 2차원 행렬을 내적한다.
	  4. 내적한 결과 2차원 행렬을 다시 4차원으로 변경한다.

	convolution층의 역할?  이미지의 특징을 추출하는 feature map을 여러개 만든다.






★ 풀링(pooling) 층의 역할

	convolution층이 이미지의 특징을 잡아내는 역할을 한다면 pooling층은 이미지를 선명하게 만드는
	역할을 한다.
	"말 그대로 출력값에서 일부분만 취하는 기능"

	convolution층이 이렇게 저렇게 망쳐놓은 그림들을 가지고 feature map이미지의 각 부분에서 대표들을
	뽑아 사이즈가 작은 이미지로 만드는 것이다.
	마치 사진을 축소하면 해상도가 좋아지는 듯한 효과와 비슷하다.


	* 풀링(pooling)의 종류 3가지
		1. 최대풀링 : 컨볼루션 데이터에서 가장 큰 값을 대표값으로 선정
			
		2. 평균풀링 : 컨볼루션 데이터에서 모든값의 평균값을 대표값으로 선정
		3. 확률적 풀링 : 컨볼루션 데이터에서 임의 확률로 한개를 선정




★ CNN구현하기 (p 250)




■ 문제 모음

문제 1. 아래의 행렬을 numpy로 만드시오 !

	1 2
	3 4

	import numpy as np
	
	a = np.array( [[1, 2], [3, 4]] )
	print(a)

	[[1 2]
	 [3 4]]


문제 2. 위의 a 행렬의 각 요소에 5를 더한 값을 출력하시오 !

	import numpy as np
	
	a = np.array( [[1, 2], [3, 4]] )
	
	print(a+5)

	[[6 7]
	 [8 9]]



문제 3. 아래의 배열의 원소들의 평균값을 출력하시오 !

	import numpy as np
	
	a = np.array( [1, 2, 3, 4, 5, 5, 7, 10, 13, 18, 21])
	print( np.mean(a))
	
	8.09090909091


	※ 예제
		중앙값   : print( np.median(a) )
		최대값   : print( np.max(a) )
		최소값   : print( np.min(a) )
		표준편차 : print( np.std(a) )
		분산     : print( np.var(a) )




문제 4. 아래의 행렬식을 numpy로 구현하시오 !

	      1 3 7        0 0 5
	              +            =  ?
	      1 0 0        7 5 0
	
	import numpy as np
	
	a = np.array( [ [1, 3, 7], [1, 0, 0] ] )
	b = np.array( [ [0, 0, 5], [7, 5, 0] ] )
	print(a + b)

	[[ 1  3 12]
	 [ 8  5  0]]



문제 5. 위에서 만든 x와 y가 각각 몇행 몇열인지를 38페이지를 참고해서 알아보시오 !

	print( x.shape )
	print( y.shape )
	
	(3,)		# 1행 3열
	(3,)		# 1행 3열




문제 6. 아래의 덧셈 행렬의 결과를 파이썬으로 구현하시오 !

	     1 2     10  20
	          +          =  ?
	     3 4     30  40
	
	import numpy as np
	x = np.array( [ [1,2], [3,4] ])
	y = np.array( [ [10,20], [30,40] ])
	print(x + y)
	
	[[11 22]
	 [33 44]]



문제 7. 문제 6번의 덧셈 행렬을 numpy를 이용하지 않고 파이썬 코드로만 수행하시오 !

	     1 2     10  20
	          +          =  ?
	     3 4     30  40
	
	a = [ [1, 2], [3, 4] ]
	b = [ [10, 20], [30, 40] ]
	
	for i in range(len(a)):
	    for j in range(len(a[i])):
	        print(a[i][j] + b[i][j])
	
	11
	22
	33
	44


문제 8. 아래의 행렬 곱을 numpy로 구현하는데 브로드 캐스트가 되는지 확인하시오 !

	그림 (1.5.5_브로드캐스트_그림1_1.png)

	import numpy as np
	a = np.array( [[1,2], [3,4]] )
	b = np.array( [10] )
	
	print( a * b )

	[[10 20]
	 [30 40]]



문제 9. 아래의 그림을 numpy로 구현하시오 ! 브로드캐스트가 되는지 확인하시오 !

	그림 ( 1.5.5_브로드캐스트_그림1_2 )

	import numpy as np
	a = np.array( [[1, 2], [3, 4]] )
	b = np.array( [10, 20] )
	
	print( a * b )
	
	[[10 40]
	 [30 80]]




문제 10. 문제 9번을 numpy를 이용하지 말고 구현하시오 !

	a = [[1, 2], [3, 4]]
	b = [10, 20]
	
	for i in range(len(a)):
	    for j in range(len(a[i])):
	        print(a[i][j] * b[j])
	
	10
	40
	30
	80




문제 11. 아래의 행렬식을 만들고 아래의 행렬의 요소에서 15이상인 것만 출력하시오 !

	   51  55
	   14  19
	    0   4
	
	a = [[51, 55], [14, 19], [0, 4]]
	
	for i in range(len(a)):
	    for j in range(len(a[i])):
	        if a[i][j] >= 15:
	            print(a[i][j])




문제 12. 문제 11번을 numpy로 구현하시오 !

	import numpy as np
	
	a = np.array( [ [51, 55], [14, 19], [0, 4] ] )

	print( a >= 15 )
	print( a[a>=15] )
	
	[[ True  True]
	 [False  True]
	 [False False]]

	[51 55 19]



문제 13. 위의 그래프에 grid(격자)를 추가하시오 !

	import matplotlib.pyplot as plt
	import numpy as np
	
	t = np.arange(0, 12, 0.01)
	print(t)
	
	plt.plot(t)
	plt.grid()
	plt.show()
	
	[  0.00000000e+00   1.00000000e-02   2.00000000e-02 ...,   1.19700000e+01
	   1.19800000e+01   1.19900000e+01]
	



문제 14. 아래의 그래프를 그리시오 !

	import matplotlib.pyplot as plt
	plt.plot([6,4,2,0,2,4,6], color = 'red')
	plt.grid()
	plt.show()



문제 15. 아래의 그래프를 그리시오 !

	import matplotlib.pyplot as plt
	
	plt.plot([6,8,8,6,8,8,6], color = 'red')
	plt.grid()
	plt.show()



문제 16. 문제 14번 코드와 문제 15번 코드를 조합해서 아래의 그래프를 그리시오 !

	
	import matplotlib.pyplot as plt
	
	plt.plot([6,4,2,0,2,4,6], color = 'red')
	plt.plot([6,8,8,6,8,8,6], color = 'red')
	plt.show()




문제 17. 위의 그래프에 제목을 아래와 같이 붙이시오 !

     제목 : 재혁이를 향한 우용이의 마음

	import matplotlib.pyplot as plt
	from matplotlib import font_manager, rc
	    
	# 한글 폰트 설정
	font_name = font_manager.FontProperties(fname="C:/Windows/Fonts/H2PORM.TTF").get_name()
	rc('font', family=font_name)
	
	plt.figure()
	plt.plot([6, 4, 2, 0, 2, 4, 6], color = 'red')
	plt.plot([6, 8, 8, 6, 8, 8, 6], color = 'red')
	plt.grid()
	plt.title('재혁이를 향한 우용이의 마음')
	plt.show()
	

문제 18. 아래의 numpy배열로 산포도 그래프를 그리사오 !

	x = np.array( [ 0,1,2,3,4,5,6,7,8,9 ] )
	y = np.array( [ 9,8,7,9,8,3,2,4,3,4 ] )
	
	import numpy as np
	import matplotlib.pyplot as plt
	
	x = np.array( [ 0,1,2,3,4,5,6,7,8,9 ] )
	y = np.array( [ 9,8,7,9,8,3,2,4,3,4 ] )
	
	plt.scatter(x,y)
	plt.show()
	


문제 19. 위의 그래프를 라인그래프로 그리시오 !

	import numpy as np
	import matplotlib.pyplot as plt
	
	x = np.array( [ 0,1,2,3,4,5,6,7,8,9 ] )
	y = np.array( [ 9,8,7,9,8,3,2,4,3,4 ] )
	plt.plot(x,y)
	plt.show()
	

문제 20. 위의 plot 그래프에 x축의 라벨을 '월' 이라고 하고 y축의 라벨을 '아파트 매매 가격'이라고 하고 제목을
	 '아파트 매매가격 변동 추이' 라고 하시오 !

	import numpy as np
	import matplotlib.pyplot as plt
	from matplotlib import font_manager, rc

	font_name = font_manager.FontProperties(fname="C:/Windows/Fonts/H2PORM.TTF").get_name()
	rc('font', family=font_name)
	
	x = np.array( [ 0,1,2,3,4,5,6,7,8,9 ] )
	y = np.array( [ 9,8,7,9,8,3,2,4,3,4 ] )
	
	plt.plot(x,y)
	plt.xlabel('월')
	plt.ylabel('아파트 매매가격')
	plt.title('아파트 매매가격 변동 추이')
	plt.show()
	



문제 21. 창업건수.csv 파일을 이용해서 치킨집 년도별 창업건수를 가지고 라인그래프를 그리시오 !

	import numpy as np
	from matplotlib import pyplot as plt
	from matplotlib import font_manager, rc
	    
	# 한글 폰트 설정
	font_name = font_manager.FontProperties (fname="C:/Windows/Fonts/H2PORM.TTF").get_name()
	rc('font', family=font_name)
	
	
	chi = np.loadtxt('c:\\창업건수.csv', skiprows=1,
	                  unpack=True, delimiter=',')
	
	# 설명 : unpack = False 면  csv 파일을 그대로 읽어오는것이고
	#        unpack = True 면 csv 파일을 pivot 해서 읽어오는것이다
	
	print( chi )
	
	x = chi[0]
	y = chi[4]
	
	plt.plot(x,y)
	plt.xlabel('년도')
	plt.ylabel('치킨집 창업건수')
	plt.title('년도별 치킨집 창업 현황')
	plt.show()

	[[ 2005.  2006.  2007.  2008.  2009.  2010.  2011.  2012.  2013.  2014.]
	 [ 2196.  2028.  1802.  1691.  1826.  1798.  1688.  1767.  1965.  1980.]
	 [ 1034.   950.  1036.  1127.  1086.  1105.  1199.  1183.  1432.  1870.]
	 [  540.   577.   620.   561.   645.   669.   736.   753.   839.  1095.]
	 [  530.   525.   507.   543.   711.   865.   837.   986.   954.  1193.]
	 [  454.   483.   575.   772.   845.  1291.  1671.  1847.  2287.  3053.]
	 [ 5994.  5504.  6148.  6036.  6577.  6689.  6900.  7082.  7708.  9772.]
	 [  635.   591.   544.   525.   627.   553.   638.   687.   769.  1272.]]
	


문제 22. 치킨집 폐업 현황을 라인그래프로 시각화 하시오 !

	import numpy as np
	from matplotlib import pyplot as plt
	from matplotlib import font_manager, rc
	    
	font_name = font_manager.FontProperties (fname="C:/Windows/Fonts/H2PORM.TTF").get_name()
	rc('font', family=font_name)

	chi = np.loadtxt('c:\\폐업건수.csv', skiprows=1,
	                  unpack=True, delimiter=',')
	print( chi )
	
	x = chi[0]
	y = chi[4]
	
	plt.plot(x,y)
	plt.xlabel('년도')
	plt.ylabel('치킨집 창업건수')
	plt.title('년도별 치킨집 창업 현황')
	plt.show()
	




문제 23. 위의 두개의 그래프를 겹치게 해서 출력하시오 !

	import numpy as np
	from matplotlib import pyplot as plt
	from matplotlib import font_manager, rc
	    
	font_name = font_manager.FontProperties (fname="C:/Windows/Fonts/H2PORM.TTF").get_name()
	rc('font', family=font_name)
	
	
	chi1 = np.loadtxt('c:\\창업건수.csv', skiprows=1,
	                  unpack=True, delimiter=',')
	
	chi2 = np.loadtxt('c:\\폐업건수.csv', skiprows=1,
	                  unpack=True, delimiter=',')
	
	x = chi1[0]
	y = chi1[4]
	
	x1 = chi2[0]
	y1 = chi2[4]
	
	plt.plot(x,y, label = '치킨집 창업건수')
	plt.plot(x1,y1, label = '치킨집 폐업건수')
	plt.xlabel('년도')
	plt.title('년도별 치킨집 창업 현황')
	plt.legend()
	plt.show()
	


문제 24. 책 44페이지의 레나 사진을 파이썬에서 구현하시오 !

	import numpy as np
	from matplotlib import pyplot as plt
	from matplotlib.image import imread
	
	img = imread('c:\lena.png')
	
	plt.imshow(img)
	plt.show()

	



문제 25. (오늘의 마지막 문제) 한식집의 창업과 폐업 건수를 라인그래프로 시각화 하시오 !

	import numpy as np
	from matplotlib import pyplot as plt
	from matplotlib import font_manager, rc
	    
	font_name = font_manager.FontProperties (fname="C:/Windows/Fonts/H2PORM.TTF").get_name()
	rc('font', family=font_name)
	
	
	chi1 = np.loadtxt('c:\\창업건수.csv', skiprows=1,
	                  unpack=True, delimiter=',')
	
	chi2 = np.loadtxt('c:\\폐업건수.csv', skiprows=1,
	                  unpack=True, delimiter=',')
	
	x = chi1[0]
	y = chi1[6]
	
	x1 = chi2[0]
	y1 = chi2[6]
	
	plt.plot(x,y, label = '한식집 창업건수')
	plt.plot(x1,y1, label = '한식집 폐업건수')
	plt.xlabel('년도')
	plt.title('년도별 치킨집 창업 현황')
	plt.legend()
	plt.show()
	



문제 26. and게이트 퍼셉트론을 구현하기 위해서 입력값과 target그리고 가중치를 아래와 같이 numpy로 만드시오 !

	입력값 : [0, 0], [0, 1], [1, 0] [1, 1]

	Target : [0], [0], [0], [1]

	가중치 : [0.4], [0.35], [0.05]

	import numpy as np
	inputs = np.array( [ [0, 0], [0, 1], [1, 0], [1, 1] ] )
	target = np.array( [[0], [0], [0], [1]] )
	w = np.array ( [[0.4], [0.35], [0.05]] )
	print(inputs)
	print(target)
	print(w)
	
	[[0 0]
	 [0 1]
	 [1 0]
	 [1 1]]
	[[0]
	 [0]
	 [0]
	 [1]]
	[[ 0.4 ]
	 [ 0.35]
	 [ 0.05]]




문제 27. 아래와 같은 활성화 함수를 생성하시오 !

	def active_function(k):
	    if k > 0:
	        return 0
	    elif k <= 0:
	        return 1    

	print(active_function(-0.2) )
	print(active_function(0.4))
	print(active_function(0))




문제 28. 아래의 numpy array로 만든 입력값(x)와 가중치(w)의 행렬 곱을 구하시오 !

	import numpy as np
	x = np.array( [0,1] )
	w = np.array( [0.5,0.5] )
	
	print(x * w)

	[ 0.   0.5]



문제 29. 위에서 출력된 두개의 원소의 합을 출력하시오 !

	import numpy as np
	x = np.array( [0,1] )
	w = np.array( [0.5,0.5] )
	
	print(np.sum(x*w))

	0.5


문제 30. 아래의 입력값(x)과 가중치(w) 곱의 합을 출력하시오 !

	import numpy as np
	x = np.array( [ [-1, 0, 0], [-1, 0, 1], [-1, 1, 0], [-1, 1, 1] ] )
	target = np.array( [[0], [0], [0], [1]] )
	w = np.array ( [0.4, 0.35, 0.05] )
	x0 = np.array( [-1] )
	
	def and_sum(x,w):
	    xx = x*w
	    return np.round(sum(xx.T),decimals=2)
	    #return np.round(np.sum(xx, axis=1),decimals=2)   # axis=1 : 행 끼리 합을 구할때
							      # axis=0 : 열 끼리 합을 구할때
	print(and_sum(x,w))

	[-0.4  -0.35 -0.05 -0.  ]




문제 32. 위에서 나온 결과를 target값의 차이를 출력하는데 결과가 아래와 같이 출력 되게끔 코드를 작성하시오 !

	import numpy as np
	x = np.array( [ [-1, 0, 0], [-1, 0, 1], [-1, 1, 0], [-1, 1, 1] ] )
	target = np.array( [0, 0, 0, 1] )
	w = np.array ( [0.4, 0.35, 0.05] )
	def active_func(k):
	   if k > 0:
	      return 1
	   else:
	      return 0
	    
	def and_sum(x,w):
	    k_list = np.round( np.sum(x*w, axis=1), decimals=2 )
	    k = [ active_func(i) for i in k_list ]
	    return np.array(k) - target
	
	
	print(and_sum(x,w))




문제 33. 아래의 공식을 대입해서 최적의 가중치인 w0,w1,w2를 출력하게끔 and_sum함수에 코드를 추가하시오 !

	공식 : wi = wi + 0.05 * xi * (target - k)

	import numpy as np
	x = np.array( [ [-1, 0, 0], [-1, 0, 1], [-1, 1, 0], [-1, 1, 1] ] )
	targets = np.array( [[0], [0], [0], [1]] )
	w = np.array ( [0.4, 0.35, 0.05] )
	
	def active_func(k):
	    if k > 0:
	        return 1
	    elif k <= 0:
	        return 0
	    
	def and_sum(x,w):
	    while True:
	        w1 = []
	        k_list = np.round( np.sum(x*w, axis=1), decimals=2 )
	        for i in range(len(k_list)):
	            k = active_func(k_list[i])
	            if (targets[i][0] - k) != 0:
	                for j in range(len(w)):
	                    w1.append(w[j] + 0.05*x[i][j]*(targets[i][0] - k))
	        if w1 == []:
	            break
	        w = np.array(w1)
	    return w
	
	print(and_sum(x,w))



문제 34. 아래의 식을 파이썬으로 구현하시오 !
	
	그림 2-_-6
	
	x1*w1 + x2*w2 = y
	x1 = 0
	x2 = 1
	w1 = 0.5
	w2 = 0.5
	
	import numpy as np
	
	x = np.array( [0, 1] )
	w = np.array( [0.5, 0.5] )
	print(x*w)
	print(np.sum(x*w))
	
	[ 0.   0.5]
	0.5




문제 35. 위의 식에 책 52쪽에 나오는 편향을 더해서 완성한 아래의 식을 파이썬으로 구현하시오 !

	 b(편향) = -0.7
	 x1 = 0
	 x2 = 1
	 w1 = 0.5
	 w2 = 0.5
	
	import numpy as np
	
	b = np.array( [-0.7] )
	x = np.array( [0, 1] )
	w = np.array( [0.5, 0.5] )
	print(np.sum(x*w)+b)

	[-0.2]



문제 36. and게이트를 파이썬으로 구현하시오 !
         (파이썬 함수를 생성하는데 함수 이름은 AND이다)

	책 49페이지 그림 2-2 AND게이트 진리표
	 (w1 : 0.5, w2 : 0.5, theta : 0.7)
	
	 그림 2-_-7

	import numpy as np
	
	def AND(x1,x2):
	    w1,w2,theta = 0.5, 0.5, 0.7
	    if x1*w1 + x2*w2 > theta:
	        return 1
	    else:
	        return 0
	    
	print(AND(0,0))
	print(AND(0,1))
	print(AND(1,0))
	print(AND(1,1))

	0
	0
	0
	1

문제 37. 위에서 만든 and 퍼셉트론 함수를 이용해서 아래의 inputdata를 이용해서 출력 결과를 for loop문으로
	 한번에 출력하시오 !

	import numpy as np
	inputData = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
	
	def AND(x1,x2):
	    w1,w2,theta = 0.5, 0.5, 0.7
	    if x1*w1 + x2*w2 > theta:
	        return 1
	    else:
	        return 0
	
	for i in inputData:
	    print('%d, %d ===> %d'%(i[0], i[1], AND(i[0],i[1])))

	0, 0 ===> 0
	0, 1 ===> 0
	1, 0 ===> 0
	1, 1 ===> 1



문제 38. 문제 37번 코드를 가지고 좀 수정해서 or 게이트 함수를 생성해서 아래와 같이 출력하시오 !

	import numpy as np
	
	inputData = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
	
	def OR(x1,x2):
	    w1,w2,theta = 0.5, 0.5, 0.3
	    if x1*w1 + x2*w2 > theta:
	        return 1
	    else:
	        return 0
	
	for i in inputData:
	    print('%d, %d ===> %d'%(i[0], i[1], OR(i[0],i[1])))

	0, 0 ===> 0
	0, 1 ===> 1
	1, 0 ===> 1
	1, 1 ===> 1



문제 39. (점심시간 문제) Not And Perceptron함수를 구현하시오 !

	import numpy as np
	
	inputData = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
	
	def NAND(x1,x2):
	    w1,w2,theta = 0.5, 0.5, 0.7
	    if x1*w1 + x2*w2 > theta:
	        return 0
	    else:
	        return 1
	
	for i in inputData:
	    print('%d, %d ===> %d'%(i[0], i[1], NAND(i[0],i[1])))
	
	0, 0 ===> 1
	0, 1 ===> 1
	1, 0 ===> 1
	1, 1 ===> 0




문제 40. 위에서 만든 3개의 함수 (OR, NAND, AND)를 이용해서 XOR함수를 생성해서 아래와 같이 결과를
	  출력하시오 !

	import numpy as np
	inputData = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
	
	def OR(x1,x2):
	    w1,w2,theta = 0.5, 0.5, 0.3
	    if x1*w1 + x2*w2 > theta:
	        return 1
	    else:
	        return 0
	
	def NAND(x1,x2):
	    w1,w2,theta = 0.5, 0.5, 0.7
	    if x1*w1 + x2*w2 > theta:
	        return 0
	    else:
	        return 1
	
	def AND(x1,x2):
	    w1,w2,theta = 0.5, 0.5, 0.7
	    if x1*w1 + x2*w2 > theta:
	        return 1
	    else:
	        return 0
	    
	def XOR(x1,x2):
	    xx1 = OR(x1,x2)
	    xx2 = NAND(x1,x2)
	    return AND(xx1, xx2)
		    
	for i in inputData:
	    print('%d, %d ===> %d'%(i[0], i[1], XOR(i[0],i[1])))

	0, 0 ===> 0
	0, 1 ===> 1
	1, 0 ===> 1
	1, 1 ===> 0




문제 41. 2장에서 배운 퍼셉트론의 4개의 게이트함수를 이용해서 아래의 결과를 출력하시오 !

	import numpy as np
	inputData = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
	
	def OR(x1,x2):
	    w1,w2,theta = 0.5, 0.5, 0.3
	    if x1*w1 + x2*w2 > theta:
	        return 1
	    else:
	        return 0
	
	def NAND(x1,x2):
	    w1,w2,theta = 0.5, 0.5, 0.7
	    if x1*w1 + x2*w2 > theta:
	        return 0
	    else:
	        return 1
	
	def AND(x1,x2):
	    w1,w2,theta = 0.5, 0.5, 0.7
	    if x1*w1 + x2*w2 > theta:
	        return 1
	    else:
	        return 0
	    
	def XOR(x1,x2):
	    xx1 = OR(x1,x2)
	    xx2 = NAND(x1,x2)
	    return AND(xx1, xx2)
	
	print('---- And 퍼셉트론 ----')
	for i in inputData:
	    print('%d, %d ===> %d'%(i[0], i[1], AND(i[0],i[1])))
	    
	print('---- Or 퍼셉트론 ----')
	for i in inputData:
	    print('%d, %d ===> %d'%(i[0], i[1], OR(i[0],i[1])))
	
	print('---- Nand 퍼셉트론 ----')
	for i in inputData:
	    print('%d, %d ===> %d'%(i[0], i[1], NAND(i[0],i[1])))
	    
	print('---- Xor 퍼셉트론 ----')
	for i in inputData:
	    print('%d, %d ===> %d'%(i[0], i[1], XOR(i[0],i[1])))

	---- And 퍼셉트론 ----
	0, 0 ===> 0
	0, 1 ===> 0
	1, 0 ===> 0
	1, 1 ===> 1
	---- Or 퍼셉트론 ----
	0, 0 ===> 0
	0, 1 ===> 1
	1, 0 ===> 1
	1, 1 ===> 1
	---- Nand 퍼셉트론 ----
	0, 0 ===> 1
	0, 1 ===> 1
	1, 0 ===> 1
	1, 1 ===> 0
	---- Xor 퍼셉트론 ----
	0, 0 ===> 0
	0, 1 ===> 1
	1, 0 ===> 1
	1, 1 ===> 0







문제 42. 2장 총정리 62페이지에 나오는 글 맨 마지막에 "다층 퍼셉트론을 컴퓨터로 구현할 수 있다"는 것을
	 파이썬 코드로 구현하시오 !

	1. 어제 구현한 단층 함수인 AND 퍼셉트론 함수로 AND에 해당하는 w0, w1, w2만 변경을 했는데

	2. 다층은 AND, NAND, OR 3개의 함수의 각각의 w0, w1, w2를 갱신해야 한다.


	결과 :
	  
	NAND의 w : [-0.1  -0.05 -0.05]
	OR의   w : [ 0.15  0.45  0.25]
	AND의  w : [ 0.2  -0.05  0.25]


	import numpy as np
	x = np.array( [ [-1, 0, 0], [-1, 0, 1], [-1, 1, 0], [-1, 1, 1] ] )
	xx = np.array( [ [-1, 0, 1], [-1, 1, 1], [-1, 1, 1], [-1, 1, 0] ] )
	w = np.array ( [0.4, 0.35, 0.05] )
	
	or_targets = np.array( [[0], [1], [1], [1]] )
	nand_targets = np.array( [[1], [1], [1], [0]] )
	and_targets = np.array( [[0], [1], [1], [0]] )
	
	def active_func(k):
	    if k > 0:
	        return 1
	    elif k <= 0:
	        return 0
	    
	def logi_sum(x,w, targets):
	    while True:
	        w1 = []
	        k_list = np.sum(x*w, axis=1)
	        for i in range(len(k_list)):
	            k = active_func(k_list[i])
	            if (targets[i][0] - k) != 0:
	                w1 = []
	                for j in range(len(w)):
	                    w1.append(w[j] + 0.05*x[i][j]*(targets[i][0] - k))
	                break
	        if w1 == []:
	            break
	        w = np.array(w1)
	    return w
	
	print('OR  의 w :',logi_sum(x,w,or_targets))
	print('NAND의 w :',logi_sum(x,w,nand_targets))
	print('AND 의 w :',logi_sum(xx,w,and_targets))

	OR  의 w : [ 0.2   0.35  0.25]
	NAND의 w : [-0.15 -0.1  -0.05]
	AND 의 w : [ 0.4   0.35  0.1 ]





문제 44. 파이썬으로 계단함수를 만드시오 !

	"숫자 0과 1을 리턴하는 함수"
	 x <= 0 -----> 0을 출력
	 x >  0 -----> 1을 출력

	예시 : x_data = np.array([-1,0,1])
	       print( step_func(x_data))

	import numpy as np
	x_data = np.array([-1,0,1])

	def step_data(data):
	    result = []
	    for i in data:
	        if i <= 0:
	            result.append(0)
	        else:
	            result.append(1)
	    return np.array(result)

	print(step_data(x_data))
	
	[0 0 1]




문제 45. 계단함수를 파이썬으로 시각화 하시오 !

	import numpy as np
	import matplotlib.pyplot as plt
	
	x = np.arange(-5,5,0.01)
	def step_func(data):
	    result = []
	    for i in data:
	        if i <= 0:
	            result.append(0)
	        else:
	            result.append(1)
	    return np.array(result)
	
	y = step_func(x)
	plt.plot(x,y)
	plt.ylim(-0.1, 1.1)
	plt.show()



문제 46. 시그모이드 함수를 파이썬으로 구현하시오 !

	 * 시그모이드 함수 식 : h(x) = 1 / (1 + np.exp(-x) )

	import numpy as np
	x = np.array([1.0, 2.0])
	
	def sigmoid(data):
	    return 1/( 1 + np.exp(-data) )
	
	print( sigmoid(x))

	[ 0.73105858  0.88079708]




문제 47. 시그모이드 함수의 그래프를 그리시오 !

	import numpy as np
	import matplotlib.pyplot as plt

	x = np.arange(-5,5,0.1)
	
	def sigmoid(data):
	    return 1/( 1 + np.exp(-data) )
	
	y = sigmoid(x)
	plt.plot(x,y)
	plt.show()
	



문제 48. Relu 함수를 생성하시오 !

	import numpy as np

	x = np.array([-2, 0.3])
	def relu(data):
	    result = []
	    for i in data:
	        if i <= 0:
	            result.append(0)
	        else:
	            result.append(i)
	    return np.array(result)
	
	print(relu(x))

	[ 0.   0.3]

	선생님 답

	import numpy as np

	x = np.array([-2, 0.3])
	def relu(x):
	    return np.maximum(0,x)
	
	print(relu(x))

	[ 0.   0.3]



문제 49. (점심시간 문제) Relu 함수의 그래프로 그리시오 !

	import numpy as np
	import matplotlib.pyplot as plt
	
	x = np.arange(-5,5, 0.1)
	def relu(data):
	    result = []
	    for i in data:
	        if i <= 0:
	            result.append(0)
	        else:
	            result.append(i)
	    return np.array(result)
	
	y = relu(x)
	plt.plot(x,y)
	plt.show()
	




문제 50. 그림 3-_-8 두개의 행렬의 내적을 numpy로 구현하시오 !

	import numpy as np	
	
	a = np.array( [[1, 2], [3, 4]] )
	b = np.array( [[5, 6], [7, 8]] )
	
	print( np.dot(a,b) )
	
	[[19 22]
	 [43 50]]

	또는 np.matrix로 구현한다면 ?
	
	import numpy as np
	
	a = np.matrix( [[1, 2], [3, 4]] )
	b = np.matrix( [[5, 6], [7, 8]] )
	
	print( a*b )


		※ array 와 matrix의 차이는
			1. array는 다차원을 나타낼수 있는데 matrix는 2차원 밖에 안된다.
			2. array 는 matrix를 포함하고 있기 때문에 matrix는 array가 쓰는 모든 함수를 사용할
			   수 있다.
			3. matrix는 array와 달리 연산의 notation이 더 간단해 진다.



문제 51. 아래의 2차원 행렬을 만드시오 !

	import numpy as np
	
	a = np.array( [[1, 2], [3, 4]] )
	
	print(a.ndim)
	
	2



문제 52. 3차원 행렬을 만들어 보시오 !

	import numpy as np
	
	a = np.array( [[[1, 2], [3, 4]], [[5, 6], [7, 8]]] )
	
	print(a.ndim)
	
	3



문제 53. 4차원 행렬을 만들어 보시오 !

	import numpy as np
	
	a = np.array( [[[[1,2],[3,4]],[[5,6],[7,8]]],[[[9,10],[11,12]],[[13,14],[15,16]]]] )
	print(a)
	print(a.ndim)



문제 54. 아래의 3차원 행렬에서 숫자 5를 출력하시오 !

	import numpy as np
	
	a = np.array( [[[1, 2], [3, 4]], [[5, 6], [7, 8]]] )
	
	print(a[1][0][0])

	5



문제 55. 아래의 4차원 배열에서 숫자 5를 출력하시오 !

	import numpy as np	
	
	a = np.array( [[[[1,2],[3,4]],[[5,6],[7,8]]],[[[9,10],[11,12]],[[13,14],[15,16]]]] )
	
	print(a[0][1][0][0])

	5



문제 56. 아래의 행렬의 내적을 파이썬으로 구현하시오 !

		1 2 3	     5 6
			⊙   7 8
		4 5 6        9 10


	import numpy as np
	
	a = np.array( [[1, 2, 3], [4, 5, 6]] )
	b = np.array( [[5, 6], [7, 8], [9, 10]] )
	print(np.dot(a,b))




문제 57. 위의 신경망과 연관된 아래의 방정식에서 x1과 x2가 1과 2이면 y1과 y2와 y3의 값은 무엇인가?
	 파이썬의 numpy로 y1, y2, y3 을 출력하시오 !

	import numpy as np
	
	x = np.array([1, 2])
	w = np.array([[1, 2], [3 ,4], [5, 6]])
	print(x)
	print(w.T)
	print(np.dot(x,w.T))



문제 58. 아래의 단층 신경망의 출력값 y1 과 y2를 출력하시오 !

	import numpy as np
	
	x = np.array([4, 5, 7, 2])
	w = np.array([[8, 4, 6, 12], [21, 5, 34, 2], [1, 9, 4, 5]])
	
	print(np.dot(x,w.T))
	
	[118 351  87]




문제 59. 문제 57번 신경망에 가중의 총합인 y값을 시그모이드 함수에 통과시켜서 나온 y_hat을 출력하시오 !

	import numpy as np
	
	x = np.array([1, 2])
	w = np.array([[1, 2], [3 ,4], [5, 6]])
	
	def sigmoid(data):
	    return 1/( 1 + np.exp(-data) )
	
	y = np.dot(x,w.T)
	y_hat = sigmoid(y)
	print(y_hat)

	[ 0.99330715  0.9999833   0.99999996]




문제 60. 아래의 3층 신경망을 구현하고 j1과 j2를 출력하시오 ! (3층 신경망)

	그림 3-_-10

	import numpy as np
	
	x = np.array([1, 2])
	w1 = np.array([[1, 2], [3 ,4], [5, 6]])
	w2 = np.array([[3, 5, 7], [4, 6, 8]])
	w3 = np.array([[4, 6], [5, 7]])
	
	def sigmoid(data):
	    return 1/( 1 + np.exp(-data) )
	
	y = np.dot(x,w1.T)
	y_hat = sigmoid(y)
	z = np.dot(y_hat,w2.T)
	z_hat = sigmoid(z)
	j = np.dot(z_hat, w3.T)
	print(j)

	[  9.99999866  11.99999833]




문제 61. 입력값을 받아 그대로 출력하는 항등함수를 identify_function이라는 이름으로 생성하시오 !

	x = np.array([[1, 2]])
	
	def identify_func(x):
	    return x
	
	print(identify_func(x))
	
	[[1 2]]



문제 62. 문제 60번에서 만든 3층 신경망의 3층 출력층에 항등함수를 달아서 j1_hat과 j2_hat을 구하는 코드로
	 완성시키시오 !

	import numpy as np
	
	def sigmoid(data):
	    return 1/( 1 + np.exp(-data) )
	
	def identify_func(x):
	    return x

	# 0층 (입력층)
	x = np.array([1, 2])

	# 1층 (은닉 1층)
	w1 = np.array([[1, 2], [3 ,4], [5, 6]])   # 1층의 가중치
	y = np.dot(x,w1.T)
	y_hat = sigmoid(y)

	# 2층 (은닉 2층)
	w2 = np.array([[3, 5, 7], [4, 6, 8]])   # 2층의 가중치
	z = np.dot(y_hat,w2.T)
	z_hat = sigmoid(z)

	# 출력층
	w3 = np.array([[4, 6], [5, 7]])   # 3층의 가중치
	j = np.dot(z_hat, w3.T)
	j_hat = identify_func(j)
	print(j_hat)
	
	[  9.99999866  11.99999833]



문제 63. 위의 3층 신경망에 활성화 함수를 relu 함수로 변경하시오 !

	import numpy as np
	
	def sigmoid(data):
	    return 1/( 1 + np.exp(-data) )
	
	def identify_func(x):
	    return x
	
	def relu(x):
	    return np.maximum(0,x)
	
	# 0층 (입력층)
	x = np.array([1, 2])
	
	# 1층 (은닉 1층)
	w1 = np.array([[1, 2], [3 ,4], [5, 6]])   # 1층의 가중치
	y = np.dot(x,w1.T)
	y_hat = relu(y)
	
	# 2층 (은닉 2층)
	w2 = np.array([[3, 5, 7], [4, 6, 8]])   # 2층의 가중치
	z = np.dot(y_hat,w2.T)
	z_hat = relu(z)
	
	# 출력층
	w3 = np.array([[4, 6], [5, 7]])   # 3층의 가중치
	j = np.dot(z_hat, w3.T)
	j_hat = identify_func(j)
	print(j_hat)

	[2088 2499]



문제 64. 위의 코드를 함수로 만들어서 아래와 같이 실행될 수 있게 하시오 ! (함수이름 : init_network())

	def init_network():
	    import numpy as np
	    network = {}
	    network['w1'] = np.array([[1, 2], [3 ,4], [5, 6]])
	    network['w2'] = np.array([[3, 5, 7], [4, 6, 8]])
	    network['w3'] = np.array([[4, 6], [5, 7]])
	    return network
	
	network = init_network()	# 함수를 network이라는 변수에 담는다.
	print(network['w1'])
	
	[[1 2]
	 [3 4]
	 [5 6]]



문제 65. 위에서 만든 init_network()함수안에 키 w1,w2,w3의 값을 변수 w1,w2,w3에 담으시오 !

	def init_network():
	    import numpy as np
	    network = {}
	    network['w1'] = np.array([[1, 2], [3 ,4], [5, 6]])
	    network['w2'] = np.array([[3, 5, 7], [4, 6, 8]])
	    network['w3'] = np.array([[4, 6], [5, 7]])
	    return network
	
	w1, w2, w3 = init_network()['w1'], init_network()['w2'], init_network()['w3']
	
	print(w1)
	print(w2) 	
	print(w3)

	[[1 2]
	 [3 4]
	 [5 6]]
	[[3 5 7]
	 [4 6 8]]
	[[4 6]
	 [5 7]]



문제 66. 위의 코드를 가지고 어제 만든 3층 신경망 코드를 다시 작성하시오 !

	def init_network():
	    import numpy as np
	    network = {}
	    network['w1'] = np.array([[1, 2], [3 ,4], [5, 6]])
	    network['w2'] = np.array([[3, 5, 7], [4, 6, 8]])
	    network['w3'] = np.array([[4, 6], [5, 7]])
	    return network
	
	w1, w2, w3 = init_network()['w1'], init_network()['w2'], init_network()['w3']
	
	def sigmoid(data):
	    return 1/( 1 + np.exp(-data) )
	
	def identify_func(x):
	    return x
	
	def relu(x):
	    return np.maximum(0,x)
	
	# 0층 (입력층)
	x = np.array([1, 2])
	
	# 1층 (은닉 1층)
	y = np.dot(x,w1.T)
	y_hat = relu(y)
	
	# 2층 (은닉 2층)
	z = np.dot(y_hat,w2.T)
	z_hat = relu(z)
	
	# 출력층
	j = np.dot(z_hat, w3.T)
	j_hat = identify_func(j)
	print(j_hat)

	[2088 2499]



문제 67. 입력층, 은닉1층, 은닉2층, 출력층 코드를 가지고 network과 입력값 x를 매개변수로 받는
	 forward(network, x)라는 함수를 생성하시오 !

	def init_network():
	    import numpy as np
	    network = {}
	    network['w1'] = np.array([[1, 2], [3 ,4], [5, 6]])
	    network['w2'] = np.array([[3, 5, 7], [4, 6, 8]])
	    network['w3'] = np.array([[4, 6], [5, 7]])
	    return network
	
	def sigmoid(data):
	    return 1/( 1 + np.exp(-data) )
	
	def identify_func(x):
	    return x
	
	def forword(network, x):
	    w1, w2, w3 = network['w1'], network['w2'], network['w3']
	    # 1층 (은닉 1층)
	    y = np.dot(x,w1.T)
	    y_hat = sigmoid(y)
	
	    # 2층 (은닉 2층)
	    z = np.dot(y_hat,w2.T)
	    z_hat = sigmoid(z)
	
	    # 출력층
	    j = np.dot(z_hat, w3.T)
	    j_hat = identify_func(j)
	    return j_hat
	
	
	# 실행 코드
	network = init_network()   #가중치 생성
	x = np.array([1, 2])       #입력값
	print(forword(network, x)) # 순전파




문제 68. 입력값이 1, 2가 아니라 4, 5일때의 위의 3층 신경망을 통과한 결과를 아래와 같이 출력되게 하시오 !

	import layers_3 as la
	import numpy as np

	network = la.init_network()   #가중치 생성
	x = np.array([4, 5])          #입력값
	print(la.forword(network, x)) # 순전파



문제 69. 아래의 리스트를 무리수(자연상수)의 제곱으로 해서 계산값이 무엇인가 ?

	import numpy as np
	
	a = np.array([1010, 1000, 990])
	print(np.exp(a))
	
	[ inf  inf  inf]



	
문제 70. a리스트에서 각 요소값을 가장 큰 요소의 값으로 뺀 결과를 출력하시오 !

	import numpy as np
	
	a = np.array([1010, 1000, 990])
	c = np.max(a)
	print(c)
	print(a - c)

	1010
	[  0 -10 -20]
	


문제 71. 위의 [0 -10 -20]을 자연상수 e의 제곱으로 해서 출력된 결과가 무엇인지 확인하시오 !

	import numpy as np
	
	a = np.array([1010, 1000, 990])
	c = np.max(a)
	minus = a - c
	
	print(np.exp(minus))
	
	[  1.00000000e+00   4.53999298e-05   2.06115362e-09]



문제 72. 위의 코드를 softmax라는 함수 이름으로 만드시오 !

	import numpy as np
	a = np.array([1010, 1000, 990])
	
	def softmax(a):
	    c = np.max(a)
	    minus = a - c
	    return np.exp(minus)
	
	print(softmax(a))

	[  1.00000000e+00   4.53999298e-05   2.06115362e-09]



문제 73. softmax 함수를 분모까지 포함해서 완전히 완성하시오 !

	import numpy as np
	a = np.array([1010, 1000, 990])
	
	def softmax(a):
	    c = np.max(a)
	    minus = a - c
	    exp_a = np.exp(minus)
	    sum_exp_a = np.sum(exp_a)
	    return exp_a / sum_exp_a
	
	print(softmax(a))
	
	[  9.99954600e-01   4.53978686e-05   2.06106005e-09]



 오늘의 마지막 문제
문제 74. 위의 출력된 결과의 합이 1인지 확인하시오 !

	import numpy as np
	a = np.array([1010, 1000, 990])
	
	def softmax(a):
	    c = np.max(a)
	    minus = a - c
	    exp_a = np.exp(minus)
	    sum_exp_a = np.sum(exp_a)
	    return exp_a / sum_exp_a
	
	print(np.sum(softmax(a)))
	
	1.0





문제 76. 문제 75 번까지 해서 만들었던 3층 신경망(항등함수로 구현)의 출력층 함수를 소프트 맥스 함수로
	 변경하시오 !

	import numpy as np
	
	def init_network():
	    network = {}
	    network['w1'] = np.array([[1, 2], [3 ,4], [5, 6]])
	    network['w2'] = np.array([[3, 5, 7], [4, 6, 8]])
	    network['w3'] = np.array([[4, 6], [5, 7]])
	    return network
	
	def sigmoid(data):
	    return 1/( 1 + np.exp(-data) )
	
	def identify_func(x):
	    return x
	
	def relu(x):
	    return np.maximum(0,x)
	
	def softmax(a):
	    c = np.max(a)
	    minus = a - c
	    exp_a = np.exp(minus)
	    sum_exp_a = np.sum(exp_a)
	    return exp_a / sum_exp_a
	
	def forword(network, x):
	    w1, w2, w3 = network['w1'], network['w2'], network['w3']
	    # 1층 (은닉 1층)
	    y = np.dot(x,w1.T)
	    y_hat = sigmoid(y)
	
	    # 2층 (은닉 2층)
	    z = np.dot(y_hat,w2.T)
	    z_hat = sigmoid(z)
	
	    # 출력층
	    j = np.dot(z_hat, w3.T)
	    j_hat = softmax(j)
	    return j_hat
	
	# 실행 코드
	network = init_network()   #가중치 생성
	x = np.array([1, 2])       #입력값
	print(forword(network, x)) # 순전파

	# layer_3모듈 수정완료


		※ 신경망   학습  -------> 소프트 맥스 함수를 사용 o
			   테스트 -------> 소프트 맥스 함수를 사용 x
			"자원 낭비를 줄이고자"



문제 77. 아래의 코드의 dataset 패키지의 mnist.py에 load_mnist라는 함수가 있는지 확인하시오 !

	def load_mnist(normalize=True, flatten=True, one_hot_label=False):
	    """MNIST 데이터셋 읽기
	    
	    Parameters
	    ----------
	    normalize : 이미지의 픽셀 값을 0.0~1.0 사이의 값으로 정규화할지 정한다.
	    one_hot_label :
	        one_hot_label이 True면、레이블을 원-핫(one-hot) 배열로 돌려준다.
	        one-hot 배열은 예를 들어 [0,0,1,0,0,0,0,0,0,0]처럼 한 원소만 1인 배열이다.
	    flatten : 입력 이미지를 1차원 배열로 만들지를 정한다.
	    
	    Returns
	    -------
	    (훈련 이미지, 훈련 레이블), (시험 이미지, 시험 레이블)
	    """
	    if not os.path.exists(save_file):
	        init_mnist()
	        
	    with open(save_file, 'rb') as f:
	        dataset = pickle.load(f)
	    
	    if normalize:
	        for key in ('train_img', 'test_img'):
	            dataset[key] = dataset[key].astype(np.float32)
	            dataset[key] /= 255.0
	            
	    if one_hot_label:
	        dataset['train_label'] = _change_one_hot_label(dataset['train_label'])
	        dataset['test_label'] = _change_one_hot_label(dataset['test_label'])    
	    
	    if not flatten:
	         for key in ('train_img', 'test_img'):
	            dataset[key] = dataset[key].reshape(-1, 1, 28, 28)
	
	    return (dataset['train_img'], dataset['train_label']), (dataset['test_img'], dataset['test_label'])
	
	
	if __name__ == '__main__':
	    init_mnist()



문제 78. x_train데이터를 print해보시오 !

	# coding: utf-8
	
	import sys, os
	
	sys.path.append(os.pardir)  # 부모디렉토리의 파일을 가져올수
	                            # 있도록 설정
	
	import numpy as np
	from dataset.mnist import load_mnist
	from PIL import Image
	
	(x_train, t_train), (x_test, t_test) = load_mnist(flatten=True, normalize=False)
	
	print(x_train)

	[[0 0 0 ..., 0 0 0]
	 [0 0 0 ..., 0 0 0]
	 [0 0 0 ..., 0 0 0]
	 ...,
	 [0 0 0 ..., 0 0 0]
	 [0 0 0 ..., 0 0 0]
	 [0 0 0 ..., 0 0 0]]



문제 79. x_train데이터가 6만장이 맞는지 확인하시오 !

	print(len(x_train))
	print(x_train.shape)
	print(x_train.ndim)
	
	60000
	(60000, 784)
	2



문제 80. 아래의 리스트를 numpy array리스트로 변경하시오 !

	import numpy as np
	
	x = [[0.1,0.05,0.1,0.0,0.05,0.1,0.0,0.1,0.0,0.0],
	     [0.1,0.05,0.2,0.0,0.05,0.1,0.0,0.6,0.0,0.0],
	     [0.0,0.05,0.3,0.0,0.05,0.1,0.0,0.6,0.0,0.0],
	     [0.0,0.05,0.4,0.0,0.05,0.0,0.0,0.5,0.0,0.0],
	     [0.0,0.05,0.5,0.0,0.05,0.0,0.0,0.4,0.0,0.0],
	     [0.0,0.05,0.6,0.0,0.05,0.0,0.0,0.3,0.0,0.0],
	     [0.0,0.05,0.7,0.0,0.05,0.0,0.0,0.2,0.0,0.0],
	     [0.0,0.1,0.8,0.0,0.1,0.0,0.0,0.2,0.0,0.0],
	     [0.0,0.05,0.9,0.0,0.05,0.0,0.0,0.0,0.0,0.0]]
	
	x = np.array(x)
	print(x)

	[[ 0.1   0.05  0.1   0.    0.05  0.1   0.    0.1   0.    0.  ]
	 [ 0.1   0.05  0.2   0.    0.05  0.1   0.    0.6   0.    0.  ]
	 [ 0.    0.05  0.3   0.    0.05  0.1   0.    0.6   0.    0.  ]
	 [ 0.    0.05  0.4   0.    0.05  0.    0.    0.5   0.    0.  ]
	 [ 0.    0.05  0.5   0.    0.05  0.    0.    0.4   0.    0.  ]
	 [ 0.    0.05  0.6   0.    0.05  0.    0.    0.3   0.    0.  ]
	 [ 0.    0.05  0.7   0.    0.05  0.    0.    0.2   0.    0.  ]
	 [ 0.    0.1   0.8   0.    0.1   0.    0.    0.2   0.    0.  ]
	 [ 0.    0.05  0.9   0.    0.05  0.    0.    0.    0.    0.  ]]



문제 81. 위의 numpy array로 변경한 리스트의 차원을 확인하시오 !

	print(x.ndim)
	
	2



문제 82. 위의 리스트를 1차원으로 변경하시오 !

	import numpy as np
	
	a = np.array(x)
	print(a.ndim)
	a2 = a.flatten()
	print(a2.ndim)

	2
	1



문제 83. mnist데이터를 load_mnist 함수로 로드할 때 flatten을 false로 해서 로드하고 훈련 데이터의 shape를
	 확인해보시오 !

	# coding: utf-8
	
	import sys, os
	
	sys.path.append(os.pardir)  # 부모디렉토리의 파일을 가져올수
	                            # 있도록 설정
	
	import numpy as np
	from dataset.mnist import load_mnist
	from PIL import Image
	
	(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False, normalize=False)
	
	
	print(len(x_train))
	print(x_train.shape)
	print(x_train.ndim)
	
	60000
	(60000, 1, 28, 28)
	       ↑  ↑  ↑
	     흑백 가로 세로	컬러면 ?  RGB값으로 해서 3이 출력됨.
	4




문제 84. x_train[0]이 숫자 4였는데 x_train[0]의 라벨(정답) 인 t_train[0]을 출력해서 숫자 5가 맞는지 확인하시오 !

	# coding: utf-8
	
	import sys, os
	
	sys.path.append(os.pardir)  # 부모디렉토리의 파일을 가져올수
	                            # 있도록 설정
	
	import numpy as np
	from dataset.mnist import load_mnist
	from PIL import Image
	
	(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False, normalize=False)
	
	print(t_train[0])
	
	5


문제 85. t_train 데이터를 출력할 때 one_hot_encoding을 True로 해서 출력하시오 !

	# coding: utf-8
	
	import sys, os
	
	sys.path.append(os.pardir)  # 부모디렉토리의 파일을 가져올수
	                            # 있도록 설정
	
	import numpy as np
	from dataset.mnist import load_mnist
	from PIL import Image
	
	(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False, normalize=False, one_hot_label=True)
	
	print(t_train[0])
	
	[ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]
	  0   1   2   3   4   5   6   7   8   9




문제 86. mnist 데이터를 가져오는 코드를 가지고 아래의 get_data()함수를 생성하고 아래와 같이 실행되게 하시오 !

	def get_data():
	    (x_train, t_train), (x_test, t_test) = load_mnist(flatten=True, normalize=False)
	    return x_test, t_test
	x, t = get_data()
	
	print(x.shape)
	print(len(t))

	(10000, 784)
	10000



문제 87. 자신이 만든 필기체 숫자를 784의 flatten된 numpy배열로 만드시오 !(오늘의 마지막 문제)

	import cv2
	j = 'tttt.png'
	import numpy as np
	import matplotlib.pyplot as plt
	import matplotlib.image as mpimg
	
	def rgb2gray(rgb):
	    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])
	
	img = mpimg.imread(j)
	gray = rgb2gray(img)

	#이미지프린트
	#plt.imshow(gray, cmap = plt.get_cmap('gray'))
	#plt.show()
	#print(gray.shape)

	a = np.array(gray)
	x= a.flatten()
	# print(x.shape)
	
	import sys, os
	
	sys.path.append(os.pardir)
	
	from dataset.mnist import load_mnist
	from PIL import Image
	import  pickle
	
	def sigmoid(num):
	    rst = (1 / (1 + np.exp(-num)))
	    return (rst)
	
	
	def identity_function(x):
	    return x
	
	
	def softmax(a):
	    c = np.max(a)
	    minus = a - c
	    exp_a = np.exp(minus)
	    sum_exp_a = np.sum(exp_a)
	    y = exp_a / sum_exp_a
	    return y
	
	def  init_network():
	    with open("sample_weight.pkl",'rb')  as  f:
	        network = pickle.load(f)
	    return  network
	
	
	def  predict(network, x):
	    W1, W2, W3 = network['W1'], network['W2'], network['W3']
	    b1, b2, b3 = network['b1'], network['b2'], network['b3']
	
	    a1 = np.dot(x,W1) + b1
	    z1 = sigmoid(a1)
	    a2 = np.dot(z1,W2) + b2
	    z2 = sigmoid(a2)
	    a3 = np.dot(z2,W3) + b3
	    y = softmax(a3)
	
	    return  y
	
	def  get_data():
	    (x_train, t_train) , (x_test, t_test) = \
	    load_mnist(normalize=True, flatten=True, one_hot_label=False)
	    return  x_test, t_test
	
	#x, t = get_data()
	network = init_network()
	y = predict(network,x)
	print(np.argmax(y))



문제 88. 아래의 10개의 원소를 갖는 x라는 리스트를 만들고 x리스트에 가장 큰 원소가 몇번째 인덱스인지
	 알아내시오 !

	import numpy as np
	
	x = [0.05, 0.01, 0.02, 0.02, 0.1, 0.2, 0.3, 0.4, 0.05, 0.04]
	y = np.argmax(x,axis=0)
	print(y)
	
	print(x.index(max(x)))



문제 89. 아래의 리스트를 numpy array 리스트로 변환하고 shape를 확인하시오 !

	x = [[0.1,0.05,0.1,0.0,0.05,0.1,0.0,0.1,0.0,0.0],
	     [0.1,0.05,0.2,0.0,0.05,0.1,0.0,0.6,0.0,0.0],
	     [0.0,0.05,0.3,0.0,0.05,0.1,0.0,0.6,0.0,0.0],
	     [0.0,0.05,0.4,0.0,0.05,0.0,0.0,0.5,0.0,0.0],
	     [0.0,0.05,0.5,0.0,0.05,0.0,0.0,0.4,0.0,0.0],
	     [0.0,0.05,0.6,0.0,0.05,0.0,0.0,0.3,0.0,0.0],
	     [0.0,0.05,0.7,0.0,0.05,0.0,0.0,0.2,0.0,0.0],
	     [0.0,0.1 ,0.8,0.0,0.1 ,0.0,0.0,0.2,0.0,0.0],
	     [0.0,0.05,0.9,0.0,0.05,0.0,0.0,0.0,0.0,0.0],
	     [0.0,0.05,0.6,0.0,0.05,0.0,0.0,0.3,0.0,0.0] ]
	import numpy as np
	y = np.array(x)
	print(y.shape)
	
	(10, 10)



문제 90. 아래의 10행 10열에서 각 행에서 가장 큰 원소가 몇번째에 있는지 출력하시오 !

	x = [[0.2,0.05,0.1,0.0,0.05,0.0,0.0,0.1,0.0,0.0],   
	     [0.1,0.05,0.2,0.0,0.05,0.1,0.0,0.6,0.0,0.0],
	     [0.0,0.05,0.3,0.0,0.05,0.1,0.0,0.6,0.0,0.0],
	     [0.0,0.05,0.4,0.0,0.05,0.0,0.0,0.5,0.0,0.0],
	     [0.0,0.05,0.5,0.0,0.05,0.0,0.0,0.4,0.0,0.0],
	     [0.0,0.05,0.6,0.0,0.05,0.0,0.0,0.3,0.0,0.0],
	     [0.0,0.05,0.7,0.0,0.05,0.0,0.0,0.2,0.0,0.0],
	     [0.0,0.1 ,0.8,0.0,0.1 ,0.0,0.0,0.2,0.0,0.0],
	     [0.0,0.05,0.9,0.0,0.05,0.0,0.0,0.0,0.0,0.0],
	     [0.0,0.05,0.6,0.0,0.05,0.0,0.0,0.3,0.0,0.0] ]
	
	import numpy as np
	y = np.array(x)
	print(np.argmax(y,axis=1))    # 행기준
	# print(np.argmax(y,axis=0))  # 열기준

		※ 설명 : axis = 0  : 열
			  axis = 1  : 행




문제 91. 테스트 데이터 하나 x[34]의 필기체 의 라벨이 무엇인지 확인하시오 !

	import sys, os
	
	sys.path.append(os.pardir)
	
	import numpy as np
	from dataset.mnist import load_mnist
	from PIL import Image
	import  pickle
	import  numpy  as np
	
	def  get_data():
	    (x_train, t_train) , (x_test, t_test) = \
	    load_mnist(normalize=True, flatten=True, one_hot_label=False)
	    return  x_test, t_test
	
	x, t = get_data()
	print(x.shape)
	print(t[34])



문제 92. 테스트 데이터 하나인 x[34]의 필기체를 신경망에 넣고 신경망이 예측한 것과 라벨이 서로 일치하는지
	 확인하시오 !

	import sys, os
	
	sys.path.append(os.pardir)
	
	import numpy as np
	from dataset.mnist import load_mnist
	from PIL import Image
	import  pickle
	import  numpy  as np
	
	
	# 신경망 함수들
	def sigmoid(num):
	    rst = (1 / (1 + np.exp(-num)))
	    return (rst)
	
	
	def identity_function(x):
	    return x
	
	
	def softmax(a):
	    c = np.max(a)
	    minus = a - c
	    exp_a = np.exp(minus)
	    sum_exp_a = np.sum(exp_a)
	    y = exp_a / sum_exp_a
	    return y
	
	def  init_network():
	    with open("sample_weight.pkl",'rb')  as  f:
	        network = pickle.load(f)
	    return  network
	
	
	def  predict(network, x):
	    W1, W2, W3 = network['W1'], network['W2'], network['W3']
	    b1, b2, b3 = network['b1'], network['b2'], network['b3']
	
	    a1 = np.dot(x,W1) + b1
	    z1 = sigmoid(a1)
	    a2 = np.dot(z1,W2) + b2
	    z2 = sigmoid(a2)
	    a3 = np.dot(z2,W3) + b3
	    y = softmax(a3)
	
	    return  y
	
	def  get_data():
	    (x_train, t_train) , (x_test, t_test) = \
	    load_mnist(normalize=True, flatten=True, one_hot_label=False)
	    return  x_test, t_test
	
	x, t = get_data()
	network = init_network()
	y = predict(network,x[34])
	print(np.argmax(y))
	print(t[34])

	7
	7




문제 93. 위의 코드에 for loop문을 입혀서 테스트 데이터 10000장을 위의 3층 신경망에 넣고 10000장 중에 몇장을
	 3층 신경망이 맞추는지 확인하시오 !

	import sys, os
	
	sys.path.append(os.pardir)
	
	import numpy as np
	from dataset.mnist import load_mnist
	from PIL import Image
	import  pickle
	import  numpy  as np
	
	
	# 신경망 함수들
	def sigmoid(num):
	    rst = (1 / (1 + np.exp(-num)))
	    return (rst)
	
	
	def identity_function(x):
	    return x
	
	
	def softmax(a):
	    c = np.max(a)
	    minus = a - c
	    exp_a = np.exp(minus)
	    sum_exp_a = np.sum(exp_a)
	    y = exp_a / sum_exp_a
	    return y
	
	def  init_network():
	    with open("sample_weight.pkl",'rb')  as  f:
	        network = pickle.load(f)
	    return  network
	
	
	def  predict(network, x):
	    W1, W2, W3 = network['W1'], network['W2'], network['W3']
	    b1, b2, b3 = network['b1'], network['b2'], network['b3']
	
	    a1 = np.dot(x,W1) + b1
	    z1 = sigmoid(a1)
	    a2 = np.dot(z1,W2) + b2
	    z2 = sigmoid(a2)
	    a3 = np.dot(z2,W3) + b3
	    y = softmax(a3)
	
	    return  y
	
	def  get_data():
	    (x_train, t_train) , (x_test, t_test) = \
	    load_mnist(normalize=True, flatten=True, one_hot_label=False)
	    return  x_test, t_test
	
	x, t = get_data()
	network = init_network()
	t_count = 0
	f_count = 0
	for i in range(0,len(x)):
	    y = predict(network,x[i])
	    if np.argmax(y) == t[i]:
	        t_count += 1
	    else:
	        f_count += 1
	print(t_count, f_count)

	9352 648




문제 94. 위의 결과가 9352가 아니라 아래와 같이 정확도가 출력되게 하시오 !

	x, t = get_data()
	network = init_network()
	t_count = 0
	for i in range(0,10000):
	    y = predict(network,x[i])
	    if np.argmax(y) == t[i]:
	        t_count += 1
	print('정확도 :', t_count/len(x))

	정확도 : 0.9352




문제 95. 아래의 결과를 출력하시오 !
	 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
	
	y = list(range(0, 10))
	print(y)




문제 96. 아래의 결과를 출력하시오 !
	[0, 3, 6, 9]
	
	y = list( range(0, 10, 3))
	print(y)




문제 97. 아래의 리스트에서 최대값의 원소의 인덱스를 출력하시오 !
	[0, 3, 6, 9]
	
	import numpy as np
	y = list( range(0, 10, 3))
	print(np.argmax(y))
	3




문제 98. 아래의 행렬을 numpy 배열로 생성하시오 !

	 0.1  0.8  0.1
	 0.3  0.1  0.6
	 0.2  0.5  0.3
	 0.8  0.1  0.1

	import numpy as np
	
	y = np.array([0.1, 0.8, 0.1,
	              0.3, 0.1, 0.6,
	              0.2, 0.5, 0.3,
	              0.8, 0.1, 0.1])
	y = y.reshape(4,3)
	print(y)

	[[ 0.1  0.8  0.1]
	 [ 0.3  0.1  0.6]
	 [ 0.2  0.5  0.3]
	 [ 0.8  0.1  0.1]]




문제 99. numpy의 argmax를 이용해서 아래의 행렬에서 행중에 최대값 원소의 인덱스를 출력하시오 !
	  0.1  0.8  0.1     1
	  0.3  0.1  0.6     2
	  0.2  0.5  0.3     1
	  0.8  0.1  0.1     0
	
	import numpy as np
	
	y = np.array([0.1, 0.8, 0.1,
	              0.3, 0.1, 0.6,
	              0.2, 0.5, 0.3,
	              0.8, 0.1, 0.1])
	y = y.reshape(4,3)
	print(np.argmax(y, axis = 1))
	
	[1 2 1 0]




문제 100. 아래의 2개의 리스트를 만들고 서로 같은 자리에 같은 숫자가 몇개가 있는지 출력하시오 !
	 [2, 1, 3, 5, 1, 4, 2, 1, 1, 0]
	 [2, 1, 3, 4, 5, 4, 2, 1, 1, 2]
	
	import numpy as np
	
	x = np.array([2, 1, 3, 5, 1, 4, 2, 1, 1, 0])
	y = np.array([2, 1, 3, 4, 5, 4, 2, 1, 1, 2])
	print(np.sum(x==y))
	
	7




문제 101. 아래의 리스트를 x라는 변수에 담고 앞에 5개의 숫자만 출력하시오 !
	 [7, 3, 2, 1, 6, 7, 7, 8, 2, 4]
	
	x = [7, 3, 2, 1, 6, 7, 7, 8, 2, 4]
	print(x[:5])
	# print(x[5:10])

	[7 3 2 1 6]




문제 103. 아래의 코드를 mnist의 훈련 데이터를 100개씩 가져오는 코드로 작성하시오 !

	#----------1장씩 입력해서 예측 숫자 1개씩 출력하는 코드-----------
	# x, t = get_data()
	# network = init_network()
	# t_count = 0
	# for i in range(0,len(x)):
	#     y = predict(network,x[i])
	#     print(len(y))
	#-----------------------------------------------------------------
	
	x, t = get_data()
	network = init_network()
	t_count = 0
	batch_size = 100
	for i in range(0,len(x), batch_size):
	    x_batch = x[i:i+batch_size]
	    print(len(x_batch))
	#     y = predict(network,x_batch_size)
	#     print(len(y))




문제 104. 100개씩 가지고온 훈련 데이터를 predict함수에 입력해서 예측 숫자 100개를 출력하는 코드를 작성하시오 !

	#----------1장씩 입력해서 예측 숫자 1개씩 출력하는 코드-----------
	# x, t = get_data()
	# network = init_network()
	# t_count = 0
	# for i in range(0,len(x)):
	#     y = predict(network,x[i])
	#     print(len(y))
	#-----------------------------------------------------------------
	
	x, t = get_data()
	network = init_network()
	t_count = 0
	batch_size = 100
	for i in range(0,len(x), batch_size):
	    x_batch = x[i:i+batch_size]
	    y = predict(network,x_batch)
	    print(len(y))




문제 105. 위의 코드에서 y_batch의 shape를 확인하시오 !

	x, t = get_data()
	network = init_network()
	t_count = 0
	batch_size = 100
	for i in range(0,len(x), batch_size):
	    x_batch = x[i:i+batch_size]
	    y = predict(network,x_batch)
	    print(y.shape)
	
	

문제 106. (오늘의 마지막 문제) 위에 예측한 100개의 숫자와 실제 숫자(라벨)과 비교해서 정확도를 출력하시오 !

	x, t = get_data()
	network = init_network()
	t_count = 0
	batch_size = 100
	cnt = 0
	for i in range(0,len(x), batch_size):
	    x_batch = x[i:i+batch_size]
	    y = predict(network,x_batch)
	    cnt += sum(np.argmax(y, axis=1) == t[i:i+batch_size])
	print('정확도 :', cnt/len(x))

	정확도 : 0.9352




문제 107. 그림 4-_-1 식을 가지고 평균제곱 오차함수를 파이썬으로 구현하시오 !

	import numpy as np
	
	def mean_squared_error(x,t):
	    return 0.5*np.sum((x-t)**2)
	
	y = np.array([0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0])
	t = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])
	
	print(mean_squared_error(y,t))
	
	0.0975 # 이 오차를 신경망으로 역전파 시켜서 가중치(W)를 갱신을 한다.




문제 108. 아래의 확률벡터를 평균제곱 오차 함수를 이용해서 target(실제값)과 예측값의 오차율이 어떻게 되는지
	  for loop문으로 한번에 알아내시오 !

	import numpy as np
	
	def mean_squared_error(x,t):
	    return 0.5*np.sum((x-t)**2)
	
	t = [0,0,1,0,0,0,0,0,0,0]
	y1 = [0.1,0.05,0.1,0.0,0.05,0.1,0.0,0.1,0.0,0.0]  
	y2 = [0.1,0.05,0.2,0.0,0.05,0.1,0.0,0.6,0.0,0.0]
	y3 = [0.0,0.05,0.3,0.0,0.05,0.1,0.0,0.6,0.0,0.0]
	y4 = [0.0,0.05,0.4,0.0,0.05,0.0,0.0,0.5,0.0,0.0]
	y5 = [0.0,0.05,0.5,0.0,0.05,0.0,0.0,0.4,0.0,0.0]
	y6 = [0.0,0.05,0.6,0.0,0.05,0.0,0.0,0.3,0.0,0.0]
	y7 = [0.0,0.05,0.7,0.0,0.05,0.0,0.0,0.2,0.0,0.0]
	y8 = [0.0,0.1,0.8,0.0,0.1,0.0,0.0,0.2,0.0,0.0]
	y9 = [0.0,0.05,0.9,0.0,0.05,0.0,0.0,0.0,0.0,0.0]
	
	
	for i in range(1, 10):
	    print('y%d의 오차는 :'%i,mean_squared_error(np.array(eval('y'+str(i))), np.array(t)))

	y1의 오차율 : 0.4225
	y2의 오차율 : 0.5125
	y3의 오차율 : 0.4325
	y4의 오차율 : 0.3075
	y5의 오차율 : 0.2075
	y6의 오차율 : 0.1275
	y7의 오차율 : 0.0675
	y8의 오차율 : 0.05
	y9의 오차율 : 0.0075




문제 109. 확률 0.1과 확률 0.9를 교차 엔트로피 공식에 대입한 결과가 각각 어떻게 되는지 확인하시오 !

	import numpy as np
	
	print( -np.log(0.1), -np.log(0.9))

	2.30258509299 0.105360515658



문제 110. 교차 엔트로피 함수를 생성하시오 !

	import numpy as np
	
	def cross_entropy_error(x,t):
	    delta = 1e-7
	    return -np.sum(t*np.log(x+delta))
	
	t = [0,0,1,0,0,0,0,0,0,0]
	y1 = [0.1,0.05,0.1,0.0,0.05,0.1,0.0,0.1,0.0,0.0]
	y9 = [0.0,0.05,0.9,0.0,0.05,0.0,0.0,0.0,0.0,0.0]
	
	print(cross_entropy_error(np.array(y1), np.array(t)))
	print(cross_entropy_error(np.array(y9), np.array(t)))
	
	2.30258409299
	0.105360404547



문제 111. 평균제곱 오차함수와 교차엔트로피 오차함수중에 어떤것이 더 큰 오차를 신경망에 역전파
	  시켜주겠는지 테스트 해 보시오 !

	import numpy as np
	
	def mean_squared_error(x,t):
	    return 0.5*np.sum((x-t)**2)
	
	def cross_entropy_error(x,t):
	    delta = 1e-7
	    return -np.sum(t*np.log(x+delta))
	
	t = [0,0,1,0,0,0,0,0,0,0]
	y1 = [0.1,0.05,0.1,0.0,0.05,0.1,0.0,0.1,0.0,0.0]  
	y2 = [0.1,0.05,0.2,0.0,0.05,0.1,0.0,0.6,0.0,0.0]
	y3 = [0.0,0.05,0.3,0.0,0.05,0.1,0.0,0.6,0.0,0.0]
	y4 = [0.0,0.05,0.4,0.0,0.05,0.0,0.0,0.5,0.0,0.0]
	y5 = [0.0,0.05,0.5,0.0,0.05,0.0,0.0,0.4,0.0,0.0]
	y6 = [0.0,0.05,0.6,0.0,0.05,0.0,0.0,0.3,0.0,0.0]
	y7 = [0.0,0.05,0.7,0.0,0.05,0.0,0.0,0.2,0.0,0.0]
	y8 = [0.0,0.1 ,0.8,0.0,0.1 ,0.0,0.0,0.2,0.0,0.0]
	y9 = [0.0,0.05,0.9,0.0,0.05,0.0,0.0,0.0,0.0,0.0]
	
	for i in range(1, 10):
	    print('y%d의 오차율(평제) :'%i,mean_squared_error(np.array(eval('y'+str(i))), np.array(t)),end = '    
')
	    print('y%d의 오차율(엔트) :'%i,cross_entropy_error(np.array(eval('y'+str(i))), np.array(t)),end = '\n')
	
	y1의 오차율(평제) : 0.4225    y1의 오차율(엔트) : 2.30258409299
	y2의 오차율(평제) : 0.5125    y2의 오차율(엔트) : 1.60943741243
	y3의 오차율(평제) : 0.4325    y3의 오차율(엔트) : 1.20397247099
	y4의 오차율(평제) : 0.3075    y4의 오차율(엔트) : 0.916290481874
	y5의 오차율(평제) : 0.2075    y5의 오차율(엔트) : 0.69314698056
	y6의 오차율(평제) : 0.1275    y6의 오차율(엔트) : 0.510825457099
	y7의 오차율(평제) : 0.0675    y7의 오차율(엔트) : 0.356674801082
	y8의 오차율(평제) : 0.05      y8의 오차율(엔트) : 0.223143426314
	y9의 오차율(평제) : 0.0075    y9의 오차율(엔트) : 0.105360404547




문제 112. 1 ~ 60000개의 숫자중에 무작위로 10개를 출력하시오 !

	import numpy as np
	
	print(np.random.choice(np.arange(60000), 10))
	
	[58689 56878 34252 35130  5495 32837 58336 22841  1928 17641]




문제 113. mnist의 테스트 데이터 10000wnddp 랜덤으로 100장을 추출하는 코드를 작성하시오 !

	x, t = get_data()
	for i in range(0, 10000, 100):
	    batch_mask = np.random.choice(10000, 100)
	    print(batch_mask)
	    x_batch = x[batch_mask]
	    print(len(x_batch))




문제 114. 위의 코드는 복원추출인데 비복원 추출이 되게끔 코드를 수정하시오 !

	x, t = get_data()
	for i in range(0, 10000, 100):
	#     batch_mask = np.random.choice(10000, 100)
	#     print(batch_mask)
	#     x_batch = x[batch_mask]    # 복원 추출
	    x_batch = x[i:i+100]         # 비복원 추출
	    print([j for j in range(i, i+100)])
	    print(len(x_batch))



문제 115. 위의 코드를 5에폭 돌게 코드를 수정하시오 !

	x, t = get_data()
	for a in range(5):
	    for i in range(0, 10000, 100):
	    #     batch_mask = np.random.choice(10000, 100)
	    #     print(batch_mask)
	    #     x_batch = x[batch_mask]    # 복원 추출
	        x_batch = x[i:i+100]         # 비복원 추출
	        print([j for j in range(i, i+100)])
	        print(len(x_batch))




문제 116. 미니배치(100장)에 맞게끔 교차엔트로피 함수를 수정하시오 !

	def cross_entropy_error(x,t):
	    delta = 1e-7
	    return -np.sum(t*np.log(x+delta))/x.shape[0]





문제 117. y=2x^2+2의 함수를 loss2라는 이름으로 생성하시오 !

	def loss2(x):
	    return 2(x**2) + 2

	print(loss2(4))
	34



문제 118.  y=2x^2 + 2 의 함수의 x = 7인 곳의 기울기를 손으로 구하시오 !
	기울기를 구하려면 도함수를 알아야한다
	도함수 ? 함수를 미분해서 얻은 함수

	y = 4x
		28 <---- 기울기



문제 119. y=2x^2 + 2 의 함수의 x = 7인 곳의 기울기를 구하시오 !

	def numerical_diff(f,x):
	    h = 1e-4
	    return (f(x+h) - f(x-h)) / (2*h)
	
	def loss2(x):
	    return 2*(x**2) + 2
	
	print(numerical_diff(loss2,7)) # 파이썬에서는 함수 안에 함수를 넣을 수 있다.

	27.999999999934744



문제 120. 문제 119번의 함수를 접선의 미분 공식으로 만든 함수로 미분했을 때의 x=7에서의 기울기를 구하시오 !

	def numerical_diff(f,x):
	    h = 1e-4 # 0에 가까운 숫자 0.000000.....1(0이 50개)
	    return (f(x+h) - f(x)) / h
	
	print(numerical_diff(loss2,7))

	28.000199999951292




문제 121. 아래의 비용함수를 시각화 하시오 !

	y = x**2 + 4**2

	구글에서 y = x^2 + 4^2 로 검색창에 입력한다.




문제 122. 아래의 그래프를 구글에서 시각화 하시오 !

	f(x0, x1) = x0**2 + x1**2

	구글에서 z = x^2 + y^2로 검색창에 입력한다.



문제 123. 아래의 비용함수를 만들고 이 비용함수의 x = 6인 지점에서의 미분계수(기울기)를 손으로 구하시오 !

	y = 4x^2 +7^2

	답 : 48



문제 124. 위의 식을 loss3라는 비용함수로 만들고 loss3 함수를 파이썬으로 미분해서 x = 6인 지점의
	  미분계수(기울기) 를 구하시오 !

	def numerical_diff(f,x):
	    h = 1e-4
	    return (f(x+h) - f(x-h)) / (2*h)
	
	def loss3(x):
	    return 4*(x**2) + 7**2
	
	print(numerical_diff(loss3,6))
	
	47.99999999988813





문제 125. 손으로 위의 공식을 먼저 편미분을 수행하시오 !

	f(x0, x1) = x0**2 + x1**2 함수를 편미분 하는데
	x0 = 3, x1 = 4 일 때 x0에 대해서 편미분 하시오 !

	df/dx0(3,4) = 6
	df/dx1(3,4) = 8




문제 126. f(x0, x1) = x0**2 + x1**2 함수를 편미분 하는데 x0 = 3, x1 = 4 일 때 x0에 대해서 편미분을
	  파이썬으로 하시오 !

	def numerical_diff(f,x):
	    h = 1e-4
	    return (f(x+h) - f(x-h)) / (2*h)
	
	def loss3(x0):
	    return x0**2 + 4**2
	
	print(numerical_diff(loss3, 3))
	
	6.00000000000378




문제 127. f(x0, x1) = x0**2 + x1**2 함수를 편미분 하는데 x0 = 3, x1 = 4 일 때 x1에 대해서 편미분을
	  파이썬으로 하시오 !

	def numerical_diff(f,x):
	    h = 1e-4
	    return (f(x+h) - f(x-h)) / (2*h)
	
	def loss3(x1):
	    return 3**2 + x1**2
	
	print(numerical_diff(loss3, 4))

	7.999999999999119




문제 128. 위에서는 f(x0, x1) = x0**2 + x1**2 함수를 편미분하는것을 각각 수행했는데 이번에는 편미분이
	  한번에 수행되게끔 코드를 작성하시오 !

	import numpy as np
	
	def loss_func(x):
	    return x[0]**2 + x[1]**2
	
	def numerical_gradient(f,x):
	    h = 1e-4
	    grad = np.zeros_like(x)
	    for i in range(x.size):
	        dummy = x[i]
	        x[i] = dummy +h
	        fxh1 = f(x)
	        x[i] = dummy - h
	        fxh2 = f(x)
	        grad[i] = (fxh1 - fxh2) / (2*h)
	    return grad
	
	x = np.array([3.0, 4.0])
	print(numerical_gradient(loss_func, x))

	[ 6.  8.]



문제 129. 아래의 x0, x1지점에서의 기울기를 각각 구하시오 !

	x1 = np.array([3.0, 4.0])
	x2 = np.array([0.0, 2.0])
	x3 = np.array([3.0, 0.0])
	
	import numpy as np
	
	def loss_func(x):
	    return x[0]**2 + x[1]**2
	
	def numerical_gradient(f,x):
	    h = 1e-4
	    grad = np.zeros_like(x)
	    for i in range(x.size):
	        dummy = x[i]
	        x[i] = dummy +h
	        fxh1 = f(x)
	        x[i] = dummy - h
	        fxh2 = f(x)
	        grad[i] = (fxh1 - fxh2) / (2*h)
	    return grad
	
	print(numerical_gradient(loss_func, x1))
	print(numerical_gradient(loss_func, x2))
	print(numerical_gradient(loss_func, x3))

	[ 6.  8.]
	[ 0.  4.]
	[ 6.  0.]



문제 130. 책 131페이지에 나오는 경사 하상 함수 gradient_descent 함수를 생성하시오 !

	def gradient_descent(f, init_x, lr=0.01, step_num=100):
	    x = init_x
	    for i in range(step_num):
	        grad = numerical_gradient(f,x)
	        x -= lr*grad
	    return x




문제 131. 위에서 만든 gradient_descent 함수를 가지고 x[0] = 3.0, x[1] = 4.0 인 지점에서 학습을 100번
	  수행했을 때 global minima에 도착하는지 확인하시오 !

	x = np.array([3.0, 4.0])
	
	import numpy as np
	
	def loss_func(x):
	    return x[0]**2 + x[1]**2
	
	def numerical_gradient(f,x):
	    h = 1e-4
	    grad = np.zeros_like(x)
	    for i in range(x.size):
	        dummy = x[i]
	        x[i] = dummy +h
	        fxh1 = f(x)
	        x[i] = dummy - h
	        fxh2 = f(x)
	        grad[i] = (fxh1 - fxh2) / (2*h)
	    return grad
	
	def gradient_descent(f, init_x, lr=0.01, step_num=100):
	    x = init_x
	    for i in range(step_num):
	        grad = numerical_gradient(f,x)
	        x -= lr*grad
	    return x
	
	print(gradient_descent(loss_func, x))

	[ 0.39352177  0.52614132]




문제 132. 학습율을 0.1로 변경해서 수행해 보시오 !

	x = np.array([3.0, 4.0])
	
	import numpy as np
	
	def loss_func(x):
	    return x[0]**2 + x[1]**2
	
	def numerical_gradient(f,x):
	    h = 1e-4
	    grad = np.zeros_like(x)
	    for i in range(x.size):
	        dummy = x[i]
	        x[i] = dummy +h
	        fxh1 = f(x)
	        x[i] = dummy - h
	        fxh2 = f(x)
	        grad[i] = (fxh1 - fxh2) / (2*h)
	    return grad
	
	def gradient_descent(f, init_x, lr=0.01, step_num=100):
	    x = init_x
	    for i in range(step_num):
	        grad = numerical_gradient(f,x)
	        x -= lr*grad
	    return x.round()
	
	print(gradient_descent(loss_func, x, 0.1))




문제 133. 러닝 레이트(학습률)을 크게 주고 학습을 하면 결과가 어떻게 나오는지 확인하고 러닝 레이트(학습률)을
	  작게 주고 학습을 하면 결과가 어떻게 나오는지 확인하시오 !

	#1. 학습률 : 10 ----> ?
	#2. 학습률 : 1e-10 -----> ?
	
	x = np.array([3.0, 4.0])
	
	import numpy as np
	
	def loss_func(x):
	    return x[0]**2 + x[1]**2
	
	def numerical_gradient(f,x):
	    h = 1e-4
	    grad = np.zeros_like(x)
	    for i in range(x.size):
	        dummy = x[i]
	        x[i] = dummy +h
	        fxh1 = f(x)
	        x[i] = dummy - h
	        fxh2 = f(x)
	        grad[i] = (fxh1 - fxh2) / (2*h)
	    return grad
	
	def gradient_descent(f, init_x, lr=0.01, step_num=100):
	    x = init_x
	    for i in range(step_num):
	        grad = numerical_gradient(f,x)
	        x -= lr*grad
	    return x
	
	print(gradient_descent(loss_func, x, 10))
	# print(gradient_descent(loss_func, x, lr2))
	
	[  2.58983747e+13  -1.24261502e+12]
	[ 2.98999994  3.98999992]



문제 134. 2 x 3 행렬을 생성하는데 값은 랜덤 값으로 생성되게 하시오 !

	import numpy as np
	
	w = np.random.randn(2,3)
	print(w)

	[[-0.52436211 -0.18096486  0.4574922 ]
	 [ 0.35155406 -0.25271191 -1.63404372]]




문제 135. 위에서 구한 w값으로 아래의 입력값과의 행렬 내적을 출력하시오 !

	import numpy as np
	
	w = np.random.randn(2,3)
	x = np.array([0.6, 0.9])
	
	print(np.dot(x,w))
	
	[-0.69005445  0.74740697  1.40580175]



문제 136.  문제 134번 코드를 __init__라는 함수로 생성하시오 !

	def __init__():
	    w = np.random.randn(2,3)
	    return w
	
	print(__init__())

	[[-0.22429096 -0.87969553 -1.33996677]
	 [-0.52800461  0.21629998 -0.84615578]]




문제 137. 위에서 만든 가중치와 아래의 입력값을 받아 행렬을 내적하는 predict이라는 함수를 만드시오 !

	x = np.array([0.6, 0.9])
	w = __init__()
	
	def predict(x, w):
	    return np.dot(x, w)
	
	print(predict(x,w))
	
	[-1.19477317  0.88869325  1.80707382]




문제 138. 위의 predict 함수에 나온 결과를 softmax출력층 함수에 통과시킨 결과를 출력하시오 !

	def __init__():
	    w = np.random.randn(2,3)
	    return w
	
	def predict(x, w):
	    return np.dot(x, w)
	
	def softmax(a):
	    c = np.max(a)
	    minus = a - c
	    exp_a = np.exp(minus)
	    sum_exp_a = np.sum(exp_a)
	    y = exp_a / sum_exp_a
	    return y
	
	x = np.array([0.6, 0.9])
	w = __init__()
	
	print(softmax(predict(x,w)))
	
	[ 0.40787945  0.1027099   0.48941065]




문제 139. 위에서 출력된 소프트맥스 함수의 결과와 아래의 target값과의 오차를 출력하기위해
	  cross_entropy_error함수를 생성하고 cross_entropy_error함수를 통과한 결과를 출력하시오 !

	def __init__():
	    w = np.random.randn(2,3)
	    return w
	
	def predict(x, w):
	    return np.dot(x, w)
	
	def softmax(a):
	    c = np.max(a)
	    minus = a - c
	    exp_a = np.exp(minus)
	    sum_exp_a = np.sum(exp_a)
	    y = exp_a / sum_exp_a
	    return y
	
	def cross_entropy_error(x,t):
	    delta = 1e-7
	    return -np.sum(t*np.log(x+delta))
	
	x = np.array([0.6, 0.9])
	w = __init__()
	y = softmax(predict(x,w))
	t = np.array([0, 0, 1])
	
	print(cross_entropy_error(y, t))

	0.575857971144




문제 140. 위의 simpleNet클래스의 loss함수를 실행시켜 보시오 !

	import numpy as np
	
	net = simpleNet()
	x = np.array([0.6, 0.9])
	t = np.array([0, 0, 1])
	print(net.loss(x,t))

	2.73113096106




문제 141. simpleNet 클래스의 가중치 W가 몇행 몇열인지 출력하시오 !

	net = simpleNet()
	print(net.w.shape)

	(2, 3)




문제 142. 칠판에 나온대로 simpleNet클래스의 __init__함수를 가중치 W1은 784 x 50 으로 생성되게 하고 가중치
	  W2는 50 x 10 으로 가중치가 랜덤으로 생성되게 코드를 수정하시오 !

	import numpy as np
	
	class simpleNet:
	    def __init__(self):
	        self.params = {}
	        self.params['W1'] = 0.01 * np.random.randn(784, 50)  # 0.01 --> 가중치 초기화에 대한 값
	        self.params['b1'] = np.zeros(50)                     #          6장에서 배운다
	        self.params['W2'] = 0.01 * np.random.randn(50, 10)
	        self.params['b2'] = np.zeros(10)




문제 143. simpleNet 클래스의 가중치 W1과 W2의 shape을 출력하시오 !

	net = simpleNet()
	
	print(net.params.get('W1').shape)
	print(net.params.get('W2').shape)
	print(net.params.get('b1').shape)
	print(net.params.get('b1').shape)

	(784, 50)
	(50, 10)
	(50,)
	(50,)




문제 144. simpleNet class를 객체화 시킬 때 아래와 같이 실행되게 하시오 !

	import numpy as np
	
	class simpleNet:
	    def __init__(self, input_size, hidden_size, output_size):
	        self.params = {}
	        self.params['W1'] = 0.01 * np.random.randn(input_size, hidden_size)
	        self.params['b1'] = np.zeros(hidden_size)
	        self.params['W2'] = 0.01 * np.random.randn(hidden_size, output_size)
	        self.params['b2'] = np.zeros(output_size)


	net = simpleNet(input_size = 784, hidden_size = 50, output_size = 10)
	print('W1', net.params['W1'].shape)
	print('W2', net.params['W2'].shape)
	print('b1', net.params['b1'].shape)
	print('b2', net.params['b2'].shape)

	W1 (784, 50)
	W2 (50, 10)
	b1 (50,)
	b2 (10,)




문제 146. mnist데이터 100장만 입력해서 predict 함수를 통과한 결과가 어떻게 되는지 확인하시오 !

	import numpy as np
	from dataset.mnist import load_mnist

	def  get_data():
	    (x_train, t_train) , (x_test, t_test) = \
	    load_mnist(normalize=True, flatten=True, one_hot_label=True)
	    return  x_test, t_test
	
	
	class simpleNet:
	    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):
	        self.params = {}
	        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)
	        self.params['b1'] = np.zeros(hidden_size)
	        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)
	        self.params['b2'] = np.zeros(output_size)
	
	    def sigmoid(self, data):
	        return 1/( 1 + np.exp(-data) )
	
	    def predict(self,x):
	        W1, W2 , b1, b2 = self.params['W1'], self.params['W2'], self.params['b1'], self.params['b2']
	        a = np.dot(x, W1) + b1
	        a1 = self.sigmoid(a)
	        b = np.dot(a1, W2) + b2
	        b1 = self.sigmoid(b)
	        return b1
	
	    def softmax(self,a):
	        c = np.max(a)
	        minus = a - c
	        exp_a = np.exp(minus)
	        sum_exp_a = np.sum(exp_a)
	        y = exp_a / sum_exp_a
	        return y
	
	    def cross_entropy_error(self,x,t):
	        delta = 1e-7
	        return -np.sum(t*np.log(x+delta))
	    
	    def loss(self,x,t):
	        p = self.predict(x)
	        s = self.softmax(p)
	        loss = self.cross_entropy_error(s, t)
	        return loss
	    
	net = simpleNet(input_size = 784, hidden_size = 50, output_size = 10)
	x, t = get_data()
	y = net.predict(x[:100])
	print(y.shape)
	
	(100, 10)



문제 147. 정확도를 구하는 accuracy함수를 simpleNet클래스에 추가하고 100장을 입력해서 정확도를
	  출력해보시오 !

	import numpy as np
	from dataset.mnist import load_mnist
	
	class simpleNet:
	    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):
	        self.params = {}
	        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)
	        self.params['b1'] = np.zeros(hidden_size)
	        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)
	        self.params['b2'] = np.zeros(output_size)
	
	    def  get_data():
	        (x_train, t_train) , (x_test, t_test) = \
	        load_mnist(normalize=True, flatten=True, one_hot_label=True)
	        return  x_test, t_test
	    
	    def sigmoid(self, data):
	        return 1/( 1 + np.exp(-data) )
	
	    def predict(self,x):
	        W1, W2 , b1, b2 = self.params['W1'], self.params['W2'], self.params['b1'], self.params['b2']
	        a = np.dot(x, W1) + b1
	        a1 = self.sigmoid(a)
	        b = np.dot(a1, W2) + b2
	        b1 = self.sigmoid(b)
	        return b1
	
	    def accuracy(self, x, t):
	        y = self.predict(x)
	        y = np.argmax(y, axis=1)
	        t = np.argmax(t, axis=1)
	        accuracy = np.sum(y == t) / float(x.shape[0])
	        return accuracy
	
	    def softmax(self,a):
	        c = np.max(a)
	        minus = a - c
	        exp_a = np.exp(minus)
	        sum_exp_a = np.sum(exp_a)
	        y = exp_a / sum_exp_a
	        return y
	
	    def cross_entropy_error(self,x,t):
	        delta = 1e-7
	        return -np.sum(t*np.log(x+delta))
	    
	    def loss(self,x,t):
	        p = self.predict(x)
	        s = self.softmax(p)
	        loss = self.cross_entropy_error(s, t)
	        return loss
	    
	net = simpleNet(input_size = 784, hidden_size = 50, output_size = 10)
	x, t = get_data()
	print(net.accuracy(x[:100], t[:100]))
	
	0.14




문제 148. x[0]와 t[0]을 입력해서 얻은 기울기 grads['W1'], grads['b1'], grads['W2'], grads['b2']
	  행렬의 shape는 어떻게 되는가?

	import numpy as np
	from dataset.mnist import load_mnist
	from common.gradient import numerical_gradient

	class simpleNet:
	    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):
	        self.params = {}
	        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)
	        self.params['b1'] = np.zeros(hidden_size)
	        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)
	        self.params['b2'] = np.zeros(output_size)
	
	    def  get_data(self):
	        (x_train, t_train) , (x_test, t_test) = \
	        load_mnist(normalize=True, flatten=True, one_hot_label=True)
	        return  x_test, t_test
	    
	    def sigmoid(self, data):
	        return 1/( 1 + np.exp(-data) )
	
	    def predict(self,x):
	        W1, W2 , b1, b2 = self.params['W1'], self.params['W2'], self.params['b1'], self.params['b2']
	        a = np.dot(x, W1) + b1
	        a1 = self.sigmoid(a)
	        b = np.dot(a1, W2) + b2
	        b1 = self.sigmoid(b)
	        return b1
	
	    def accuracy(self, x, t):
	        y = self.predict(x)
	        y = np.argmax(y, axis=1)
	        t = np.argmax(t, axis=1)
	        accuracy = np.sum(y == t) / float(x.shape[0])
	        return accuracy
	
	    def softmax(self,a):
	        c = np.max(a)
	        minus = a - c
	        exp_a = np.exp(minus)
	        sum_exp_a = np.sum(exp_a)
	        y = exp_a / sum_exp_a
	        return y
	
	    def cross_entropy_error(self,x,t):
	        delta = 1e-7
	        return -np.sum(t*np.log(x+delta))
	    
	    def loss(self,x,t):
	        p = self.predict(x)
	        s = self.softmax(p)
	        loss = self.cross_entropy_error(s, t)
	        return loss
	
	    def numerical_differentiation(self,x,t):
	        loss_W = lambda W: self.loss(x, t)
	        grads = {}
	        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])
	        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])
	        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])
	        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])
	        return grads
	
	net = simpleNet(input_size = 784, hidden_size = 50, output_size = 10)
	x, t = net.get_data()
	y = net.numerical_differentiation(x[0], t[0])
	print(y['W1'].shape)
	print(y['b1'].shape)
	print(y['W2'].shape)
	print(y['b2'].shape)

	(784, 50)
	(50,)
	(50, 10)
	(10,)

	그림 4-_-6




문제 149. 지금까지 완성한 simpleNet 클래스를 가지고 만든 신경망에 훈련데이터 100장씩 미니배치해서
	  학습시키는 코드를 구현하시오 !

	import numpy as np
	from dataset.mnist import load_mnist
	from common.gradient import numerical_gradient
	
	(x_train, t_train) , (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)
	
	train_loss_list = []
	
	#하이퍼 파라미터
	iters_num = 10000 #반복횟수
	train_size = x_train.shape[0]
	batch_size = 100
	learning_rate = 0.1
	net = simpleNet(input_size = 784, hidden_size = 50, output_size = 10)
	
	for i in range(iters_num):
	    #미니배치 획득
	    batch_mask = np.random.choice(train_size, batch_size)
	    x_batch = x_train[batch_mask]
	    t_batch = t_train[batch_mask]
	    
	    #기울기 계산
	    grad = net.numerical_differentiation(x_batch, t_batch)
	
	    #매개변수 갱신
	    for key in ('W1', 'b1', 'W2', 'b2'):
	        net.params[key] -= learning_rate * grad[key]
	    
	    #학습 경과 기록
	    loss = net.loss(x_batch, t_batch)
	    train_loss_list.append(loss)




문제 150. 위의 train_loss_list 에 들어간 오차들을 그래프로 시각화 하면 어떻게 되겠는가?

	




문제 151. 위의 신경망의 정확도를 시각화 하면 어떻게 되겠는가 ?

	
문제 152. 위의 정확도 그래프를 실제로 파이썬 코드로 시각화 하시오 !

	코드 참고 (수치미분_오차역전파 코드를 그래프까지 그리는 코드.txt)





문제 153. 위의 코드를 수정해서 그림 4-11 에 해당하는 오차 그래프를 그리시오 !
	  ( 오차가 점점 줄어드는 그래프를 그리시오 )

	# 그래프 그리기
	
	markers = {'train': 'o', 'test': 's'}
	x = np.arange(len(train_loss_list))
	plt.plot(x, train_loss_list, label='loss')
	plt.xlabel("iteration")
	plt.ylabel("loss")
	plt.ylim(0, 1.0)
	plt.legend(loc='lower right')
	plt.show()




문제 154. 방금 학습시킨 2층 신경망의 가중치와 바이어스를 two_layer_w.pkl 이라는 pickle 파일로
	  생성하시오 !

	import pickle
	
	with open('d:\\two_layer_w.pkl', 'wb') as f:
	    pickle.dump(network.params,f)
	
		※ 설명 : 지금 생성한 pickle 파일은 2층 신경망의 가중치와 바이어스
			  (W1,W2,b1,b2) 의 값을 저장한 파일이다.





문제 155. 지난번 저자가 만들어온 가중치와 바이어스의 pickle 파일을 로드해서 만든 3층 신경망에 nist 숫자
	  데이터 한장을 넣어서 인공지능이 잘 맞추는지 확인하는 코드를 다시 돌려보시오 !

	import sys, os
	sys.path.append(os.pardir)
	
	import numpy as np
	from dataset.mnist import load_mnist
	from PIL import Image
	import  pickle
	import  numpy  as np
	
	# 신경망 함수들
	def sigmoid(num):
	    rst = (1 / (1 + np.exp(-num)))
	    return (rst)

	def identity_function(x):
	    return x
	
	def softmax(a):
	    c = np.max(a)
	    minus = a - c
	    exp_a = np.exp(minus)
	    sum_exp_a = np.sum(exp_a)
	    y = exp_a / sum_exp_a
	    return y
	
	def  init_network():
	    with open("d:\\two_layer_w.pkl",'rb')  as  f:
	        network = pickle.load(f)
	    return  network
	
	
	def  predict(network, x):
	    W1, W2, W3 = network['W1'], network['W2'], network['W3']
	    b1, b2, b3 = network['b1'], network['b2'], network['b3']
	    a1 = np.dot(x,W1) + b1
	    z1 = sigmoid(a1)
	    a2 = np.dot(z1,W2) + b2
	    z2 = sigmoid(a2)
	    a3 = np.dot(z2,W3) + b3
	    y = softmax(a3)
	    return  y
	
	def  get_data():
	    (x_train, t_train) , (x_test, t_test) = \
	    load_mnist(normalize=True, flatten=True, one_hot_label=False)
	    return  x_test, t_test
	
	x, t = get_data()
	network = init_network()
	y = predict(network,x[0])
	print(np.argmax(y))





문제 156. 위의 코드를 수정해서 지금 방금 만든 two_layer_w.pkl 파일을 로드해서 x[0] 숫자를 잘
	  맞추는지 확인하시오 !

	저자가 만든 신경망	vs 	우리가 만든 신경망
	
		3층 신경망		2층 신경망
	





문제 157. 지금 pickle 파일 로드해서 만든 2층 신경망에 선혜가 만든 필기체 그림을 입력해서 숫자 4를 잘
	  맞추는지 확인하시오 !
	  (점심시간 문제)

	import sys, os
	sys.path.append(os.pardir)
	
	import numpy as np
	from dataset.mnist import load_mnist
	from PIL import Image
	import  pickle
	import  numpy  as np

	# 신경망 함수들
	def sigmoid(num):
	    rst = (1 / (1 + np.exp(-num)))
	    return (rst)
	
	def identity_function(x):
	    return x
	
	def softmax(a):
	    c = np.max(a)
	    minus = a - c
	    exp_a = np.exp(minus)
	    sum_exp_a = np.sum(exp_a)
	    y = exp_a / sum_exp_a
	    return y
	
	def  init_network():
	    with open("d:\\two_layer_w.pkl",'rb')  as  f:
	        network = pickle.load(f)
	    return  network
	
	def  predict(network, x):
	    W1, W2= network['W1'], network['W2']
	    b1, b2= network['b1'], network['b2']
	    a1 = np.dot(x,W1) + b1
	    z1 = sigmoid(a1)
	    a2 = np.dot(z1,W2) + b2
	    y = softmax(a2)
	    return  y
	
	def  get_data():
	    (x_train, t_train) , (x_test, t_test) = \
	    load_mnist(normalize=True, flatten=True, one_hot_label=False)
	    return  x_test, t_test
	
	import cv2
	j = 'D:\\5pic.jpg'
	import numpy as np
	import matplotlib.pyplot as plt
	import matplotlib.image as mpimg
	
	def rgb2gray(rgb):
	    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])
	
	img = mpimg.imread(j)
	gray = rgb2gray(img)
	a = np.array(gray)
	pic= a.flatten()
	print(pic.shape)
	
	x, t = get_data()
	network = init_network()
	y = predict(network,pic)
	
	print(np.argmax(y))




문제 158. 곱셈 계층을 파이썬으로 구현하시오 ! ( p 161)

	class MulLayer:
	    def __init__(self):
	        self.x = None
	        self.y = None
	    
	    def forward(self, x, y):
	        self.x = x
	        self.y = y
	        out = x * y
	        return out
	    
	    def backward(self, out):
	        dx = dout * self.y
	        dy = dout * self.x
	        return dx, dy




문제 159. 위에서 만든 곱셈 클래스를 객체화 시켜서 사과 가격의 총 가격을 구하시오 !

	apple =100
	apple_num = 2
	tax = 1.1
	
	#계층
	mul_apple_layer = MulLayer()
	mul_tax_layer = MulLayer()
	
	#순전파
	apple_price = mul_apple_layer.forward(apple, apple_num)
	price = mul_tax_layer.forward(apple_price, tax)
	
	print(price)
	
	220.00000000000003




문제 160. 덧셈 계층을 파이썬으로 구현하시오 ! (클래스 생성) (p 163)

	class AddLayer:
	    def __init__(self):
	        pass
	    
	    def forward(self, x, y):
	        out = x + y
	        return out
	    
	    def backward(self, dout):
	        dx = dout * 1  # 덧셈노드는 상류값을 여과없이 하류로 흘려보냄
	        dy = dout * 1
	        return dx, dy




문제 161. 위에서 만든 곱셈 클래스와 덧셈 클래스를 이용해서 책 149쪽의 그림 5-3의 신경망을 구현하시오 !

	apple = 100
	apple_num = 2
	orange = 150
	orange_num = 3
	tax = 1.1
	
	mul_apple_layer = MulLayer()
	mul_orange_layer = MulLayer()
	add_total_layer = AddLayer()
	mul_tax_layer = MulLayer()
	
	apple_price = mul_apple_layer.forward(apple, apple_num)
	orange_price = mul_orange_layer.forward(orange, orange_num)
	total_layer = add_total_layer.forward(apple_price, orange_price)
	price = mul_tax_layer.forward(total_layer, tax)
	print(price)




문제 162. 이번에는 순전파로 출력한 과일 가격의 총합 신경망의 역전파를 구현하시오 !

	dprice = 1
	dtotal_layer, dtax = mul_tax_layer.backward(dprice)
	dapple_price, dorange_price = add_total_layer.backward(dtotal_layer)
	dapple, dapple_num = mul_apple_layer.backward(dapple_price)
	dorange, dorange_num = mul_orange_layer.backward(dorange_price)
	print(dapple, dapple_num, dorange, dorange_num, dtax)




문제 163. 책 166페이지의 Relu클래스를 생성하시오 !

	from copy import copy
	
	class Relu:
	    def __init__(self):
	        self.mask = None
	        
	    def forward(self, x):
	        self.mask = (x <= 0)
	        out = x.copy()
	        out[self.mask] = 0
	        return out
	    
	    def backward(self, dout):
	        dout[self.mask] = 0
	        dx = dout
	        return dx
	    
	import numpy as np
	x = np.array([[1.0, -0.5], [-2.0, 3.0]])
	relu = Relu()
	print(relu.forward(x))
	
	[[ 1.  0.]
	 [ 0.  3.]]




문제 164. 위의 relu함수의 역전파를 실행하시오 !
	  역전파때 사용하는 입력값은 순전파의 결과값을 그대로 사용하시오 !

	import numpy as np
	x = np.array([[1.0, -0.5], [-2.0, 3.0]])
	
	relu = Relu()
	
	#순전파(mask정보 생성)
	print(relu.forward(x))
	
	[[ 1.  0.]
	 [ 0.  3.]]

	#역전파
	dout = np.array([[2.0, 3.0], [-3.0, -4.0]])
	print(relu.backward(x))

	[[ 2.  0.]
	 [ 0. -4.]]

		※ 설명 : 순전파일 때 사용했던 부울 값이 있는 mask정보를 역전파일때도 사용하는 이유는
			  순전파일때 전파가 되지 않았으면 역전파일때도 역전파가 되지 않게 하기 위해서
			  이다.




문제 165. 파이썬으로 sigmoid 클래스를 생성하시오 ! (p 170)

	class Sigmoid:
	    def __init__(self):
	        self.out = None
	    
	    def forward(self, x):
	        out = 1 / (1 + np.exp(-x))
	        self.out = out
	        return out
	    
	    def backward(self, dout):
	        dx = dout * (1.0 - self.out) * self.out
	        return dx





문제 166. sigmoid클래스를 객체화 시켜서 아래의 입력값을 순전파로 보낸 결과와 역전파로 나온 결과를 출력하시오 !

	sig = Sigmoid()

	#순전파
	x = np.array([[2.0, 4.0], [-0.3, 0.9]])
	print(sig.forward(x))
	
	[[ 0.88079708  0.98201379]
	 [ 0.42555748  0.7109495 ]]
	
	#역전파(순전파 결과값 정보 필요)
	dout = np.array([[4.0, 2.0], [-0.1, 0.8]])
	print(sig.backward(dout))
	
	[[ 0.41997434  0.03532541]
	 [-0.02444583  0.16440025]]

		※ 설명 170페이지 이 구현에서는 순전파의 출력을 인스턴스 변수 out에 보관했다가





문제 167. 파이썬으로 Affine 클래스를 생성하시오 ! (p 175)

	class Affine:
	    def __init__(self, W, b):
	        self.W = W
	        self.b = b
	        self.x = None
	        self.dW = None
	        self.db = None
	        
	    def forward(self, x):
	        self.x = x
	        out = np.dot(x, self.W) + self.b
	        return out
	    
	    def backward(self, dout):
	        dx = np.dot(dout, self.W.T)
	        self.dW = np.dot(self.x.T, dout)
	        self.db = np.sum(dout, axis = 0)
	        return dx




문제 168. 위의 Affine 클래스를 객체화시켜 아래의 입력값 행렬, 가중치 행렬, 바이어스 행렬을 흘려보낸
	  순전파 결과가 어떻게 되는지 출력하시오 !

	x = np.random.rand(2)
	W = np.random.rand(2,3)
	b = np.random.rand(3)
	
	print(x)
	print(W)
	print(b)
	
	affine = Affine(W, b)
	
	print(affine.forward(x))




문제 169. (점심시간 문제)아래의 수학식을 증명하시오 !
	  (손으로 써서 증명 또는 파이썬으로 증명)

	X ◎ W != W ◎ X   (교환 법칙이 성립하지 않는다)

	[ a b ]      [ e f ]      [ ae+bg  af+bh ]
                 ◎            =  
	[ c d ]      [ g h ]      [ ce+dg  cf+dh ]


	[ e f ]      [ a b ]      [ ae+fc  eb+fd ]
                 ◎            =  
	[ g h ]      [ c d ]      [ ga+hd  bg+dh ]

		
	[ ae+bg  af+bh ]               [ ae+fc  eb+fd ]
       			       0!=        
	[ ce+dg  cf+dh ]               [ ga+hd  bg+dh ]
 



문제 170. X ◎ W = (W^T ◎ X^T)^T 라는 것을 파이썬으로 증명하시오 !

	X = np.array([[1, 2], [3, 4]])
	W = np.array([[5, 6], [7, 8]])
	
	print(np.dot(X, W))
	print(np.dot(W.T, X.T).T)

	[[19 22]
	 [43 50]]
	[[19 22]
	 [43 50]]




문제 171. 2 x 3 행렬을 전치시키면 ? x ? 행렬일지 파이썬으로 구현해 보시오 !

	x = np.array([[1, 2, 3], [4, 5, 6]])
	print(x.T.shape)

	(3, 2)




문제 172. 2 x 3 ◎ 3 x 4 = 2 x 4  --->  4 x 3 ◎ 3 x 2 = 4 x 2 ----> 2 x 4
	  위의 식을 파이썬으로 구현하시오 !

	x = np.array([[1, 2, 3], [4, 5, 6]])
	y = np.array([[7, 8, 9, 10], [11, 12, 13, 14], [15, 16, 17, 18]])
	
	print(x.shape, 'x', y.shape, '=', np.dot(x,y).shape)
	print('=',np.dot(x,y))
	print('-------------------------')
	print(y.T.shape, 'x', x.T.shape, '=', np.dot(y.T, x.T).shape)
	print('->', np.dot(y.T,x.T).T.shape)
	print('=', np.dot(y.T,x.T).T)




문제 173. 문제 167번에서 구성한 Affine클래스를 객체화 시켜서 forward와 backward를 구현하시오 !

	X = np.array([[1, 2], [3, 4]])
	W = np.array([[1, 3, 5], [2, 4, 6]])
	b = np.array([[1, 1, 1], [1, 1, 1]])
	dout = np.array([[1, 2, 3], [4, 5, 6]])
	
	affine = Affine(W, b)
	print(affine.forward(X))
	print(affine.backward(dout))
	
	[[ 6 12 18]
	 [12 26 40]]
	[[22 28]
	 [49 64]]




문제 174. Affine 클래스를 이용해서 2층 신경망의 순전파를 구현하시오 !
	그림 5-_-3

	x = np.array([[1,2]])
	
	w1 = np.array([[1, 3, 5], [2, 4, 6]])
	b1 = np.array([[1, 2, 3]])
	
	w2 = np.array([[1, 4], [2, 5], [3, 6]])
	b2 = np.array([[1, 2]])
	
	affine1 = Affine(w1, b1)
	affine2 = Affine(w2, b2)
	
	out1 = affine1.forward(x)
	out2 = affine2.forward(out1)
	print(out2)

	[[ 93 211]]




문제 175. 위의 2층신경망의 은닉층(1층)에 활성화 함수로 Relu를 추가해서 구현하시오 !
	  (Relu 클래스를 가져와서 객체화 시켜야 한다.)

	x = np.array([[1,2]])
	
	w1 = np.array([[1, 3, 5], [2, 4, 6]])
	b1 = np.array([[1, 2, 3]])
	
	w2 = np.array([[1, 4], [2, 5], [3, 6]])
	b2 = np.array([[1, 2]])
	
	affine1 = Affine(w1, b1)
	relu1 = Relu()
	affine2 = Affine(w2, b2)
	
	out1 = affine1.forward(x)
	out1_hat = relu1.forward(out1)
	out2 = affine2.forward(out1_hat)
	print(out2)

	[[ 93 211]]




문제 176. 위의 2층 신경망의 역전파를 Affine 클래스를 이용해서 구현하시오 !
	  역전파시 입력할 dY는 위의 순전파의 결과인 out2행렬값을 그대로 사용하시오 !

	x = np.array([[1,2]])
	
	w1 = np.array([[1, 3, 5], [2, 4, 6]])
	b1 = np.array([[1, 2, 3]])
	
	w2 = np.array([[1, 4], [2, 5], [3, 6]])
	b2 = np.array([[1, 2]])
	
	# 각 층 객체화
	affine1 = Affine(w1, b1)
	relu1 = Relu()
	affine2 = Affine(w2, b2)
	
	# 순전파
	out1 = affine1.forward(x)
	out1_hat = relu1.forward(out1)
	out2 = affine2.forward(out1_hat)
	
	# 역전파
	dout2 = out2
	dout1_hat = affine2.backward(dout2)
	dout1 = relu.backward(dout1_hat)
	dx = affine1.backward(dout1)
	dw1 = affine1.dW
	db1 = affine1.db
	
	print(dx)
	print(dw1)
	print(db1)

	[[12385 16108]]
	[[ 937 1241 1545]
	 [1874 2482 3090]]
	[ 937 1241 1545]




문제 177. (오늘의 마지막 문제) 위의 2층 신경망을 3층 신경망으로 변경하시오 !

	x = np.array([[1,2]])
	
	w1 = np.array([[1, 3, 5], [2, 4, 6]])
	b1 = np.array([[1, 2, 3]])
	
	w2 = np.array([[1, 4], [2, 5], [3, 6]])
	b2 = np.array([[1, 2]])
	
	w3 = np.array([[5, 6, 7], [2, 3, 4]])
	b3 = np.array([[1, 1, 1]])
	
	#각 층 객체화
	affine1 = Affine(w1, b1)
	relu1 = Relu()
	affine2 = Affine(w2, b2)
	relu2 = Relu()
	affine3 = Affine(w3, b3)
	
	# 순전파
	out1 = affine1.forward(x)
	out1_hat = relu1.forward(out1)
	out2 = affine2.forward(out1_hat)
	out2_hat = relu2.forward(out2)
	out3 = affine3.forward(out2_hat)
	print(out3)

	[[ 888 1192 1496]]


문제 179. 위에서 만든 SoftmaxWithLoss클래스를 객체화 시켜서 아래의 x (입력값), t(target value)를
	  입력해서 순전파의 오차율을 출력하고 역전파도 출력하시오 !

	t = np.array([0,0,1,0,0,0,0,0,0,0])   # 숫자2
	
	x1 = np.array([0.01,0.01,0.01,0.01,0.01,0.01,0.05,0.3,0.1,0.5])
	x2 = np.array([0.01,0.01,0.9,0.01,0.01,0.01,0.01,0.01,0.01,0.02])
	
	# 클래스 객체화
	swl1 = SoftmaxWithLoss()
	swl2 = SoftmaxWithLoss()
	
	#순전파
	out1 = swl1.forward(x1, t)
	out2 = swl2.forward(x2, t)
	print(out1)
	print(out2)
	
	2.40727986639
	1.5475681948
	
	
	#역전파
	dx1 = swl1.backward()
	dx2 = swl2.backward()
	print(dx1)
	print(dx2)
	
	[ 0.00900598  0.00900598 -0.09099402  0.00900598  0.00900598  0.00900598
	  0.00937352  0.01203584  0.00985412  0.01470061]
	[ 0.0087373   0.0087373  -0.07872354  0.0087373   0.0087373   0.0087373
	  0.0087373   0.0087373   0.0087373   0.00882511]




문제 180. 아래의 x값 9개를 t과 비교해서 오차율이 아래와 같이 출력되게 하시오 !
	  ( SoftmaxWithLoss클래스를 이용해서)

	t = [0,0,1,0,0,0,0,0,0,0]    # 숫자2
	
	x1 = [0.1,0.05,0.1,0.0,0.05,0.1,0.0,0.1,0.0,0.0]  
	x2 = [0.1,0.05,0.2,0.0,0.05,0.1,0.0,0.6,0.0,0.0]
	x3 = [0.0,0.05,0.3,0.0,0.05,0.1,0.0,0.6,0.0,0.0]
	x4 = [0.0,0.05,0.4,0.0,0.05,0.0,0.0,0.5,0.0,0.0]
	x5 = [0.0,0.05,0.5,0.0,0.05,0.0,0.0,0.4,0.0,0.0]
	x6 = [0.0,0.05,0.6,0.0,0.05,0.0,0.0,0.3,0.0,0.0]
	x7 = [0.0,0.05,0.7,0.0,0.05,0.0,0.0,0.2,0.0,0.0]
	x8 = [0.0,0.1,0.8,0.0,0.1,0.0,0.0,0.2,0.0,0.0]
	x9 = [0.0,0.05,0.9,0.0,0.05,0.0,0.0,0.0,0.0,0.0]
	
	for i in range(1, 10):
	    swl = SoftmaxWithLoss()
	    print('x%d의 오차 : '%i, swl.forward(eval('x'+str(i)), t))




문제 181. 어제 마지막 문제였던 문제 229번 3층 신경망의 마지막 층으로 SoftmaxWithLoss 클래스를 추가하시오 !
	
	import numpy as np
	
	x = np.array([[1,2]])
	t = np.array([0, 0, 1])
	
	w1 = np.array([[1, 3, 5], [2, 4, 6]])
	b1 = np.array([[1, 2, 3]])
	
	w2 = np.array([[1, 4], [2, 5], [3, 6]])
	b2 = np.array([[1, 2]])
	
	w3 = np.array([[5, 6, 7], [2, 3, 4]])
	b3 = np.array([[1, 1, 1]])
	
	#각 층 객체화
	affine1 = Affine(w1, b1)
	relu1 = Relu()
	affine2 = Affine(w2, b2)
	relu2 = Relu()
	affine3 = Affine(w3, b3)
	swl = SoftmaxWithLoss()
	
	# 순전파
	out1 = affine1.forward(x)
	out1_hat = relu1.forward(out1)
	out2 = affine2.forward(out1_hat)
	out2_hat = relu2.forward(out2)
	out3 = affine3.forward(out2_hat)
	result = swl.forward(out3, t)
	print(result)

	-9.99999950584e-08




문제 182. 이번에는 문제 181번 코드의 역전파 코드를 구현하시오 ! dout = 1로 한다.

	import numpy as np
	
	x = np.array([[1,2]])
	t = np.array([0, 0, 1])
	
	w1 = np.array([[1, 3, 5], [2, 4, 6]])
	b1 = np.array([[1, 2, 3]])
	
	w2 = np.array([[1, 4], [2, 5], [3, 6]])
	b2 = np.array([[1, 2]])
	
	w3 = np.array([[5, 6, 7], [2, 3, 4]])
	b3 = np.array([[1, 1, 1]])
	
	#각 층 객체화
	affine1 = Affine(w1, b1)
	relu1 = Relu()
	affine2 = Affine(w2, b2)
	relu2 = Relu()
	affine3 = Affine(w3, b3)
	swl = SoftmaxWithLoss()
	
	# 순전파
	out1 = affine1.forward(x)
	out1_hat = relu1.forward(out1)
	out2 = affine2.forward(out1_hat)
	out2_hat = relu2.forward(out2)
	out3 = affine3.forward(out2_hat)
	result = swl.forward(out3, t)
	# print(result)
	
	# 역전파
		
	dout3 = swl.backward()
	dout2_hat = affine3.backward(dout3)
	dout2 = relu2.backward(dout2_hat)
	dout1_hat = affine2.backward(dout2)
	dout1 = relu1.backward(dout1_hat)
	dx = affine1.backward(dout1)
	dw1 = affine1.dW
	db1 = affine1.db
	print(dx)
	print(dw1)
	print(db1)
	
	[[  8.76920959e-131   1.13151091e-130]]
	[[  5.65755457e-132   8.48633186e-132   1.13151091e-131]
	 [  1.13151091e-131   1.69726637e-131   2.26302183e-131]]
	[  5.65755457e-132   8.48633186e-132   1.13151091e-131]




문제 183. OrderedDict함수를 이용해서 answp 182번 코드의 순전파 코드를 구현하시오 ! (참고 p182)

	import numpy as np
	import collections
	
	x = np.array([[1,2]])
	t = np.array([[0, 0, 1]])
	
	w1 = np.array([[1, 3, 5], [2, 4, 6]])
	b1 = np.array([[1, 2, 3]])
	
	w2 = np.array([[1, 4], [2, 5], [3, 6]])
	b2 = np.array([[1, 2]])
	
	w3 = np.array([[5, 6, 7], [2, 3, 4]])
	b3 = np.array([[1, 1, 1]])
	
	#각 층 객체화
	layers = collections.OrderedDict()
	layers['affine1'] = Affine(w1, b1)
	layers['relu1'] = Relu()
	layers['affine2'] = Affine(w2, b2)
	layers['relu2'] = Relu()
	layers['affine3'] = Affine(w3, b3)
	
	
	for layer in layers.values():
	    x = layer.forward(x)
	
	    
	swl = SoftmaxWithLoss()
	
	print(swl.forward(x, t))
	
	-9.99999950584e-08




문제 184. 위의 3층 신경망의 역전파 코드를 구현하시오 !

	import numpy as np
	import collections
	
	x = np.array([[1,2]])
	t = np.array([[0, 0, 1]])
	
	w1 = np.array([[1, 3, 5], [2, 4, 6]])
	b1 = np.array([[1, 2, 3]])
	
	w2 = np.array([[1, 4], [2, 5], [3, 6]])
	b2 = np.array([[1, 2]])
	
	w3 = np.array([[5, 6, 7], [2, 3, 4]])
	b3 = np.array([[1, 1, 1]])
	
	#각 층 객체화
	layers = collections.OrderedDict()
	layers['affine1'] = Affine(w1, b1)
	layers['relu1'] = Relu()
	layers['affine2'] = Affine(w2, b2)
	layers['relu2'] = Relu()
	layers['affine3'] = Affine(w3, b3)
	
	# 순전파
	for layer in layers.values():
	    x = layer.forward(x)
	
	swl = SoftmaxWithLoss()
	out = swl.forward(x, t)
	
	# 역전파
	dout = 1
	dout = swl.backward(dout)
	
	layer = list(layers.values())
	layer.reverse()
	for i in layer:
	    dout = i.backward(dout)
	
	print(dout)
	print(layers['affine1'].dW)
	print(layers['affine1'].db)
	
	[[  2.63076288e-130   3.39453274e-130]]
	[[  1.69726637e-131   2.54589956e-131   3.39453274e-131]
	 [  3.39453274e-131   5.09179911e-131   6.78906549e-131]]
	[  1.69726637e-131   2.54589956e-131   3.39453274e-131]



문제 185. 위의 2층 신경망을 활성화 함수로 relu를 사용하고 있고 최종 정확도는 아래와 같다.
	  그런데 현업에서는 sigmoid함수보다 relu함수를 더 선호한다고 하는데 그 이유를 테스트로 확인해보시오 !

	1. Relu의 최종 정확도
	 train acc, test acc | 0.976566666667, 0.9693
	
	2. Sigmoid의 최종 정확도
	 train acc, test acc | 0.947233333333, 0.9474




문제 186. (오늘의 마지막 문제)
	
	2층 신경망 vs 3층 신경망의 정확도 차이는 발생하는가?
	층을 많이 쌓으면 쌓을수록 정확도가 높아지는가?
	
	2층 신경망 (Relu 사용)
	정확도 : train acc, test acc | 0.976566666667, 0.9693
	
	3층 신경망 (Relu사용)
	정확도 : train acc, test acc | 0.911116666667, 0.9123




문제 187. (점심시간 문제) 어제 만든 오차역전파 3층 신경망에 SGD 경사감소를 입히고 학습시켜 보시오 !
	  191페이지의 SGD 클래스를 만들고 객체화시켜서 수행

	(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)
	network = TwoLayerNet(input_size=784, hidden_size1=50, hidden_size2=100, output_size1 = 100,  \
	output_size2=10)
	iters_num = 10000
	train_size = x_train.shape[0] # 60000 개
	batch_size = 100
	learning_rate = 0.1
	train_loss_list = []
	train_acc_list = []
	test_acc_list = []
	
	iter_per_epoch = max(train_size / batch_size, 1)
	
	optimizer = SGD()
	
	for i in range(iters_num):
	    batch_mask = np.random.choice(train_size, batch_size)
	    x_batch = x_train[batch_mask]
	    t_batch = t_train[batch_mask]
	    grad = network.gradient(x_batch, t_batch)
	    for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):
	        grads = network.gradient(x_batch, t_batch)
	        params = network.params
	        optimizer.update(params,grads)
	    loss = network.loss(x_batch, t_batch)
	    train_loss_list.append(loss)
	    if i % iter_per_epoch == 0:
	        train_acc = network.accuracy(x_train, t_train)
	        test_acc = network.accuracy(x_test, t_test)
	        train_acc_list.append(train_acc)
	        test_acc_list.append(test_acc)
	        print("train acc, test acc | " + str(train_acc) + ", " + str(test_acc))
	
	markers = {'train': 'o', 'test': 's'}
	x = np.arange(len(train_acc_list))
	plt.plot(x, train_acc_list, label='train acc')
	plt.plot(x, test_acc_list, label='test acc', linestyle='--')
	plt.xlabel("epochs")
	plt.ylabel("accuracy")
	plt.ylim(0, 1.0)
	plt.legend(loc='lower right')
	plt.show()

	train acc, test acc | 0.97295, 0.9646




문제 188. 경사감소법의 종류를 Momentum으로 변경해서 위의 3층 신경망을 학습시키고 SGD와 속도와 정확도를
	  비교하시오 ! (p 195 페이지 참고)
	(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)
	network = TwoLayerNet(input_size=784, hidden_size1=50, hidden_size2=100, output_size1 = 100,\
	  output_size2=10)
	iters_num = 10000
	train_size = x_train.shape[0] # 60000 개
	batch_size = 100
	learning_rate = 0.1
	train_loss_list = []
	train_acc_list = []
	test_acc_list = []
	
	iter_per_epoch = max(train_size / batch_size, 1)
	
	optimizer = Momentum()
	
	for i in range(iters_num):
	    batch_mask = np.random.choice(train_size, batch_size)
	    x_batch = x_train[batch_mask]
	    t_batch = t_train[batch_mask]
	    grad = network.gradient(x_batch, t_batch)
	    for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):
	        grads = network.gradient(x_batch, t_batch)
	        params = network.params
	        optimizer.update(params,grads)
	    loss = network.loss(x_batch, t_batch)
	    train_loss_list.append(loss)
	    if i % iter_per_epoch == 0:
	        train_acc = network.accuracy(x_train, t_train)
	        test_acc = network.accuracy(x_test, t_test)
	        train_acc_list.append(train_acc)
	        test_acc_list.append(test_acc)
	        print("train acc, test acc | " + str(train_acc) + ", " + str(test_acc))
	
	markers = {'train': 'o', 'test': 's'}
	x = np.arange(len(train_acc_list))
	plt.plot(x, train_acc_list, label='train acc')
	plt.plot(x, test_acc_list, label='test acc', linestyle='--')
	plt.xlabel("epochs")
	plt.ylabel("accuracy")
	plt.ylim(0, 1.0)
	plt.legend(loc='lower right')
	plt.show()

	train acc, test acc | 0.994783333333, 0.9746




문제 189.  Adagrade 경사감소법으로 위의 3층 신경망을 학습시키시오 (p 197 Adagrade 클래스 참고)
	(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)
	network = TwoLayerNet(input_size=784, hidden_size1=50, hidden_size2=100, output_size1 = 100, \
	 output_size2=10)
	iters_num = 10000
	train_size = x_train.shape[0] # 60000 개
	batch_size = 100
	learning_rate = 0.1
	train_loss_list = []
	train_acc_list = []
	test_acc_list = []
	
	iter_per_epoch = max(train_size / batch_size, 1)
	
	optimizer = Adamgrade()
	
	for i in range(iters_num):
	    batch_mask = np.random.choice(train_size, batch_size)
	    x_batch = x_train[batch_mask]
	    t_batch = t_train[batch_mask]
	    grad = network.gradient(x_batch, t_batch)
	    for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):
	        grads = network.gradient(x_batch, t_batch)
	        params = network.params
	        optimizer.update(params,grads)
	    loss = network.loss(x_batch, t_batch)
	    train_loss_list.append(loss)
	    if i % iter_per_epoch == 0:
	        train_acc = network.accuracy(x_train, t_train)
	        test_acc = network.accuracy(x_test, t_test)
	        train_acc_list.append(train_acc)
	        test_acc_list.append(test_acc)
	        print("train acc, test acc | " + str(train_acc) + ", " + str(test_acc))
	
	markers = {'train': 'o', 'test': 's'}
	x = np.arange(len(train_acc_list))
	plt.plot(x, train_acc_list, label='train acc')
	plt.plot(x, test_acc_list, label='test acc', linestyle='--')
	plt.xlabel("epochs")
	plt.ylabel("accuracy")
	plt.ylim(0, 1.0)
	plt.legend(loc='lower right')
	plt.show()

	train acc, test acc | 0.98205, 0.9682









문제 190. Adam 경사감소법을 사용한 위의 3층 신경망을 구현하시오 !

	(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)
	network = TwoLayerNet(input_size=784, hidden_size1=50, hidden_size2=100, output_size1 = 100, \
	 output_size2=10)
	iters_num = 10000
	train_size = x_train.shape[0] # 60000 개
	batch_size = 100
	learning_rate = 0.1
	train_loss_list = []
	train_acc_list = []
	test_acc_list = []
	
	iter_per_epoch = max(train_size / batch_size, 1)
	
	optimizer = Adam()
	
	for i in range(iters_num):
	    batch_mask = np.random.choice(train_size, batch_size)
	    x_batch = x_train[batch_mask]
	    t_batch = t_train[batch_mask]
	    grad = network.gradient(x_batch, t_batch)
	    for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):
	        grads = network.gradient(x_batch, t_batch)
	        params = network.params
	        optimizer.update(params,grads)
	    loss = network.loss(x_batch, t_batch)
	    train_loss_list.append(loss)
	    if i % iter_per_epoch == 0:
	        train_acc = network.accuracy(x_train, t_train)
	        test_acc = network.accuracy(x_test, t_test)
	        train_acc_list.append(train_acc)
	        test_acc_list.append(test_acc)
	        print("train acc, test acc | " + str(train_acc) + ", " + str(test_acc))
	
	markers = {'train': 'o', 'test': 's'}
	x = np.arange(len(train_acc_list))
	plt.plot(x, train_acc_list, label='train acc')
	plt.plot(x, test_acc_list, label='test acc', linestyle='--')
	plt.xlabel("epochs")
	plt.ylabel("accuracy")
	plt.ylim(0, 1.0)
	plt.legend(loc='lower right')
	plt.show()
	
	train acc, test acc | 0.9915, 0.9725




문제 191. 우리가 가지고 있는 3층 신경망에 가중치 초기화를 He를 써서 구현하고 정확도를 확인하시오 !

	기존 가중치 초기화값 : weight_init_std = 0.01
	변경된 가중치 초기화값 : weight_init_std = np.sqrt(2/50)
	
	class TwoLayerNet:
	    def __init__(self, input_size, hidden_size1, hidden_size2, output_size1, output_size2, \
	weight_init_std=np.sqrt(2/50)):
	        self.params = {}
	        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size1)
	        self.params['b1'] = np.zeros(hidden_size1)
	        self.params['W2'] = weight_init_std * np.random.randn(hidden_size1, output_size1)
	        self.params['b2'] = np.zeros(output_size1)
	        self.params['W3'] = weight_init_std * np.random.randn(hidden_size2, output_size2)
	        self.params['b3'] = np.zeros(output_size2)
	        self.layers = OrderedDict()
	        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])
	        self.layers['Relu1'] = Relu()
	        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])
	        self.layers['Relu2'] = Relu()
	        self.layers['Affine3'] = Affine(self.params['W3'], self.params['b3'])
	        self.lastLayer = SoftmaxWithLoss()
	
	    def predict(self, x):
	        for layer in self.layers.values():
	            x = layer.forward(x)
	        return x
	
	    def loss(self, x, t):
	        y = self.predict(x)
	        return self.lastLayer.forward(y, t)
	
	
	    def accuracy(self, x, t):
	        y = self.predict(x)
	        y = np.argmax(y, axis=1)
	        if t.ndim != 1: t = np.argmax(t, axis=1)
	        accuracy = np.sum(y == t) / float(x.shape[0])
	        return accuracy
	
	    def numerical_gradient(self, x, t):
	        loss_W = lambda W: self.loss(x, t)
	        grads = {}
	        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])
	        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])
	        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])
	        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])
	        grads['W3'] = numerical_gradient(loss_W, self.params['W3'])
	        grads['b3'] = numerical_gradient(loss_W, self.params['b3'])
	        return grads
	
	
	    def gradient(self, x, t):
	        self.loss(x, t)
	        dout = 1
	        dout = self.lastLayer.backward(dout)
	        layers = list(self.layers.values())
	        layers.reverse()
	        for layer in layers:
	            dout = layer.backward(dout)
	        grads = {}
	        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db
	        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db
	        grads['W3'], grads['b3'] = self.layers['Affine3'].dW, self.layers['Affine3'].db
	        return grads
	    
	
	(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)
	network = TwoLayerNet(input_size=784, hidden_size1=50, hidden_size2=100, output_size1 = 100, \
	 output_size2=10)
	iters_num = 10000
	train_size = x_train.shape[0] # 60000 개
	batch_size = 100
	learning_rate = 0.1
	train_loss_list = []
	train_acc_list = []
	test_acc_list = []
	
	iter_per_epoch = max(train_size / batch_size, 1)
	
	optimizer = Adam()
	
	for i in range(iters_num):
	    batch_mask = np.random.choice(train_size, batch_size)
	    x_batch = x_train[batch_mask]
	    t_batch = t_train[batch_mask]
	    grad = network.gradient(x_batch, t_batch)
	    for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):
	        grads = network.gradient(x_batch, t_batch)
	        params = network.params
	        optimizer.update(params,grads)
	    loss = network.loss(x_batch, t_batch)
	    train_loss_list.append(loss)
	    if i % iter_per_epoch == 0:
	        train_acc = network.accuracy(x_train, t_train)
	        test_acc = network.accuracy(x_test, t_test)
	        train_acc_list.append(train_acc)
	        test_acc_list.append(test_acc)
	        print("train acc, test acc | " + str(train_acc) + ", " + str(test_acc))
	
	markers = {'train': 'o', 'test': 's'}
	x = np.arange(len(train_acc_list))
	plt.plot(x, train_acc_list, label='train acc')
	plt.plot(x, test_acc_list, label='test acc', linestyle='--')
	plt.xlabel("epochs")
	plt.ylabel("accuracy")
	plt.ylim(0, 1.0)
	plt.legend(loc='lower right')
	plt.show()

	train acc, test acc | 0.989066666667, 0.973




문제 192. 우리가 가지고 있는 신경망에 가중치 decay를 적용해서 오버피팅이 덜 발생하는지 확인하시오 !

	class TwoLayerNet:
	    def __init__(self, input_size, hidden_size1, hidden_size2, output_size1, output_size2, \
	                 weight_init_std=np.sqrt(2/50)):
	        self.params = {}
	        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size1)
	        self.params['b1'] = np.zeros(hidden_size1)
	        self.params['W2'] = weight_init_std * np.random.randn(hidden_size1, output_size1)
	        self.params['b2'] = np.zeros(output_size1)
	        self.params['W3'] = weight_init_std * np.random.randn(hidden_size2, output_size2)
	        self.params['b3'] = np.zeros(output_size2)
	        self.layers = OrderedDict()
	        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])
	        self.layers['Relu1'] = Relu()
	        self.layers['dropout1'] = Dropout()
	        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])
	        self.layers['Relu2'] = Relu()
	        self.layers['dropout1'] = Dropout()
	        self.layers['Affine3'] = Affine(self.params['W3'], self.params['b3'])
	        self.lastLayer = SoftmaxWithLoss()
	        self.weight_decay_lambda = 0.001
	
	    def predict(self, x):
	        for layer in self.layers.values():
	            x = layer.forward(x)
	        return x
	
	    def loss(self, x, t):
	        y = self.predict(x)
	        weight_decay = 0
	        for idx in range(1, 4):
	            W = self.params['W' + str(idx)]
	            weight_decay += 0.5 * self.weight_decay_lambda * np.sum(W ** 2)
		return self.lastLayer.forward(y, t) + weight_decay
	

	    def accuracy(self, x, t):
	        y = self.predict(x)
	        y = np.argmax(y, axis=1)
	        if t.ndim != 1: t = np.argmax(t, axis=1)
	        accuracy = np.sum(y == t) / float(x.shape[0])
	        return accuracy
	
	    def numerical_gradient(self, x, t):
	        loss_W = lambda W: self.loss(x, t)
	        grads = {}
	        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])
	        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])
	        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])
	        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])
	        grads['W3'] = numerical_gradient(loss_W, self.params['W3'])
	        grads['b3'] = numerical_gradient(loss_W, self.params['b3'])
	        return grads
	
	
	    def gradient(self, x, t):
	        self.loss(x, t)
	        dout = 1
	        dout = self.lastLayer.backward(dout)
	        layers = list(self.layers.values())
	        layers.reverse()
	        for layer in layers:
	            dout = layer.backward(dout)
	        grads = {}
	        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db
	        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db
	        grads['W3'], grads['b3'] = self.layers['Affine3'].dW, self.layers['Affine3'].db
	        return grads
	    
	
	(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)
	network = TwoLayerNet(input_size=784, hidden_size1=50, hidden_size2=100, output_size1 = 100, \
	 output_size2=10)
	iters_num = 10000
	train_size = x_train.shape[0] # 60000 개
	batch_size = 100
	learning_rate = 0.1
	train_loss_list = []
	train_acc_list = []
	test_acc_list = []
	
	iter_per_epoch = max(train_size / batch_size, 1)
	
	optimizer = Adam()
	
	for i in range(iters_num):
	    batch_mask = np.random.choice(train_size, batch_size)
	    x_batch = x_train[batch_mask]
	    t_batch = t_train[batch_mask]
	    grad = network.gradient(x_batch, t_batch)
	    for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):
	        grads = network.gradient(x_batch, t_batch)
	        params = network.params
	        optimizer.update(params,grads)
	    loss = network.loss(x_batch, t_batch)
	    train_loss_list.append(loss)
	    if i % iter_per_epoch == 0:
	        train_acc = network.accuracy(x_train, t_train)
	        test_acc = network.accuracy(x_test, t_test)
	        train_acc_list.append(train_acc)
	        test_acc_list.append(test_acc)
	        print("train acc, test acc | " + str(train_acc) + ", " + str(test_acc))
	
	markers = {'train': 'o', 'test': 's'}
	x = np.arange(len(train_acc_list))
	plt.plot(x, train_acc_list, label='train acc')
	plt.plot(x, test_acc_list, label='test acc', linestyle='--')
	plt.xlabel("epochs")
	plt.ylabel("accuracy")
	plt.ylim(0, 1.0)
	plt.legend(loc='lower right')
	plt.show()
	
	train acc, test acc | 0.982716666667, 0.9649




문제 193. 우리반의 이름과 나이 데이터를 이용해서 표준화를 시도해서 나의 나이가 0~1 사이중 어느 부분에
	  해당 되는지 파이썬으로 확인하시오 !

		(요소값 - 평균) / 표준편차

	# -*- coding: utf-8 -*-
	
	import csv
	import  numpy  as np
	
	file = open('D:\\emp81.csv','r', encoding='UTF-8')
	emp2_csv = csv.reader(file)
	x2 =[]
	for emp2_list in emp2_csv:
	    x2.append(int(emp2_list[1]))
	    x3 = np.array(x2)
	
	print(x3)
	print( (x3-x3.mean())**2 / x3.std())
	




문제 194. 우리반 나이 데이터를 가지고 정규화(normalization) 을 해서 나이를 0 ~ 100 사이의 숫자로
	  나타내시오 !
	
	수식 : (요소 - 최소값) / (최대값 - 최소값)
	
	# -*- coding: utf-8 -*-
	
	import csv
	import  numpy  as np
	
	file = open('D:\\emp81.csv','r', encoding='UTF-8')
	emp2_csv = csv.reader(file)
	x2 =[]
	for emp2_list in emp2_csv:
	    x2.append(int(emp2_list[1]))
	    x3 = np.array(x2)
	
	print( ( x3 -  x3.min()) / (x3.max() - x3.min()))





문제 195. 위의 배치 정규화 계산 그래프 forward 함수를 생성하시오 !

	def batch_norm_forward(x, gamma = 1, beta = 0, eps):
	
	코드 구현
	
	print( batch_norm_forward(x,1,0,0.01) )





문제 196. 아래의 코드를 이용해서 우리가 가지고 있는 3층 신경망에 배치 정규화 코드를 추가하시오 !

	class TwoLayerNet:
	    def __init__(self, input_size, hidden_size1, hidden_size2, output_size1, output_size2, \
	                 weight_init_std=np.sqrt(2/50)):
	        self.params = {}
	        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size1)
	        self.params['b1'] = np.zeros(hidden_size1)
	        self.params['W2'] = weight_init_std * np.random.randn(hidden_size1, output_size1)
	        self.params['b2'] = np.zeros(output_size1)
	        self.params['W3'] = weight_init_std * np.random.randn(hidden_size2, output_size2)
	        self.params['b3'] = np.zeros(output_size2)
	        self.layers = OrderedDict()
	        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])
	        self.layers['BatchNorm1'] = BatchNormalization(gamma=1.0, beta=0.)
	        self.layers['Relu1'] = Relu()
	#         self.layers['dropout1'] = Dropout()
	        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])
	        self.layers['BatchNorm2'] = BatchNormalization(gamma=1.0, beta=0.)
	        self.layers['Relu2'] = Relu()
	#         self.layers['dropout1'] = Dropout()
	        self.layers['Affine3'] = Affine(self.params['W3'], self.params['b3'])
	        self.lastLayer = SoftmaxWithLoss()
	        self.weight_decay_lambda = 0.001
	
	    def predict(self, x):
	        for layer in self.layers.values():
	            x = layer.forward(x)
	        return x
	
	    def loss(self, x, t):
	        y = self.predict(x)
	        weight_decay = 0
	        for idx in range(1, 4):
	             W = self.params['W' + str(idx)]
	             weight_decay += 0.5 * self.weight_decay_lambda * np.sum(W ** 2)
	        return self.lastLayer.forward(y, t) + weight_decay
	
	
	    def accuracy(self, x, t):
	        y = self.predict(x)
	        y = np.argmax(y, axis=1)
	        if t.ndim != 1: t = np.argmax(t, axis=1)
	        accuracy = np.sum(y == t) / float(x.shape[0])
	        return accuracy
	
	    def numerical_gradient(self, x, t):
	        loss_W = lambda W: self.loss(x, t)
	        grads = {}
	        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])
	        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])
	        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])
	        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])
	        grads['W3'] = numerical_gradient(loss_W, self.params['W3'])
	        grads['b3'] = numerical_gradient(loss_W, self.params['b3'])
	        return grads
	
	
	    def gradient(self, x, t):
	        self.loss(x, t)
	        dout = 1
	        dout = self.lastLayer.backward(dout)
	        layers = list(self.layers.values())
	        layers.reverse()
	        for layer in layers:
	            dout = layer.backward(dout)
	        grads = {}
	        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db
	        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db
	        grads['W3'], grads['b3'] = self.layers['Affine3'].dW, self.layers['Affine3'].db
	        return grads
	
	(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)
	network = TwoLayerNet(input_size=784, hidden_size1=50, hidden_size2=100, output_size1 = 100, \
	 output_size2=10)
	iters_num = 10000
	train_size = x_train.shape[0] # 60000 개
	batch_size = 100
	learning_rate = 0.1
	train_loss_list = []
	train_acc_list = []
	test_acc_list = []
	
	iter_per_epoch = max(train_size / batch_size, 1)
	
	optimizer = Adam()
	
	for i in range(iters_num):
	    batch_mask = np.random.choice(train_size, batch_size)
	    x_batch = x_train[batch_mask]
	    t_batch = t_train[batch_mask]
	    grad = network.gradient(x_batch, t_batch)
	    for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):
	        grads = network.gradient(x_batch, t_batch)
	        params = network.params
	        optimizer.update(params,grads)
	    loss = network.loss(x_batch, t_batch)
	    train_loss_list.append(loss)
	    if i % iter_per_epoch == 0:
	        train_acc = network.accuracy(x_train, t_train)
	        test_acc = network.accuracy(x_test, t_test)
	        train_acc_list.append(train_acc)
	        test_acc_list.append(test_acc)
	        print("train acc, test acc | " + str(train_acc) + ", " + str(test_acc))
	
	markers = {'train': 'o', 'test': 's'}
	x = np.arange(len(train_acc_list))
	plt.plot(x, train_acc_list, label='train acc')
	plt.plot(x, test_acc_list, label='test acc', linestyle='--')
	plt.xlabel("epochs")
	plt.ylabel("accuracy")
	plt.ylim(0, 1.0)
	plt.legend(loc='lower right')
	plt.show()




문제 197. 아래의 두 행렬을 만들고 합성곱 한 결과인 15을 파이썬으로 출력하시오 !
	 1 2 3        2 0 1        2 0 3
	 0 1 2   ◎   0 1 2   =    0 1 4    =   15
	 3 0 1        1 0 2        3 0 2
	
	import numpy as np
	
	x = np.array([[1, 2, 3], [0, 1, 2], [3, 0, 1]])
	y = np.array([[2, 0, 1], [0, 1, 2], [1, 0, 2]])

	print(np.sum(x*y))
	15




문제 198. 아래의 4 x 4 행렬에서 아래의 3x3행렬만 추출하시오 !

	 1 2 3 0
	 0 1 2 3  ------->  1 2 3
	 3 0 1 2            0 1 2
	 2 3 0 1            3 0 1
	
	import numpy as np
	
	x = np.array([[1, 2, 3, 0], [0, 1, 2, 3], [3, 0, 1, 2], [2, 3, 0, 1]])
	print(x[0:3,0:3])

	[[1 2 3]
	 [0 1 2]
	 [3 0 1]]




문제 199. 아래의 행렬에서 아래의 결과 행렬만 추출하시오 !

	 1 2 3 0
	 0 1 2 3  ------->  2 3 0
	 3 0 1 2            1 2 3
	 2 3 0 1            0 1 2
	
	import numpy as np
	x = np.array([[1, 2, 3, 0], [0, 1, 2, 3], [3, 0, 1, 2], [2, 3, 0, 1]])
	y = np.array([[2, 0, 1], [0, 1, 2], [1, 0, 2]])
	
	print(x[0:y.shape[0],1:y.shape[1]+1])




문제 200. 아래의 행렬에서 for loop문을 이용해서 아래의 결과를 출력하시오 !

	import numpy as np
	x = np.array([[1, 2, 3, 0], [0, 1, 2, 3], [3, 0, 1, 2], [2, 3, 0, 1]])
	y = np.array([[2, 0, 1], [0, 1, 2], [1, 0, 2]])
	
	for i in range(x.shape[0]-y.shape[0]+1):
	    for j in range(x.shape[1] - y.shape[1]+1):
	        print(x[i:y.shape[0]+i,j:y.shape[1]+j])
	        print('-----------')
	
	[[1 2 3]
	 [0 1 2]
	 [3 0 1]]
	-----------
	[[2 3 0]
	 [1 2 3]
	 [0 1 2]]
	-----------
	[[0 1 2]
	 [3 0 1]
	 [2 3 0]]
	-----------
	[[1 2 3]
	 [0 1 2]
	 [3 0 1]]
	-----------




문제 201. 아래의 합성곱을 파이썬으로 구현하시오 !

	 1 2 3 0
	 0 1 2 3      ◎    2 0 1    =   15  16
	 3 0 1 2            0 1 2         6  15
	 2 3 0 1            1 0 2
	
	import numpy as np
	x = np.array([[1, 2, 3, 0], [0, 1, 2, 3], [3, 0, 1, 2], [2, 3, 0, 1]])
	y = np.array([[2, 0, 1], [0, 1, 2], [1, 0, 2]])
	
	gop = []
	for i in range(x.shape[0]-y.shape[0]+1):
	    for j in range(x.shape[1] - y.shape[1]+1):
	        gop.append(np.sum(x[i:y.shape[0]+i,j:y.shape[1]+j]*y))
	print(gop)

	[15, 16, 6, 15]




문제 202. 위에서 출력한 1차원 배열인 [15, 16, 6, 15]결과를 2x2행렬로 변경하시오 !

	import numpy as np
	x = np.array([[1, 2, 3, 0], [0, 1, 2, 3], [3, 0, 1, 2], [2, 3, 0, 1]])
	y = np.array([[2, 0, 1], [0, 1, 2], [1, 0, 2]])
	
	gop = []
	for i in range(x.shape[0]-y.shape[0]+1):
	    for j in range(x.shape[1] - y.shape[1]+1):
	        gop.append(np.sum(x[i:y.shape[0]+i,j:y.shape[1]+j]*y))
	print(np.array(gop).reshape(i+1,j+1))

	[[15 16]
	 [ 6 15]]




문제 203. 아래의 그림의 convolution연산을 파이썬으로 구현하시오 !
	
	
	import numpy as np
	x = np.array([[1, 2, 3, 0], [0, 1, 2, 3], [3, 0, 1, 2], [2, 3, 0, 1]])
	y = np.array([[2, 0, 1], [0, 1, 2], [1, 0, 2]])
	
	gop = []
	for i in range(x.shape[0]-y.shape[0]+1):
	    for j in range(x.shape[1] - y.shape[1]+1):
	        gop.append(np.sum(x[i:y.shape[0]+i,j:y.shape[1]+j]*y))
	
	res = np.array(gop).reshape(i+1,j+1)
	print(res+3)
	
	[[18 19]
	 [ 9 18]]





문제 204. 문제 255번에서 출력한 아래의 2x2행렬에 제로패딩 1을 수행하시오 !

	import numpy as np
	x = np.array([[1, 2, 3, 0], [0, 1, 2, 3], [3, 0, 1, 2], [2, 3, 0, 1]])
	y = np.array([[2, 0, 1], [0, 1, 2], [1, 0, 2]])
	b = np.array([3])
	
	gop = []
	for i in range(x.shape[0]-y.shape[0]+1):
	    for j in range(x.shape[1] - y.shape[1]+1):
	        gop.append(np.sum(x[i:y.shape[0]+i,j:y.shape[1]+j]*y))
	
	res = np.array(gop).reshape(i+1,j+1)
	res = res+b
	res_pad = np.pad(res, pad_width = 1, mode = 'constant', constant_values=0)
	print(res_pad)

	[[ 0  0  0  0]
	 [ 0 18 19  0]
	 [ 0  9 18  0]
	 [ 0  0  0  0]]





문제 205. 4x4 행렬에 3x3필터를 적용해서 결과로 4x4행렬이 출력되게 하려면 제로패딩을 몇을 해줘야 하는가 ?
	패딩 공식 : p234
	 P = ( (OH -1)*S - H+FH ) / 2
	입력크기를 (H,W), 필터크기를 (FH, FW), 패딩을 P, 스트라이드 S 라고 한다.
	




문제 206. 위의 패딩 공식을 구현하는 파이썬 함수를 생성하시오 !

	def padding(H,S,OH,FH):
	    return int(( (OH -1)*S - H+FH ) / 2)
	
	print(padding(4,1,4,3))

	1




문제 207. 아래의 결과가 무엇인지 출력하시오 !

                     0 0 0 0 0 0
  1  2  3  0         0 1 2 3 0 0                           
  0  1  2  3    ---> 0 0 1 2 3 0  ◎  2 0 1   =   ?  
  3  0  1  2         0 3 0 1 2 0      0 1 2
  2  3  0  1         0 2 3 0 1 0      1 0 2
                     0 0 0 0 0 0

    (4 x 4)            (6X6)         (3x3)       ( 4 x 4 )

	import numpy as np
	def padding(H,S,OH,FH):
	    return int(( (OH -1)*S - H+FH ) / 2)
	
	x = np.array([[1, 2, 3, 0], [0, 1, 2, 3], [3, 0, 1, 2], [2, 3, 0, 1]])
	y = np.array([[2, 0, 1], [0, 1, 2], [1, 0, 2]])
	b = np.array([3])
	pad = padding(x.shape[0], 1, x.shape[0], y.shape[0])
	
	x_pad = np.pad(x, pad_width = pad, mode = 'constant', constant_values=0)
	
	gop = []
	for i in range(x_pad.shape[0]-y.shape[0]+1):
	    for j in range(x_pad.shape[1] - y.shape[1]+1):
	        gop.append(np.sum(x_pad[i:y.shape[0]+i,j:y.shape[1]+j]*y))
	
	res = np.array(gop).reshape(i+1,j+1)
	res = res+b
	print(res)

	[[10 15 13  5]
	 [ 7 18 19 13]
	 [13  9 18  9]
	 [11 13  7  6]]




문제 208. 레드벨벳의 아이린 사진을 3차원 행렬로 변환하시오 !

	from PIL import Image
	import numpy as np
	import matplotlib.pyplot as plt
	import matplotlib.image as mpimg
	
	##5.1. 원본 이미지 불러오기
	
	img = Image.open('c://test_pic.jpg')
	img_pixel = np.array(img)
	plt.imshow(img_pixel)
	print (img_pixel)

	[[[ 79  96 104]
	  [113 118 138]
	  [147 137 172]
	  ...,
	  ...,
	  [165 150 147]
	  [176 160 160]
	  [184 168 169]]]  ──▶ 픽셀 하나의 숫자가 0 ~ 255사이로 나타낼 수 있고 숫자가 클 수록
	    ↑  ↑   ↑		  밝은 색이다.
	   Red Green Blue




문제 209. 아이린 사진을 numpy배열로 변환한 행렬의 shape를 확인하시오 !

	from PIL import Image
	import numpy as np
	import matplotlib.pyplot as plt
	import matplotlib.image as mpimg
	
	img = Image.open('c://test_pic.jpg')
	img_pixel = np.array(img)
	# plt.imshow(img_pixel)
	print (img_pixel.shape)

	(500, 500, 3)
	 ↑   ↑   ↑
        가로 세로 색상		색상 : 0 ─▶ Red, 1 ─▶ Green, 2 ─▶ Blue




문제 210. 아이린 사진에서 red부분의 행렬만 추력하고 red부분만 이미지로 시각화 하시오 !

	from PIL import Image
	import numpy as np
	import matplotlib.pyplot as plt
	import matplotlib.image as mpimg
	
	img = Image.open('c://test_pic.jpg')
	img_pixel = np.array(img)
	plt.imshow(img_pixel[:, :, 0])
	plt.show()
	# print (img_pixel.shape)




문제 211. 아이린 사진의 Blue행렬을 시각화 하시오 !

	from PIL import Image
	import numpy as np
	import matplotlib.pyplot as plt
	import matplotlib.image as mpimg
	
	img = Image.open('c://test_pic.jpg')
	img_pixel = np.array(img)
	plt.imshow(img_pixel[:, :, 2])
	plt.show()

	


문제 212. 아래의 3차원 행렬을 시각화 하시오 !

	from PIL import Image
	import numpy as np
	import matplotlib.pyplot as plt
	import matplotlib.image as mpimg
	
	Filter = np.array([[[255,255,255],
	                    [255,255,255],
	                    [0,0,0],
	                    [255,255,255],
	                    [255,255,255]],
	                   [[255,255,255],
	                    [255,255,255],
	                    [0,0,0],
	                    [255,255,255],
	                    [255,255,255]],
	                   [[0,0,0],
	                    [0,0,0],
	                    [0,0,0],
	                    [0,0,0],
	                    [0,0,0]],
	                   [[255,255,255],
	                    [255,255,255],
	                    [0,0,0],
	                    [255,255,255],
	                    [255,255,255]],
	                   [[255,255,255],
	                    [255,255,255],
	                    [0,0,0],
	                    [255,255,255],
	                    [255,255,255]]], dtype=np.uint8)
	print(Filter.shape)
	plt.imshow(Filter)
	
	




문제 213. 위의 Filter 이미지에 Red부분행렬만 시각화하시오 !

	from PIL import Image
	import numpy as np
	import matplotlib.pyplot as plt
	import matplotlib.image as mpimg
	
	Filter = np.array([[[255,255,255],
	                    [255,255,255],
	                    [0,0,0],
	                    [255,255,255],
	                    [255,255,255]],
	                   [[255,255,255],
	                    [255,255,255],
	                    [0,0,0],
	                    [255,255,255],
	                    [255,255,255]],
	                   [[0,0,0],
	                    [0,0,0],
	                    [0,0,0],
	                    [0,0,0],
	                    [0,0,0]],
	                   [[255,255,255],
	                    [255,255,255],
	                    [0,0,0],
	                    [255,255,255],
	                    [255,255,255]],
	                   [[255,255,255],
	                    [255,255,255],
	                    [0,0,0],
	                    [255,255,255],
	                    [255,255,255]]], dtype=np.uint8)
	print(Filter[:, :, 0])
	plt.imshow(Filter[:, :, 0])

	




문제 214. R, G, B 행렬을 이해하기 위해 아래의 numpy array를 이해하시오 !

	import numpy as np
	data = np.array(
	       [
	         [[2, 2, 1, 1, 0],
	          [0, 0, 1, 0, 0],
	          [0, 2, 0, 0, 1],
	          [1, 2, 1, 1, 1],    #  --->  Red
	          [1, 0, 1, 0, 1]],
	         [[2, 0, 0, 0, 1],
	          [0, 2, 2, 0, 1],
	          [0, 0, 0, 0, 2],    # ---->  Green
	          [0, 1, 2, 0, 1],
	          [2, 0, 2, 2, 2]],
	         [[4, 2, 1, 2,2],
	          [0, 1, 0, 4,1],      # ---->  Blue
	          [3, 0, 6, 2,1],
	          [4, 2, 4, 5,4],
	          [0, 1, 2, 0, 1]]
	       ])
	
	print(data.shape)
	
	(3,    5,   5)
	 ↑   ↑   ↑
        가로 세로 색상





문제 215. 위의 3차원 행렬의 이미지를 확인하시오 !

	import numpy as np
	
	data = np.array(
	       [
	         [[2, 2, 1, 1, 0],
	          [0, 0, 1, 0, 0],
	          [0, 2, 0, 0, 1],
	          [1, 2, 1, 1, 1],    #  --->  Red
	          [1, 0, 1, 0, 1]],
	         [[2, 0, 0, 0, 1],
	          [0, 2, 2, 0, 1],
	          [0, 0, 0, 0, 2],    # ---->  Green
	          [0, 1, 2, 0, 1],
	          [2, 0, 2, 2, 2]],
	         [[4, 2, 1, 2,2],
	          [0, 1, 0, 4,1],      # ---->  Blue
	          [3, 0, 6, 2,1],
	          [4, 2, 4, 5,4],
	          [0, 1, 2, 0, 1]]
	       ])
	
	print(data.shape)
	print(data[0, :, :].T)
	plt.imshow(data.T[:, :, 0])
	
	(3, 5, 5)
	[[2 0 0 1 1]
	 [2 0 2 2 0]
	 [1 1 0 1 1]
	 [1 0 0 1 0]
	 [0 0 1 1 1]]

	



문제 216. 문제 210에서 만든 Filter 이미지 행렬을 이해하기 쉽도록 만든 문제 266번 형태로 변경하시오 !

	from PIL import Image
	import numpy as np
	import matplotlib.pyplot as plt
	import matplotlib.image as mpimg
	
	Filter = np.array([[[255,255,255],
	                    [255,255,255],
	                    [0,0,0],
	                    [255,255,255],
	                    [255,255,255]],
	                   [[255,255,255],
	                    [255,255,255],
	                    [0,0,0],
	                    [255,255,255],
	                    [255,255,255]],
	                   [[0,0,0],
	                    [0,0,0],
	                    [0,0,0],
	                    [0,0,0],
	                    [0,0,0]],
	                   [[255,255,255],
	                    [255,255,255],
	                    [0,0,0],
	                    [255,255,255],
	                    [255,255,255]],
	                   [[255,255,255],
	                    [255,255,255],
	                    [0,0,0],
	                    [255,255,255],
	                    [255,255,255]]], dtype=np.uint8)
	Filter = Filter.T
	print(Filter)
	plt.imshow(Filter.T)
	
	[[[255 255   0 255 255]
	  [255 255   0 255 255]
	  [  0   0   0   0   0]
	  [255 255   0 255 255]
	  [255 255   0 255 255]]
	
	 [[255 255   0 255 255]
	  [255 255   0 255 255]
	  [  0   0   0   0   0]
	  [255 255   0 255 255]
	  [255 255   0 255 255]]
	
	 [[255 255   0 255 255]
	  [255 255   0 255 255]
	  [  0   0   0   0   0]
	  [255 255   0 255 255]
	  [255 255   0 255 255]]]




문제 217. 아래의 이미지를 출력하시오 !

	from PIL import Image
	import numpy as np
	import matplotlib.pyplot as plt
	import matplotlib.image as mpimg
	
	Filter = np.array([[[255,0,0],
	                    [255,0,0],
	                    [0,0,0],
	                    [255, 0, 0],
	                    [255, 0, 0]],
	                   [[255,0,0],
	                    [255, 0, 0],
	                    [0,0,0],
	                    [255, 0, 0],
	                    [255, 0, 0]],
	                   [[0,0,0],
	                    [0,0,0],
	                    [0,0,0],
	                    [0,0,0],
	                    [0,0,0]],
	                   [[255,0,0],
	                    [255, 0, 0],
	                    [0,0,0],
	                    [255, 0, 0],
	                    [255, 0, 0]],
	                   [[255,0,0],
	                    [255, 0, 0],
	                    [0,0,0],
	                    [255, 0, 0],
	                    [255, 0, 0]]], dtype=np.uint8)
	
	plt.imshow(Filter)

	



문제 218. 위의 코드를 이용해서 이번에는 Red가 아니라 Green으로 십자가 모양의 그림이 출력되게 하시오 !

	from PIL import Image
	import numpy as np
	import matplotlib.pyplot as plt
	import matplotlib.image as mpimg
	
	Filter = np.array([[[255,0,0],
	                    [255,0,0],
	                    [0,0,0],
	                    [255, 0, 0],
	                    [255, 0, 0]],
	                   [[255,0,0],
	                    [255, 0, 0],
	                    [0,0,0],
	                    [255, 0, 0],
	                    [255, 0, 0]],
	                   [[0,0,0],
	                    [0,0,0],
	                    [0,0,0],
	                    [0,0,0],
	                    [0,0,0]],
	                   [[255,0,0],
	                    [255, 0, 0],
	                    [0,0,0],
	                    [255, 0, 0],
	                    [255, 0, 0]],
	                   [[255,0,0],
	                    [255, 0, 0],
	                    [0,0,0],
	                    [255, 0, 0],
	                    [255, 0, 0]]], dtype=np.uint8)
	Filter = Filter.T
	Filter1 = np.array([Filter[1], Filter[0], Filter[2]])
	plt.imshow(Filter1.T)
	
	




문제 219. 십자가 모양을 Blue로 출력하시오 !

	from PIL import Image
	import numpy as np
	import matplotlib.pyplot as plt
	import matplotlib.image as mpimg
	
	Filter = np.array([[[255,0,0],
	                    [255,0,0],
	                    [0,0,0],
	                    [255, 0, 0],
	                    [255, 0, 0]],
	                   [[255,0,0],
	                    [255, 0, 0],
	                    [0,0,0],
	                    [255, 0, 0],
	                    [255, 0, 0]],
	                   [[0,0,0],
	                    [0,0,0],
	                    [0,0,0],
	                    [0,0,0],
	                    [0,0,0]],
	                   [[255,0,0],
	                    [255, 0, 0],
	                    [0,0,0],
	                    [255, 0, 0],
	                    [255, 0, 0]],
	                   [[255,0,0],
	                    [255, 0, 0],
	                    [0,0,0],
	                    [255, 0, 0],
	                    [255, 0, 0]]], dtype=np.uint8)
	Filter = Filter.T
	Filter1 = np.array([Filter[1], Filter[2], Filter[0]])
	plt.imshow(Filter1.T)
	
	




문제 220. 아이린 사진의 Red를 출력하시오 !
	 아이린 사진의 Red행렬만 그대로 두고 아이린 사진의 Green과 Blue로 변경해서 시각화 하시오 !
	
	from PIL import Image
	import numpy as np
	import matplotlib.pyplot as plt
	import matplotlib.image as mpimg
	
	img = Image.open('c://test_pic.jpg')
	img_pixel = np.array(img)
	img_pixel.T[1] = 0
	img_pixel.T[2] = 0
	plt.imshow(img_pixel)






☆ zero 패딩한 결과
	입력 행렬에 제로패딩을 해서 출력층을 구현하는 것이다.

                     0 0 0 0 0 0
  1  2  3  0         0 1 2 3 0 0                           
  0  1  2  3    ---> 0 0 1 2 3 0  ◎  2 0 1   =   ?  
  3  0  1  2         0 3 0 1 2 0      0 1 2
  2  3  0  1         0 2 3 0 1 0      1 0 2
                     0 0 0 0 0 0

    (4 x 4)            (6X6)         (3x3)       ( 4 x 4 )




문제 221. 아래의 RGB 3차원 행렬을 zero패딩 1 한 결과를 출력하시오 !

	import numpy as np
	
	data = np.array(
	       [
	         [[2, 2, 1, 1, 0],
	          [0, 0, 1, 0, 0],
	          [0, 2, 0, 0, 1],
	          [1, 2, 1, 1, 1],    #  --->  Red
	          [1, 0, 1, 0, 1]],
	         [[2, 0, 0, 0, 1],
	          [0, 2, 2, 0, 1],
	          [0, 0, 0, 0, 2],    # ---->  Green
	          [0, 1, 2, 0, 1],
	          [2, 0, 2, 2, 2]],
        	 [[4, 2, 1, 2,2],
        	  [0, 1, 0, 4,1],      # ---->  Blue
	          [3, 0, 6, 2,1],
	          [4, 2, 4, 5,4],
	          [0, 1, 2, 0, 1]]
	       ])
	data_pad = data
	data_pad = np.pad(data, pad_width = 1, mode = 'constant', constant_values=0)[1:4]
	print(data_pad)

	[[[0 0 0 0 0 0 0]
	  [0 2 2 1 1 0 0]
	  [0 0 0 1 0 0 0]
	  [0 0 2 0 0 1 0]
	  [0 1 2 1 1 1 0]
	  [0 1 0 1 0 1 0]
	  [0 0 0 0 0 0 0]]
	
	 [[0 0 0 0 0 0 0]
	  [0 2 0 0 0 1 0]
	  [0 0 2 2 0 1 0]
	  [0 0 0 0 0 2 0]
	  [0 0 1 2 0 1 0]
	  [0 2 0 2 2 2 0]
	  [0 0 0 0 0 0 0]]
	
	 [[0 0 0 0 0 0 0]
	  [0 4 2 1 2 2 0]
	  [0 0 1 0 4 1 0]
	  [0 3 0 6 2 1 0]
	  [0 4 2 4 5 4 0]
	  [0 0 1 2 0 1 0]
	  [0 0 0 0 0 0 0]]]




문제 222. 문제 220번의 결과인 아래의 Red행렬 에서 아래의 행렬만 추출하시오 !
	  [[0 0 0 0 0 0 0]
	   [0 2 2 1 1 0 0]
	   [0 0 0 1 0 0 0]
	   [0 0 2 0 0 1 0]
	   [0 1 2 1 1 1 0]
	   [0 1 0 1 0 1 0]
	   [0 0 0 0 0 0 0]]
	
	import numpy as np
	
	data = np.array(
	       [
	         [[2, 2, 1, 1, 0],
	          [0, 0, 1, 0, 0],
	          [0, 2, 0, 0, 1],
	          [1, 2, 1, 1, 1],    #  --->  Red
	          [1, 0, 1, 0, 1]],
	         [[2, 0, 0, 0, 1],
	          [0, 2, 2, 0, 1],
	          [0, 0, 0, 0, 2],    # ---->  Green
	          [0, 1, 2, 0, 1],
	          [2, 0, 2, 2, 2]],
	         [[4, 2, 1, 2,2],
	          [0, 1, 0, 4,1],      # ---->  Blue
	          [3, 0, 6, 2,1],
	          [4, 2, 4, 5,4],
	          [0, 1, 2, 0, 1]]
	       ])
	data_pad = data
	data_pad = np.pad(data, pad_width = 1, mode = 'constant', constant_values=0)[1:4]
	print(data_pad[0, 0:3, 0:3])
	
	[[0 0 0]
	 [0 2 2]
	 [0 0 0]]




문제 223. 문제 221번에서는 Red행렬에 대해서만 결과를 추출 했는데 아래와 같이 Green과 Blue도 다 추출 하시오 !

	import numpy as np
	
	data = np.array(
	       [
	         [[2, 2, 1, 1, 0],
	          [0, 0, 1, 0, 0],
	          [0, 2, 0, 0, 1],
	          [1, 2, 1, 1, 1],    #  --->  Red
	          [1, 0, 1, 0, 1]],
	         [[2, 0, 0, 0, 1],
	          [0, 2, 2, 0, 1],
	          [0, 0, 0, 0, 2],    # ---->  Green
	          [0, 1, 2, 0, 1],
	          [2, 0, 2, 2, 2]],
	         [[4, 2, 1, 2,2],
	          [0, 1, 0, 4,1],      # ---->  Blue
	          [3, 0, 6, 2,1],
	          [4, 2, 4, 5,4],
	          [0, 1, 2, 0, 1]]
	       ])
	data_pad = data
	data_pad = np.pad(data, pad_width = 1, mode = 'constant', constant_values=0)[1:4]
	print(data_pad[0, 0:3, 0:3])
	print(data_pad[1, 0:3, 0:3])
	print(data_pad[2, 0:3, 0:3])

	[[0 0 0]
	 [0 2 2]
	 [0 0 0]]
	[[0 0 0]
	 [0 2 0]
	 [0 0 2]]
	[[0 0 0]
	 [0 4 2]
	 [0 0 1]]




문제 224. 아래의 Filter에서 Red행렬만 추출하시오 !

	import numpy as np
	
	Filter=np.array([[[1,1,-1,-1,0,0,1,1,0],   \
	                  [-1,-1,0,0,-1,1,0,-1,0], \
	                  [-1,1,1,-1,1,-1,0,0,-1]]]).reshape(3,3,3)
	print(Filter[0])
	
	[[ 1  1 -1]
	 [-1  0  0]
	 [ 1  1  0]]




문제 225. 위의 원본 이미지(data) 행렬중 Red행렬과 Filter의 Red행렬과의 곱을 아래와 같이 수행하시오 !

	import numpy as np
	
	data = np.array(
	       [
	         [[2, 2, 1, 1, 0],
	          [0, 0, 1, 0, 0],
	          [0, 2, 0, 0, 1],
	          [1, 2, 1, 1, 1],    #  --->  Red
	          [1, 0, 1, 0, 1]],
	         [[2, 0, 0, 0, 1],
	          [0, 2, 2, 0, 1],
	          [0, 0, 0, 0, 2],    # ---->  Green
	          [0, 1, 2, 0, 1],
	          [2, 0, 2, 2, 2]],
	         [[4, 2, 1, 2,2],
	          [0, 1, 0, 4,1],      # ---->  Blue
	          [3, 0, 6, 2,1],
	          [4, 2, 4, 5,4],
	          [0, 1, 2, 0, 1]]
	       ])
	Filter=np.array([[[1,1,-1,-1,0,0,1,1,0],   \
	                  [-1,-1,0,0,-1,1,0,-1,0], \
	                  [-1,1,1,-1,1,-1,0,0,-1]]]).reshape(3,3,3)
	data_pad = data
	data_pad = np.pad(data, pad_width = 1, mode = 'constant', constant_values=0)[1:4]
	
	print(data_pad[0, :3, :3]*Filter[0])
	
	[[0 0 0]
	 [0 0 0]
	 [0 0 0]]




문제 226. 아래의 원본 이미지 RGB 3개의 행렬과 아래의 필터 RGB 3개의 행렬을 각각 행렬곱 한 후 그 원소들을
	  다 합친 결과 숫자 하나를 출력하시오 !

	 [[0 0 0]         1  1 -1     0  0  0
	  [0 2 2]     *  -1  0  0  =  0  0  0
	  [0 0 0]]        1  1  0     0  0  0
	
	 [[0 0 0]        -1 -1  0     0  0  0
	  [0 2 0]     *   0 -1  1  =  0 -2  0
	  [0 0 2]]        0 -1  0     0  0  0
	
	 [[0 0 0]        -1  1  1     0  0  0
	  [0 4 2]     *  -1  1 -1  =  0  4  -2
	  [0 0 1]]        0  0 -1     0  0  -1
	
	 결과 : - 1
	
	import numpy as np
	
	data = np.array(
	       [
	         [[2, 2, 1, 1, 0],
	          [0, 0, 1, 0, 0],
	          [0, 2, 0, 0, 1],
	          [1, 2, 1, 1, 1],    #  --->  Red
	          [1, 0, 1, 0, 1]],
	         [[2, 0, 0, 0, 1],
	          [0, 2, 2, 0, 1],
	          [0, 0, 0, 0, 2],    # ---->  Green
	          [0, 1, 2, 0, 1],
	          [2, 0, 2, 2, 2]],
	         [[4, 2, 1, 2,2],
	          [0, 1, 0, 4,1],      # ---->  Blue
	          [3, 0, 6, 2,1],
	       	   [4, 2, 4, 5,4],
	       	   [0, 1, 2, 0, 1]]
	       ])
	Filter=np.array([[[1,1,-1,-1,0,0,1,1,0],   \
	                  [-1,-1,0,0,-1,1,0,-1,0], \
	                  [-1,1,1,-1,1,-1,0,0,-1]]]).reshape(3,3,3)
	data_pad = data
	data_pad = np.pad(data, pad_width = 1, mode = 'constant', constant_values=0)[1:4]
	
	res = 0
	for i in range(3):
	    res += np.sum(data_pad[i, :3, :3]*Filter[i])
	    
	print(res)

	-1




문제 227. (점심시간 문제) 위의 원본 이미지(RGB)와 Filter(RGB)와의 3차원 합성곱을 한 결과를 출력하시오 !

	그림 7-_-15

	import numpy as np
	
	data = np.array(
	       [
	         [[2, 2, 1, 1, 0],
	          [0, 0, 1, 0, 0],
	          [0, 2, 0, 0, 1],
	          [1, 2, 1, 1, 1],    #  --->  Red
	          [1, 0, 1, 0, 1]],
	         [[2, 0, 0, 0, 1],
	          [0, 2, 2, 0, 1],
	          [0, 0, 0, 0, 2],    # ---->  Green
	          [0, 1, 2, 0, 1],
	          [2, 0, 2, 2, 2]],
	         [[4, 2, 1, 2,2],
	          [0, 1, 0, 4,1],      # ---->  Blue
	          [3, 0, 6, 2,1],
	          [4, 2, 4, 5,4],
	          [0, 1, 2, 0, 1]]
	       ])
	Filter=np.array([[[1,1,-1,-1,0,0,1,1,0],   \
	                  [-1,-1,0,0,-1,1,0,-1,0], \
	                  [-1,1,1,-1,1,-1,0,0,-1]]]).reshape(3,3,3)
	data_pad = data
	data_pad = np.pad(data, pad_width = 1, mode = 'constant', constant_values=0)[1:4]
	
	result = []
	for i in range(data_pad.shape[1]-Filter.shape[1]+1):
	    for j in range(data_pad.shape[2]-Filter.shape[2]+1):
	        res = 0
	        for k in range(3):
	            res += np.sum(data_pad[k, i:Filter.shape[1]+i, j:Filter.shape[2]+j]*Filter[k])
	        result.append(res)
	result = np.array(result).reshape(data.shape[1:])
	print(result)
	plt.imshow(result)
	plt.show()




☆ 지금까지 한 핵심

	     5 x 5  ------>  convolution 층 ----->  5 x 5  
	
	  입력 이미지              ↓               출력 이미지
	                      stride :  1
	                     zero padding : 1
	
	                        0 0 0 0 0 0 0
	 [[2, 2, 1, 1, 0],      0 2 2 1 1 1 0
	  [0, 0, 1, 0, 0],      0 0 0 1 0 0 0      1,1,-1
	  [0, 2, 0, 0, 1],  --> 0 0 2 0 0 1 0   * -1,0, 0   =    ?  
	  [1, 2, 1, 1, 1],      0 1 2 1 1 1 0      1,1, 0
	  [1, 0, 1, 0, 1]],     0 1 0 1 0 1 0
	                        0 0 0 0 0 0 0  
	
	    5 x  5                7 x 7            3 x 3       5 x 5

	입력할 때의 shape 5x5와 똑같이 5x5행렬로 출력 하려면 입력할 때 zero패딩을 1을 해줘야 한다.

		      H + 2P -FH	   5 + 2x1 - 3
		OH =  ────── + 1  =  ────── + 1   =   5
			  S		         1

		      W + 2P -FW	   5 + 2x1 - 3
		OW =  ────── + 1  =  ────── + 1   =   5
			  S		         1




문제 228. 아래와 같이 입력행렬과 필터행렬과 스트라이드와 패딩을 입력받아 출력 행렬의 shape를 출력하는
	  함수를 생성하시오 !

	x = np.array([[[2, 2, 1, 1, 0],
	               [0, 0, 1, 0, 0],
	               [0, 2, 0, 0, 1],
	               [1, 2, 1, 1, 2],
	               [1, 0, 1, 0, 1]],
	              [[2, 0, 0, 0, 1],
	               [0, 2, 2, 0, 1],
	               [0, 0, 0, 0, 2],
	               [0, 1, 2, 0, 1],
	               [2, 0, 2, 2, 2]],
	              [[1, 1, 0, 1, 1],
	               [0, 1, 0, 1, 2],
	               [0, 2, 2, 0, 0],
	               [0, 2, 0, 0, 1],
	               [1, 1, 2, 1, 1]]])
	Filter=np.array([[[1,1,-1,-1,0,0,1,1,0],   \
	                  [-1,-1,0,0,-1,1,0,-1,0], \
	                  [-1,1,1,-1,1,-1,0,0,-1]]]).reshape(3,3,3)
	
	def output_size(data, Filter, stride, padding):
	    return int(((data.shape[1] + 2*padding - Filter.shape[1])/ stride ) +1)
	
	print(output_size( x, Filter, 1, 1))

	5




지금까지의 내용 정리 :

	원본이미지 1장  *  필터 50개   ───▶  feature map의 갯수 ( 50개 )
	    (7x7)	     (3x3)			(5x5)




문제 229. 설현사진 50장과 아이린 사진 50장, 총 100장의 사진을 신경망에 입력해서 설현과 아이린을
	  구분(분류)하는 신경망을 만든다고 할 때 RGB필터를 30개를 사용하면 출력 feature map이 총 몇개일까?

		100장 ───▶  conv층  ───▶  3000개

	합성곱 연산에서도 편향이 쓰이므로 편향을 더하면 어떤 그림일까 ?
		




문제 230. 칠판에 나온 아이린 사진 한장의 3차원 행렬을 만드시오 (RGB의 7x7 행렬 1장)

	import numpy as np
	
	x1 = np.random.rand(1, 3, 7, 7)
	print(x1)
	print(x1.shape)




문제 231. im2col함수를 이용해서 아래의 4차원을 2차원행렬로 변경하시오 ! ( 필터는 5x5의 RGB행렬을 사용함)

	def im2col(input_data, filter_h, filter_w, stride=1, pad=0):
	    """다수의 이미지를 입력받아 2차원 배열로 변환한다(평탄화).
	
	    Parameters
	    ----------
	    input_data : 4차원 배열 형태의 입력 데이터(이미지 수, 채널 수, 높이, 너비)
	    filter_h : 필터의 높이
	    filter_w : 필터의 너비
	    stride : 스트라이드
	    pad : 패딩
	
	    Returns
	    -------
	    col : 2차원 배열
	    """
	    N, C, H, W = input_data.shape
	    out_h = (H + 2 * pad - filter_h) // stride + 1
	    out_w = (W + 2 * pad - filter_w) // stride + 1
	
	    img = np.pad(input_data, [(0, 0), (0, 0), (pad, pad), (pad, pad)], 'constant')
	    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))
	
	    for y in range(filter_h):
	        y_max = y + stride * out_h
	        for x in range(filter_w):
	            x_max = x + stride * out_w
	            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]
	
	    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N * out_h * out_w, -1)
	    return col
	
	import numpy as np
	
	x1 = np.random.rand(1, 3, 7, 7)
	col = im2col(x1, 5, 5, stride=1, pad=0)
	print(col.shape)

	(9, 75)




문제 232. 아이린 사진 10장을 im2col 함수에 넣어서 2차원 행렬로 변환시키시오 !
	  ( 필터는 5x5의 RGB 행렬을 사용함 )

	import numpy as np
	
	x1 = np.random.rand(10, 3, 7, 7)
	col = im2col(x1, 5, 5, stride=1, pad=0)
	print(col.shape)

	(90, 75)




문제 233. 아래의 Filter를 생성하고 shape를 확인하고서 전치시키시오 !

	Filter = np.array([[[255,255,255],[255,255,255],[0,0,0],[255,255,255],[255,255,255]],
	                   [[255,255,255],[255,255,255],[0,0,0],[255,255,255],[255,255,255]],
	                   [[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],
	                   [[255,255,255],[255,255,255],[0,0,0],[255,255,255],[255,255,255]],
	                   [[255,255,255],[255,255,255],[0,0,0],[255,255,255],[255,255,255]]], dtype  = np.uint8)
	print(Filter.shape)
	print(Filter.T.shape)

	(5, 5, 3)
	(3, 5, 5)



문제 234. Filter (3, 5 ,5) 행렬을 (3, 25) 행렬로 변경하시오 !

	a = Filter.T.reshape(3,-1)
	print(a.shape)
	print(a)

	(3, 25)
	[[255 255   0 255 255 255 255   0 255 255   0   0   0   0   0 255 255   0
	  255 255 255 255   0 255 255]
	 [255 255   0 255 255 255 255   0 255 255   0   0   0   0   0 255 255   0
	  255 255 255 255   0 255 255]
	 [255 255   0 255 255 255 255   0 255 255   0   0   0   0   0 255 255   0
	  255 255 255 255   0 255 255]]




문제 235. 아래의 4차원 행렬의 filter를 numpy의 random을 이용해서 만드시오 !
	( 10, 3, 5, 5) ◀─── 4차원 행렬 ( RGB 5x5 필터 10개 )

	import numpy as np
	
	Filter2 = np.random.rand(10, 3, 5, 5)
	print(Filter2.shape)

	(10, 3, 5, 5)





문제 236. 아래의 4차원 행렬을 3차원으로 변경하시오 !

	import numpy as np
	
	Filter2 = np.random.rand(10, 3, 5, 5)
	print(Filter2.shape)
	print(Filter2.reshape(10,3,-1).shape)

	(10, 3, 5, 5)
	(10, 3, 25)




문제 237. 아래의 3차원 행렬을 2차원 행렬로 변경하시오 !
	 (10, 3, 25) ───▶ (10, 75)
	    3차원 		2차원

	import numpy as np
	
	Filter2 = np.random.rand(10, 3, 5, 5)
	
	Filter3 = Filter2.reshape(10,3,-1)
	print(Filter3.reshape(10,-1).shape)
	
	(10, 75)





문제 238. 아래의 2차원을 1차원 행렬로 변경하시오 !
	 (10, 75) ───▶ (750, )
	   2차원 	    1차원
	
	import numpy as np
	
	Filter2 = np.random.rand(10, 3, 5, 5)
	Filter3 = Filter2.reshape(10,3,-1)
	Filter4 = Filter3.reshape(10,-1)
	print(Filter4.reshape(-1).shape)
	
	(750,)




문제 239. 다음을 계산하여라 !
	x = [ 0 ,0 ,0 ,0 ,2 ,2 ,0 ,0 ,0 , 0 ,0 ,0 ,0 ,2 ,0 ,0 ,0 ,2 ,0 ,0 ,0 ,0 ,4 ,2 ,0 ,0  ,1  ]  
	y = [ 1, 1, -1, -1, 0, 0, 1, 1, 0, -1, -1, 0, 0, -1, 1, 0, -1, 0, -1,1,1,-1,1,- 1,0,0,-1]
	x = np.array(x).reshape(1,27)
	y = np.array(y).reshape(1,27)
	print(np.dot(x, y.T))

	[[-1]]





문제 240. 아래의 입력값과 Filter행렬을 점심시간 문제로 3차원 합성곱을 했었다. 그래서 결과가 아래와
	  같이 나왔다. 그런데 이번에는 3차원 합성곱하지 말고 차원을 축소시켜서 아래의 결과를 출력하시오 !
	  (오늘의 마지막 문제)

	import numpy as np
	
	data = np.array(
	       [
	         [[2, 2, 1, 1, 0],
	          [0, 0, 1, 0, 0],
	          [0, 2, 0, 0, 1],
	          [1, 2, 1, 1, 1],    #  --->  Red
	          [1, 0, 1, 0, 1]],
	         [[2, 0, 0, 0, 1],
	          [0, 2, 2, 0, 1],
	          [0, 0, 0, 0, 2],    # ---->  Green
	          [0, 1, 2, 0, 1],
	          [2, 0, 2, 2, 2]],
	         [[4, 2, 1, 2,2],
	          [0, 1, 0, 4,1],      # ---->  Blue
	          [3, 0, 6, 2,1],
	          [4, 2, 4, 5,4],
	          [0, 1, 2, 0, 1]]
	       ])
	Filter=np.array([[[1,1,-1,-1,0,0,1,1,0],   \
	                  [-1,-1,0,0,-1,1,0,-1,0], \
	                  [-1,1,1,-1,1,-1,0,0,-1]]]).reshape(3,3,3)
	
	data_im = im2col(data.reshape(1, 3, 5, 5), Filter.shape[2], Filter.shape[1], stride=1,pad=1)
	Filter2 = Filter.reshape(3,-1).reshape(-1).reshape(1,27)
	result = np.dot(data_im, Filter2.T).reshape(5,5)
	print(result)
	plt.imshow(result)
	plt.show()

	[[ -1.  -7. -10.  -1.  -3.]
	 [  5.  -3.  -4.   7.  -5.]
	 [  3. -13.  -2.  -1.  -6.]
	 [  2.  -2.   2.  -9.  -6.]
	 [  2.   3.   7.   0.  -1.]]



문제 241. 어제 마지막 문제는 4차원 행렬 원본 이미지와 4차원 행렬 필터를 2차원으로 차원 축소하여
	  내적한 결과이다. 이 2차원 결과를 다시 4차원으로 변경하시오!

	data = np.array(
	       [
	         [[2, 2, 1, 1, 0],
	          [0, 0, 1, 0, 0],
	          [0, 2, 0, 0, 1],
	          [1, 2, 1, 1, 1],    #  --->  Red
	          [1, 0, 1, 0, 1]],
	         [[2, 0, 0, 0, 1],
	          [0, 2, 2, 0, 1],
	          [0, 0, 0, 0, 2],    # ---->  Green
	          [0, 1, 2, 0, 1],
	          [2, 0, 2, 2, 2]],
	         [[4, 2, 1, 2,2],
	          [0, 1, 0, 4,1],      # ---->  Blue
	          [3, 0, 6, 2,1],
	          [4, 2, 4, 5,4],
	          [0, 1, 2, 0, 1]]
	       ])
	Filter=np.array([[[1,1,-1,-1,0,0,1,1,0],   \
	                  [-1,-1,0,0,-1,1,0,-1,0], \
	                  [-1,1,1,-1,1,-1,0,0,-1]]]).reshape(3,3,3)
	
	data_im = im2col(data.reshape(1, 3, 5, 5), Filter.shape[2], Filter.shape[1], stride=1,pad=1)
	Filter2 = Filter.reshape(3,-1).reshape(-1).reshape(1,27)
	res = np.dot(data_im, Filter2.T).reshape(5,5)
	result =res.reshape(1, 5, 5, -1)
			    ↑ ↑ ↑ ↑
			    N  H  W  C
	print(result.shape)

	(1, 5, 5, 1)


		------------------------------
		-- 아이린 사진 한장
		import  numpy  as  np
		
		x1 = np.random.rand(1,3,7,7)
				    N C H W
		print (x1)
		print (x1.shape)
		print (temp2)
		-------------------------------





문제 242. 문제 241번의 4차원 결과를 아이린 사진과 같은 4차원 결과로 출력되게 하시오 !

	data = np.array(
	       [
	         [[2, 2, 1, 1, 0],
	          [0, 0, 1, 0, 0],
	          [0, 2, 0, 0, 1],
	          [1, 2, 1, 1, 1],    #  --->  Red
	          [1, 0, 1, 0, 1]],
	         [[2, 0, 0, 0, 1],
	          [0, 2, 2, 0, 1],
	          [0, 0, 0, 0, 2],    # ---->  Green
	          [0, 1, 2, 0, 1],
	          [2, 0, 2, 2, 2]],
	         [[4, 2, 1, 2,2],
	          [0, 1, 0, 4,1],      # ---->  Blue
	          [3, 0, 6, 2,1],
	          [4, 2, 4, 5,4],
	          [0, 1, 2, 0, 1]]
	       ])
	Filter=np.array([[[1,1,-1,-1,0,0,1,1,0],   \
	                  [-1,-1,0,0,-1,1,0,-1,0], \
	                  [-1,1,1,-1,1,-1,0,0,-1]]]).reshape(3,3,3)
	
	data_im = im2col(data.reshape(1, 3, 5, 5), Filter.shape[2], Filter.shape[1], stride=1,pad=1)
	Filter2 = Filter.reshape(3,-1).reshape(-1).reshape(1,27)
	res = np.dot(data_im, Filter2.T).reshape(5,5)
	result = res.reshape(1, 5, 5, -1)
	result1 = result.transpose(0, 3, 1, 2)
	print(result1)

	[[[[ -1.  -7. -10.  -1.  -3.]
	   [  5.  -3.  -4.   7.  -5.]
	   [  3. -13.  -2.  -1.  -6.]
	   [  2.  -2.   2.  -9.  -6.]
	   [  2.   3.   7.   0.  -1.]]]]




문제 244. 책의 그림중 아래의 그림 7-28의 앞의 convolution층을 통과한 결과의 feature map의 shape를 출력하시오 !

	x = np.arange(154587).reshape(1, 3, 227, 227)
	Filter = np.arange(34848).reshape(96, 3, 11, 11)
	b1 = 1
	conv1 = Convolution(Filter, b1, stride = 4)
	fe_map = conv1.forward(x)
	print('fe_map의 shape', fe_map.shape)

	fe_map의 shape (1, 96, 55, 55)





문제 245. 파이썬으로 아래의 행렬을 만들고 max pooling을 시도해서 아래의 결과를 출력하시오 !

	 21  8  8 12
	 12 19  9  7
	  8 10  4  3  ───▶  21  12
	 18 12  9 10            18  10
	
	x = np.array([[21, 8, 8, 12], [12, 19, 9, 7], [9, 10, 4, 3], [18, 12, 9, 10]])
		
	def max_pooling(x, stride = 2):
	    res = []
	    for i in range(0, len(x), stride):
	        for j in range(0, len(x[i]), stride):
	            res.append(np.max(x[i:i+stride, j:j+stride]))
	    res = np.array(res).reshape(i,j)
	    return res
	print(max_pooling(x))
	
	[[21 12]
	 [18 10]]





문제 246. 아래의 입력 이미지 행렬 (4x4)를 4차원으로 변경한 후 im2col함수에 입력해서 결과를
	   출력하시오 !(stride=2, padding=0)

	x = np.array([[[21,8,8,12],[12,19,9,7],[8,10,4,3],[18,12,9,10]],
	              [[19,8,7,12],[1,19,9,7],[4,2,4,3],[4,12,9,10]],
	              [[2,8,8,12],[10,19,9,7],[5,6,4,3],[1,12,9,12]]])
	x2 = x.reshape(1,3,4,4)
	x3 = im2col(x2, 2, 2, stride=2)
	print(x3)

	[[ 21.   8.  12.  19.  19.   8.   1.  19.   2.   8.  10.  19.]
	 [  8.  12.   9.   7.   7.  12.   9.   7.   8.  12.   9.   7.]
	 [  8.  10.  18.  12.   4.   2.   4.  12.   5.   6.   1.  12.]
	 [  4.   3.   9.  10.   4.   3.   9.  10.   4.   3.   9.  12.]]





문제 247. 위의 결과를 4개씩 묶어서 출력하시오 !

	x = np.array([[[21,8,8,12],[12,19,9,7],[8,10,4,3],[18,12,9,10]],
	              [[19,8,7,12],[1,19,9,7],[4,2,4,3],[4,12,9,10]],
	              [[2,8,8,12],[10,19,9,7],[5,6,4,3],[1,12,9,12]]])
	x2 = x.reshape(1,3,4,4)
	x3 = im2col(x2, 2, 2, stride=2)
	x4 = x3.reshape(-1,4)
	print(x4)
	
	[[ 21.   8.  12.  19.]
	 [ 19.   8.   1.  19.]
	 [  2.   8.  10.  19.]
	 [  8.  12.   9.   7.]
	 [  7.  12.   9.   7.]
	 [  8.  12.   9.   7.]
	 [  8.  10.  18.  12.]
	 [  4.   2.   4.  12.]
	 [  5.   6.   1.  12.]
	 [  4.   3.   9.  10.]
	 [  4.   3.   9.  10.]
	 [  4.   3.   9.  12.]]





문제 248. (점심시간 문제) np.max를 이용해서 maxpooling한 결과를 출력하시오 !

	x = np.array([[[21,8,8,12],[12,19,9,7],[8,10,4,3],[18,12,9,10]],
	              [[19,8,7,12],[1,19,9,7],[4,2,4,3],[4,12,9,10]],
	              [[2,8,8,12],[10,19,9,7],[5,6,4,3],[1,12,9,12]]])
	x2 = x.reshape(1,3,4,4)
	x3 = im2col(x2, 2, 2, stride=2)
	x4 = x3.reshape(-1,4)
	x5 = np.max(x4, axis = 1)
	print(x5)

	[ 21.  19.  19.  12.  12.  12.  18.  12.  12.  10.  10.  12.]




문제 249. 칠판의 그림처럼 출력되게 하시오 !

그림 7-_-22
	x = np.array([[[21,8,8,12],[12,19,9,7],[8,10,4,3],[18,12,9,10]],
	              [[19,8,7,12],[1,19,9,7],[4,2,4,3],[4,12,9,10]],
	              [[2,8,8,12],[10,19,9,7],[5,6,4,3],[1,12,9,12]]])
	x2 = x.reshape(1,3,4,4)
	x3 = im2col(x2, 2, 2, stride=2)
	x4 = x3.reshape(-1,4)
	x5 = np.max(x4, axis = 1)
	x6 = x5.reshape(2,2,-1)
	
	print(x6)
	
	[[[ 21.  19.  19.]
	  [ 12.  12.  12.]]
	
	 [[ 18.  12.  12.]
	  [ 10.  10.  12.]]]




문제 250. 위의 결과로 아래의 결과를 출력하시오 !

	 [[[[ 21.  12.]
	    [ 18.  10.]]
	
	   [[ 19.  12.]
	    [ 12.  10.]]
	
	   [[ 19.  12.]
	    [ 12.  12.]]]]

그림 7-_-23

	x = np.array([[[21,8,8,12],[12,19,9,7],[8,10,4,3],[18,12,9,10]],
	              [[19,8,7,12],[1,19,9,7],[4,2,4,3],[4,12,9,10]],
	              [[2,8,8,12],[10,19,9,7],[5,6,4,3],[1,12,9,12]]])
	x2 = x.reshape(1,3,4,4)
	x3 = im2col(x2, 2, 2, stride=2)
	x4 = x3.reshape(-1,4)
	x5 = np.max(x4, axis = 1)
	x6 = x5.reshape(1,2,2,-1).transpose(0, 3, 1, 2)
	
	print(x6)

	[[[[ 21.  12.]
	   [ 18.  10.]]
	
	  [[ 19.  12.]
	   [ 12.  10.]]
	
	  [[ 19.  12.]
	   [ 12.  12.]]]]




문제 251. 위의 일련의 과정을 하나로 묶은것이 책 249페이지의 Pooling클래스입니다.Pooling클래스를
	  생성하세요 !

	class Max_Pooling:
	    def __init__(self, pool_h, pool_w, stride=1, pad=0):
	        self.pool_h = pool_h
	        self.pool_w = pool_w
	        self.stride = stride
	        self.pad = pad
	        
	    def forward(self, x):
	        N, C, H, W = x.shape
	        out_h = int(1 + (H - self.pool_h) / self.stride)
	        out_w = int(1 + (H - self.pool_w) / self.stride)
	        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)
	        col = col.reshape(-1, self.pool_h * self.pool_w)
	        out = np.max(col, axis = 1)
	        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)
	        return out




문제252. 위에서 만든 Pooling 클래스를 객체화 시켜서 아래의 원본 이미지의 maxpooling 한 결과를 출력하시오 !

	x = np.array([[[21,8,8,12],[12,19,9,7],[8,10,4,3],[18,12,9,10]],
	              [[19,8,7,12],[1,19,9,7],[4,2,4,3],[4,12,9,10]],
	              [[2,8,8,12],[10,19,9,7],[5,6,4,3],[1,12,9,12]]])
	
	x2 = x.reshape(1,3,4,4)
	
	pool = Max_Pooling(2, 2, stride = 2)
	print(pool.forward(x2))
	
	[[[[ 21.  12.]
	   [ 18.  10.]]
	
	  [[ 19.  12.]
	   [ 12.  10.]]
	
	  [[ 19.  12.]
	   [ 12.  12.]]]]




문제 253. 위의 코드를 이번에는 최대풀링이 아니라 평균풀링으로 구현하시오 !

	class Mean_Pooling:
	    def __init__(self, pool_h, pool_w, stride=1, pad=0):
	        self.pool_h = pool_h
	        self.pool_w = pool_w
	        self.stride = stride
	        self.pad = pad
	        
	    def forward(self, x):
	        N, C, H, W = x.shape
	        out_h = int(1 + (H - self.pool_h) / self.stride)
	        out_w = int(1 + (H - self.pool_w) / self.stride)
	        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)
	        col = col.reshape(-1, self.pool_h * self.pool_w)
	        out = np.mean(col, axis = 1)
	        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)
	        return out
	    
	x = np.array([[[21,8,8,12],[12,19,9,7],[8,10,4,3],[18,12,9,10]],
	              [[19,8,7,12],[1,19,9,7],[4,2,4,3],[4,12,9,10]],
	              [[2,8,8,12],[10,19,9,7],[5,6,4,3],[1,12,9,12]]])
	
	x2 = x.reshape(1,3,4,4)
	
	pool = Mean_Pooling(2, 2, stride = 2)
	print(pool.forward(x2))
	
	[[[[ 15.     9.  ]
	   [ 12.     6.5 ]]
	
	  [[ 11.75   8.75]
	   [  5.5    6.5 ]]
	
	  [[  9.75   9.  ]
	   [  6.     7.  ]]]]




문제 254. 위의 풀링을 이번에는 확률적 풀링으로 수행하시오 !

	class Statistical_Pooling:
	    def __init__(self, pool_h, pool_w, stride=1, pad=0):
	        self.pool_h = pool_h
	        self.pool_w = pool_w
	        self.stride = stride
	        self.pad = pad
	        
	    def forward(self, x):
	        N, C, H, W = x.shape
	        out_h = int(1 + (H - self.pool_h) / self.stride)
	        out_w = int(1 + (H - self.pool_w) / self.stride)
	        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)
	        col = col.reshape(-1, self.pool_h * self.pool_w)
	        out = np.array([np.random.choice(i) for i in col])
	        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)
	        return out
	    
	x = np.array([[[21,8,8,12],[12,19,9,7],[8,10,4,3],[18,12,9,10]],
	              [[19,8,7,12],[1,19,9,7],[4,2,4,3],[4,12,9,10]],
	              [[2,8,8,12],[10,19,9,7],[5,6,4,3],[1,12,9,12]]])
	
	x2 = x.reshape(1,3,4,4)
	
	pool = Statistical_Pooling(2, 2, stride = 2)
	print(pool.forward(x2))

	[[[[ 19.   8.]
	   [ 10.   4.]]
	
	  [[  8.  12.]
	   [  2.  10.]]
	
	  [[  8.   7.]
	   [  6.   4.]]]]





문제 255. 아래의 그림을 노트에 구현해 보시오 !
그

문제 255. 최종 코드
---------------------------------------------------------------------------------------------
# coding: utf-8
import sys, os

sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정
import pickle
import numpy as np
from collections import OrderedDict
from common.layers import *
from common.gradient import numerical_gradient
import matplotlib.pyplot as plt
from dataset.mnist import load_mnist
from common.trainer import Trainer


class SimpleConvNet:
    def __init__(self, input_dim=(1, 28, 28),
                 conv_param1={'filter_num': 32, 'filter_size': 5, 'pad': 2, 'stride': 1},
                 conv_param2={'filter_num': 64, 'filter_size': 5, 'pad': 2, 'stride': 1},
                 output_size=10, weight_init_std=np.sqrt(2/50)):

        self.params = {}

        self.layers = OrderedDict()
        
        filter_num1 = conv_param1['filter_num']
        filter_size1 = conv_param1['filter_size']
        filter_pad1 = conv_param1['pad']
        filter_stride1 = conv_param1['stride']
        input_size1 = input_dim[1]
        conv_output_size1 = (input_size1 - filter_size1 + 2 * filter_pad1) / filter_stride1 + 1
        pool_output_size1 = int(filter_num1 * (conv_output_size1 / 2) * (conv_output_size1 / 2))
        self.params['W1'] = weight_init_std * \
                            np.random.randn(filter_num1, input_dim[0], filter_size1, filter_size1)
        self.params['b1'] = np.zeros(filter_num1)
        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'], conv_param1['stride'], conv_param1['pad'])
        self.layers['Relu1'] = Relu()
        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)

        filter_num2 = conv_param2['filter_num']
        filter_size2 = conv_param2['filter_size']
        filter_pad2 = conv_param2['pad']
        filter_stride2 = conv_param2['stride']
        input_size2 = 14
        conv_output_size2 = (input_size2 - filter_size2 + 2 * filter_pad2) / filter_stride2 + 1
        pool_output_size2 = int(filter_num2 * (conv_output_size2 / 2) * (conv_output_size2 / 2))
        self.params['W2'] = weight_init_std * \
                            np.random.randn(filter_num2, 32, filter_size2, filter_size2)
        self.params['b2'] = np.zeros(filter_num2)
        self.layers['Conv2'] = Convolution(self.params['W2'], self.params['b2'], conv_param2['stride'], conv_param2['pad'])
        self.layers['Relu2'] = Relu()
        self.layers['Pool2'] = Pooling(pool_h=2, pool_w=2, stride=2)
        
        self.params['W3'] = weight_init_std * \
                            np.random.randn(pool_output_size2, output_size)
        self.params['b3'] = np.zeros(output_size)
        self.layers['Affine1'] = Affine(self.params['W3'], self.params['b3'])
        self.last_layer = SoftmaxWithLoss()

    def predict(self, x):
        for layer in self.layers.values():
            x = layer.forward(x)
        return x

    def loss(self, x, t):
        y = self.predict(x)
        return self.last_layer.forward(y, t)

    def accuracy(self, x, t, batch_size=100):
        if t.ndim != 1: t = np.argmax(t, axis=1)
        acc = 0.0
        for i in range(int(x.shape[0] / batch_size)):
            tx = x[i * batch_size:(i + 1) * batch_size]
            tt = t[i * batch_size:(i + 1) * batch_size]
            y = self.predict(tx)
            y = np.argmax(y, axis=1)
            acc += np.sum(y == tt)
        return acc / x.shape[0]

    def numerical_gradient(self, x, t):
        loss_w = lambda w: self.loss(x, t)
        grads = {}
        for idx in (1, 2, 3):
            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])
            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])
        return grads

    def gradient(self, x, t):
        self.loss(x, t)
        dout = 1
        dout = self.last_layer.backward(dout)
        layers = list(self.layers.values())
        layers.reverse()
        for layer in layers:
            dout = layer.backward(dout)
        grads = {}
        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db
        grads['W2'], grads['b2'] = self.layers['Conv2'].dW, self.layers['Conv2'].db
        grads['W3'], grads['b3'] = self.layers['Affine1'].dW, self.layers['Affine1'].db
        return grads

    def save_params(self, file_name="params.pkl"):
        params = {}
        for key, val in self.params.items():
            params[key] = val
        with open(file_name, 'wb') as f:
            pickle.dump(params, f)

    def load_params(self, file_name="params.pkl"):
        with open(file_name, 'rb') as f:
            params = pickle.load(f)
        for key, val in params.items():
            self.params[key] = val
        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):
            self.layers[key].W = self.params['W' + str(i + 1)]
            self.layers[key].b = self.params['b' + str(i + 1)]





(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)
max_epochs = 20
network = SimpleConvNet(input_dim=(1, 28, 28),
                        conv_param1={'filter_num': 32, 'filter_size': 5, 'pad': 2, 'stride': 1},
                        conv_param2={'filter_num': 64, 'filter_size': 5, 'pad': 2, 'stride': 1},
                        output_size=10, weight_init_std=0.01)



network.save_params("params.pkl")
print("Saved Network Parameters!")


iters_num = 10000
train_size = x_train.shape[0]
batch_size = 100
learning_rate = 0.1
train_loss_list = []
train_acc_list = []
test_acc_list = []



iter_per_epoch = max(train_size / batch_size, 1)
print(iter_per_epoch)

for i in range(iters_num):
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]

    grad = network.gradient(x_batch, t_batch)

    for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):
        network.params[key] -= learning_rate * grad[key]

    loss = network.loss(x_batch, t_batch)
    train_loss_list.append(loss)

    if i % iter_per_epoch == 0:
        print(x_train.shape)
        train_acc = network.accuracy(x_train, t_train)
        test_acc = network.accuracy(x_test, t_test)
        train_acc_list.append(train_acc)
        test_acc_list.append(test_acc)
        print("train acc, test acc | " + str(train_acc) + ", " + str(test_acc))

markers = {'train': 'o', 'test': 's'}
x = np.arange(len(train_acc_list))
plt.plot(x, train_acc_list, label='train acc')
plt.plot(x, test_acc_list, label='test acc', linestyle='--')
plt.xlabel("epochs")
plt.ylabel("accuracy")
plt.ylim(0, 1.0)
plt.legend(loc='lower right')
plt.show()
--------------------------------------------------------------------------------------------------
