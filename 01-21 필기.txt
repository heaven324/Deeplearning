■ 공지사항

	1. 3/26일 최종 포트폴리오 발표

		- 딥러닝 포트폴리오 발표 ( 팀별로 발표 )
			팀 구성 3명 ~ 4명 사이로
		- 지난기수 학생들의 잘된 포트폴리오를 가지고 파이썬 문제
		- 얼굴인식, 지문인식하지 않고 얼굴인식으로 출석체크 등 다양한 포트폴리오

	2. NCS 하둡 시험 --> 이번주 금요일 4시






■ 지난주까지 배웠던 딥러닝 수업 복습

	1장. 이 책을 공부하기 위해 필요한 파이썬 문법
			- numpy
			- matplotlib

	2장. 퍼셉트론 
		- 단층 퍼셉트론 ( 입력층 ---> 출력층 ) 
		- 다층 퍼셉트론 ( 입력층 ---> 은닉층 ---> 출력층 )

	3장. 3층 신경망 구현 
		" 이미 최적화 되어있는 가중치와 바이어스를 이용해서 구현"

			- 활성화 함수 ( 계단, 시그모이드, 렐루 )
			- 출력층 함수 ( 항등함수, 소프트 맥스 함수 )

	4장. 3층 신경망 학습
		"수치미분을 이용해서 신경망을 학습을 시킴"

			- 오차(비용) 함수 ( 평균 제곱 오차 함수, 교차 엔트로피 함수 )
			- 비용함수를 미분하기 위한 수치미분 함수
			- 3층 신경망 구현을 위한 클래스 생성
-----남은거
	5장. 3층 신경망 학습 ( 오차 역전파를 이용해서 학습 시킴 ) 
	6장. 신경망의 정확도를 올리기 위한 여러가지 방법들
	7장. CNN으로 신경망 구현






-------------------------------------------------------
import numpy as np

class simpleNet:
    def __init__(self):
        self.w = np.random.randn(2, 3)

    def predict(self,x):
        a = np.dot(x,self.w)
        return a

    def softmax(self,a):
        c = np.max(a)
        minus = a - c
        exp_a = np.exp(minus)
        sum_exp_a = np.sum(exp_a)
        y = exp_a / sum_exp_a
        return y

    def cross_entropy_error(self,x,t):
        delta = 1e-7
        return -np.sum(t*np.log(x+delta))
    
    def loss(self,x,t): # 종합선물세트
        p = self.predict(x)
        s = self.softmax(p)
        loss = self.cross_entropy_error(s, t)
        return loss
---------------------------------------------------------




문제 140. 위의 simpleNet클래스의 loss함수를 실행시켜 보시오 !

	import numpy as np
	
	net = simpleNet()
	x = np.array([0.6, 0.9])
	t = np.array([0, 0, 1])
	print(net.loss(x,t))

	2.73113096106




문제 141. simpleNet 클래스의 가중치 W가 몇행 몇열인지 출력하시오 !

	net = simpleNet()
	print(net.w.shape)

	(2, 3)




문제 142. 칠판에 나온대로 simpleNet클래스의 __init__함수를 가중치 W1은 784 x 50 으로 생성되게 하고 가중치 
	  W2는 50 x 10 으로 가중치가 랜덤으로 생성되게 코드를 수정하시오 !

	import numpy as np
	
	class simpleNet:
	    def __init__(self):
	        self.params = {}
	        self.params['W1'] = 0.01 * np.random.randn(784, 50)  # 0.01 --> 가중치 초기화에 대한 값
	        self.params['b1'] = np.zeros(50)                     #          6장에서 배운다
	        self.params['W2'] = 0.01 * np.random.randn(50, 10)
	        self.params['b2'] = np.zeros(10)




문제 143. simpleNet 클래스의 가중치 W1과 W2의 shape을 출력하시오 !

	net = simpleNet()
	
	print(net.params.get('W1').shape)
	print(net.params.get('W2').shape)
	print(net.params.get('b1').shape)
	print(net.params.get('b1').shape)

	(784, 50)
	(50, 10)
	(50,)
	(50,)




문제 144. simpleNet class를 객체화 시킬 때 아래와 같이 실행되게 하시오 !

	import numpy as np
	
	class simpleNet:
	    def __init__(self, input_size, hidden_size, output_size):
	        self.params = {}
	        self.params['W1'] = 0.01 * np.random.randn(input_size, hidden_size)
	        self.params['b1'] = np.zeros(hidden_size)
	        self.params['W2'] = 0.01 * np.random.randn(hidden_size, output_size)
	        self.params['b2'] = np.zeros(output_size)


	net = simpleNet(input_size = 784, hidden_size = 50, output_size = 10)
	print('W1', net.params['W1'].shape)
	print('W2', net.params['W2'].shape)
	print('b1', net.params['b1'].shape)
	print('b2', net.params['b2'].shape)

	W1 (784, 50)
	W2 (50, 10)
	b1 (50,)
	b2 (10,)




문제 146. mnist데이터 100장만 입력해서 predict 함수를 통과한 결과가 어떻게 되는지 확인하시오 !

	import numpy as np
	from dataset.mnist import load_mnist

	def  get_data():
	    (x_train, t_train) , (x_test, t_test) = \
	    load_mnist(normalize=True, flatten=True, one_hot_label=True)
	    return  x_test, t_test
	
	
	class simpleNet:
	    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):
	        self.params = {}
	        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)
	        self.params['b1'] = np.zeros(hidden_size)
	        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)
	        self.params['b2'] = np.zeros(output_size)
	
	    def sigmoid(self, data):
	        return 1/( 1 + np.exp(-data) )
	
	    def predict(self,x):
	        W1, W2 , b1, b2 = self.params['W1'], self.params['W2'], self.params['b1'], self.params['b2']
	        a = np.dot(x, W1) + b1
	        a1 = self.sigmoid(a)
	        b = np.dot(a1, W2) + b2
	        b1 = self.sigmoid(b)
	        return b1
	
	    def softmax(self,a):
	        c = np.max(a)
	        minus = a - c
	        exp_a = np.exp(minus)
	        sum_exp_a = np.sum(exp_a)
	        y = exp_a / sum_exp_a
	        return y
	
	    def cross_entropy_error(self,x,t):
	        delta = 1e-7
	        return -np.sum(t*np.log(x+delta))
	    
	    def loss(self,x,t):
	        p = self.predict(x)
	        s = self.softmax(p)
	        loss = self.cross_entropy_error(s, t)
	        return loss
	    
	net = simpleNet(input_size = 784, hidden_size = 50, output_size = 10)
	x, t = get_data()
	y = net.predict(x[:100])
	print(y.shape)
	
	(100, 10)








★ accuracy (정확도) 구하는 함수의 이해
	"예상한 숫자와 실제 숫자를 비교해서 정확도를 출력하는 함수"

	
	예제 : 
		def accuracy(self, x, t):
		    y = self.predict(x) # 10개짜리 확률벡터 100개 출력
		                        # 100 x 10 행렬
		# - y 값의 행렬 
		
		#  [ [ 0.1 0.1 0.05 0.4 0.05 0.1 0.05 0.05 0.05 0.05 0.00],  # 3
		#    [ 0.4 0.1 0.05 0.1 0.05 0.1 0.05 0.05 0.05 0.05 0.00],  # 0
		#                  :  100개
		#                  :
		#    [ 0.1 0.1 0.05 0.4 0.05 0.1 0.05 0.05 0.05 0.05 0.00] ] 
		
		# - target 의 행렬 (one hot encoding) 
		
		#             [ [ 0 0 1 0 0 0 0 0 0 0  ],
		#               [ 1 0 0 0 0 0 0 0 0 0  ],
		#                          :
		#                          :     100개 
		#               [ 0 0 1 0 0 0 0 0 0 0  ] ] 
		
		    y = np.argmax(y, axis=1) # [ 3 0 1 5 6 .......... 3 ]  # 100개 
		    t = np.argmax(t, axis=1) # [ 2 0 2 1 3 ...........3 ]  # 100개 
		    accuracy = np.sum(y == t) / float(x.shape[0])
		    return accuracy





문제 147. 정확도를 구하는 accuracy함수를 simpleNet클래스에 추가하고 100장을 입력해서 정확도를 
	  출력해보시오 !

	import numpy as np
	from dataset.mnist import load_mnist
	
	class simpleNet:
	    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):
	        self.params = {}
	        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)
	        self.params['b1'] = np.zeros(hidden_size)
	        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)
	        self.params['b2'] = np.zeros(output_size)
	
	    def  get_data():
	        (x_train, t_train) , (x_test, t_test) = \
	        load_mnist(normalize=True, flatten=True, one_hot_label=True)
	        return  x_test, t_test
	    
	    def sigmoid(self, data):
	        return 1/( 1 + np.exp(-data) )
	
	    def predict(self,x):
	        W1, W2 , b1, b2 = self.params['W1'], self.params['W2'], self.params['b1'], self.params['b2']
	        a = np.dot(x, W1) + b1
	        a1 = self.sigmoid(a)
	        b = np.dot(a1, W2) + b2
	        b1 = self.sigmoid(b)
	        return b1
	
	    def accuracy(self, x, t):
	        y = self.predict(x)
	        y = np.argmax(y, axis=1)
	        t = np.argmax(t, axis=1)
	        accuracy = np.sum(y == t) / float(x.shape[0])
	        return accuracy
	
	    def softmax(self,a):
	        c = np.max(a)
	        minus = a - c
	        exp_a = np.exp(minus)
	        sum_exp_a = np.sum(exp_a)
	        y = exp_a / sum_exp_a
	        return y
	
	    def cross_entropy_error(self,x,t):
	        delta = 1e-7
	        return -np.sum(t*np.log(x+delta))
	    
	    def loss(self,x,t):
	        p = self.predict(x)
	        s = self.softmax(p)
	        loss = self.cross_entropy_error(s, t)
	        return loss
	    
	net = simpleNet(input_size = 784, hidden_size = 50, output_size = 10)
	x, t = get_data()
	print(net.accuracy(x[:100], t[:100]))
	
	0.14








★ numerical_gradient 함수 생성

	"비용함수와 가중치 또는 바이어스를 입력받아 기울기를 출력하는 함수"
	
	예제 :
		from common.gradient import numerical_gradient
		def numerical_differentiation(self,x,t):
		    loss_W = lambda w: self.loss(x,t)
		    grads = {}
		    grads['W1'] = numerical_gradient(loss_W, self.params['W1'])
		    grads['b1'] = numerical_gradient(loss_W, self.params['b1'])
		    grads['W2'] = numerical_gradient(loss_W, self.params['W2'])
		    grads['b2'] = numerical_gradient(loss_W, self.params['b1'])
		    return grads




★ 비용함수 생성 방법

	net = simpleNet(input_size = 784, hidden_size = 50, output_size = 10)

	def f(W):
	    return metwork.loss(x, t)

	설명 : 135 페이지
		여기서 정의한 f(W) 함수의 W의 의미는 더미로 만든것 입니다.
		numerical_gradient(f,x) 내부에서 f(x)를 실행하는데 그와의 일관성을 위해서 f(W)를
		정의한 것입니다.
		lambda 표현식으로 한줄로 표현하면 아래와 같습니다.

		f = lambda W: network.loss(x, t)
		dW = numerical_gradient(f, net.W)
		↑
		W의 기울기



문제 148. x[0]와 t[0]을 입력해서 얻은 기울기 grads['W1'], grads['b1'], grads['W2'], grads['b2'] 
	  행렬의 shape는 어떻게 되는가? 

	import numpy as np
	from dataset.mnist import load_mnist
	from common.gradient import numerical_gradient

	class simpleNet:
	    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):
	        self.params = {}
	        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)
	        self.params['b1'] = np.zeros(hidden_size)
	        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)
	        self.params['b2'] = np.zeros(output_size)
	
	    def  get_data(self):
	        (x_train, t_train) , (x_test, t_test) = \
	        load_mnist(normalize=True, flatten=True, one_hot_label=True)
	        return  x_test, t_test
	    
	    def sigmoid(self, data):
	        return 1/( 1 + np.exp(-data) )
	
	    def predict(self,x):
	        W1, W2 , b1, b2 = self.params['W1'], self.params['W2'], self.params['b1'], self.params['b2']
	        a = np.dot(x, W1) + b1
	        a1 = self.sigmoid(a)
	        b = np.dot(a1, W2) + b2
	        b1 = self.sigmoid(b)
	        return b1
	
	    def accuracy(self, x, t):
	        y = self.predict(x)
	        y = np.argmax(y, axis=1)
	        t = np.argmax(t, axis=1)
	        accuracy = np.sum(y == t) / float(x.shape[0])
	        return accuracy
	
	    def softmax(self,a):
	        c = np.max(a)
	        minus = a - c
	        exp_a = np.exp(minus)
	        sum_exp_a = np.sum(exp_a)
	        y = exp_a / sum_exp_a
	        return y
	
	    def cross_entropy_error(self,x,t):
	        delta = 1e-7
	        return -np.sum(t*np.log(x+delta))
	    
	    def loss(self,x,t):
	        p = self.predict(x)
	        s = self.softmax(p)
	        loss = self.cross_entropy_error(s, t)
	        return loss
	
	    def numerical_differentiation(self,x,t):
	        loss_W = lambda W: self.loss(x, t)
	        grads = {}
	        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])
	        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])
	        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])
	        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])
	        return grads
	
	net = simpleNet(input_size = 784, hidden_size = 50, output_size = 10)
	x, t = net.get_data()
	y = net.numerical_differentiation(x[0], t[0])
	print(y['W1'].shape)
	print(y['b1'].shape)
	print(y['W2'].shape)
	print(y['b2'].shape)

	(784, 50)
	(50,)
	(50, 10)
	(10,)

	그림 4-_-6




문제 149. 지금까지 완성한 simpleNet 클래스를 가지고 만든 신경망에 훈련데이터 100장씩 미니배치해서 
	  학습시키는 코드를 구현하시오 !

	import numpy as np
	from dataset.mnist import load_mnist
	from common.gradient import numerical_gradient
	
	(x_train, t_train) , (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)
	   ↓       ↓	       ↓	↓
	60000장	   60000개   10000개  10000개
	훈련데이터 훈련데이터  테스트데이터
	   이미지    라벨	이미지,라벨

	train_loss_list = [] # [6.9, 6.7, 6.5 ...] 오차를 에폭마다 입력
	
	#하이퍼 파라미터 ( 신경망 학습을 최적화 하기 위해 사람이 조절해야하는 값)
	iters_num = 10000 		#반복횟수
	train_size = x_train.shape[0] 	# (60000 , 784) --> 60000
	batch_size = 100		# 미니배치 크기
	learning_rate = 0.1		# 학습률( 학습 발자국 )
	net = simpleNet(input_size = 784, hidden_size = 50, output_size = 10)
	
	for i in range(iters_num):	# 10000  1에폭 : 600, 2에폭 : 1200, 3에폭 : 1800 ....
	    #미니배치 획득
	    batch_mask = np.random.choice(train_size, batch_size)
					     60000       100
		# 0 ~ 60000 의 숫자중에 100개를 랜덤으로 선택하겠다.
	
		32, 5001, 4051, .................... 33 (100개)

	    x_batch = x_train[batch_mask] # 100 x 784
	    t_batch = t_train[batch_mask] # 100 x 10
	     
	    #기울기 계산
	    grad = net.numerical_differentiation(x_batch, t_batch) # 수치미분으로 구현한 함수
	
	    #매개변수 갱신
	    for key in ('W1', 'b1', 'W2', 'b2'):
	        net.params[key] -= learning_rate * grad[key]
	    
	    #학습 경과 기록
	    loss = net.loss(x_batch, t_batch)
	    train_loss_list.append(loss)


















