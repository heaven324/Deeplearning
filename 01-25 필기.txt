	수치미분을 이용한 기울기 : 
		- 3층 신경망을 학습 시켜야 한다고 하면 구해야 하는 기울기 ?
		  W1, W2, W3, b1, b2, b3에 대하여 각각 기울기를 구해야 한다.

		  W1 = W1 - 기울기 ( ∂오차함수 / ∂W1 )

		  W1만해도 벌써 784 x 100개의 W1이 있으므로 수치미분을 이용해서 기울기룰 구하려면 시간이
		  상당히 걸린다.


	오차 역전파를 이용한 기울기 :
		──▶ W1 ──▶ Relu1 ──▶ W2 ──▶ Relu2 ──▶ W3 ──▶ Softmax ──▶ Cost func
		       ↓	  ↓	      ↓	 ↓	     ↓		  ↓
		   backward    backward    backward   backward    backward     backward

			∂k    ∂m    ∂g    ∂y    ∂t    ∂E (오차함수)    ∂E
			── x ── x ── x ── x ── x ──		   = ──
			∂W1   ∂k    ∂m    ∂g    ∂y    ∂t		     ∂W1

		   (p 154) chain rule

		※ 위의 연쇄 법칙은 합성함수 미분과 같다.
		   합성함수 ? 두개의 함수를 이어붙여서 한개의 함수로 표현하는것
			x ──▶ f(x) ──▶ g(f(x)) ──▶ z(g(f(x))) ──▶ k(z(g(f(x))))
				  ↑	       ↑		↑		    ↑
				Relu1	      Relu2	      Softmax		 Loss Func

		k(z(g(f(x))))를 미분 ──▶ k(z(g(f(x))))´ * z(g(f(x)))´ * g(f(x))´ * f(x)´

			∂k      ∂k    ∂z    ∂g    ∂f
			──  =  ── x ── x ── x ──
			∂x      ∂z    ∂g    ∂f    ∂x













■ 6장. 신경망의 정확도를 높이기 위한 여러가지 방법들 소개

	* 목차
		1. 고급 경사 감소법들 	─▶ underfitting을 막기 위해
		2. 가중치 초기화값 설정 ─▶ underfitting을 막기 위해
		3. 배치 정규화 		─▶ underfitting을 막기 위해
		4. Dropout 기법 	─▶ overfitting을 막기위해

		5. 기타 방법 (이미지 한장을 회전, 밝기, 반전을 시켜서 여러개의 이미지를 만들어서 학습
			      시키는 방법)
			" 학습이 잘 되도록 데이터를 생성하는 방법 "




□ 1. 고급 경사 감소법들

	1. SGD
	2. Momentum
	3. Adagrade
	4. Adam




★ 1. SGD ( Stochastic Gradient Descent )
	      확률적    경사    감소법

	딥러닝 면접문제 : 
		GD(Gradient Descent)의 단점은 무엇인가?
			1. 학습 데이터를 모두 입력해서 한걸음 이동하므로 시간이 많이 걸린다.
				그래서 이 문제를 해결하기 위해서 나온것이 ?
					SGD ( Stochastic Gradient Descent )
					        확률적    경사    감소법

			2. 기울기가 global minima 쪽으로 기울어진 방향이 아니기 때문에 global minima
			   쪽으로 가지 못하고 local minima에 빠지는 단점이 있다.
			그림 6-_-1
			그림 6-_-2

			   그림 6-2 처럼 global minima쪽으로 기울기 방향이 있는게 아니기 때문에 
			   local minima에 빠지고 global minima쪽으로 가지 못하는 단점이 있다.

		SGD의 단점을 해결한 경사 감소법 ?





★ 2. momentum(운동량)

	기존 SGD : 가중치     ◀─    가중치 - 러닝레이트 * 기울기

	momentum :   W	      ◀─    가중치 + 속도
		   가중치     ◀─    가중치 + 속도
		    속도      ◀─    0.9 * 속도 - 러닝레이트 * 기울기
				      ↑
				   마찰 계수
				      ↑
				물체가 아무런 힘을 받지 않을 때 서서히 하강시키는 역할을 한다.
				물리에서의 지면 마찰이나 공기저항에 해당한다.
				내리막 : 기울기 음수이면 속도가 증가
				오르막 : 기울기 양수이면 속도가 감소

			기울어진 방향으로 물체가 가속되는 관성의 원리를 적용 !

		※ 관성 ──▶ 정지해 있는 물체는 계속 정지해 있으려 하고 운동하는 물체는 계속
			       운동하려는 성질
			(예 : 100미터 달리기 결승점에서 갑자기 멈춰지지 않는 현상)






★ 3. Adagrade 경사 감소법
	" Learning Rate(학습률)이 신경망 학습이 되면서 자체적으로 조정이 되는 경사 감소법"

	그림 6-_-3     그림 6-_-4

	SGD			   vs		Adagrade
			∂L				     ∂L     ∂L
	W  ◀─ W - n * ──			h  ◀─  h + ── ⊙ ──
			∂W				     ∂W     ∂W
						⊙ 는 행렬의 원소별 곱셈을 의미
								   1	 ∂L
						W  ◀─  W - η * ── x ──
								  √h    ∂W
						1. 처음에는 학습률이 크다가 조금씩 감소
						2. ⊙ 는 행렬의 원소별 곱셈을 의미하는데 h의 원소가
						   각각의 매개변수 원소변화량에 의해 결정된다.
								↓
						"각각의 매개변수 원소가 갱신되는 값이 다르다."
								↓
						"많이 움직인 원소일 수록 누적 h의 값이 크니까 갱신되는
						 정도가 그만큼 감소한다."






★ 4. Adam 경사 감소법
	"Momentum의 장점 + Adagrade의 장점을 살린 경사 감소법"
	  (가속도)	      (각 매개변수마다 학습률 조절)

	최근에 딥러닝에 가장 많이 사용하는 최적화 방법

	그림 6-_-5	그림 6-_-6








===================================================================================================
import sys, os
sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정
import numpy as np
from common.layers import *
from common.gradient import numerical_gradient
from collections import OrderedDict
import matplotlib.pyplot as plt
from dataset.mnist import load_mnist

class SGD:
    def __init__(self, lr = 0.01):
        self.lr = lr
        
    def update(self, params, grads):
        for key in params.keys():
            params[key] -= self.lr * grads[key]

class Momentum:
    def __init__(self, lr=0.01, momentum = 0.9):
        self.lr = lr
        self.momentum = momentum
        self.v = None
        
    def update(self, params, grads):
        if self.v is None:
            self.v = {}
            for key, val in params.items():
                self.v[key] = np.zeros_like(val)
        for key in params.keys():
            self.v[key] = self.momentum*self.v[key] - self.lr*grads[key]
            params[key] += self.v[key]

class Adagrade:
    def __init__(self, lr=0.01):
        self.lr = lr
        self.h = None
        
    def update(self, params, grads):
        if self.h is None:
            self.h = {}
            for key, val in params.items():
                self.h[key] = np.zeros_like(val)
        for key in params.keys():
            self.h[key] += grads[key] * grads[key]
            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)

class Adam:
    """Adam (http://arxiv.org/abs/1412.6980v8)"""
    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):
        self.lr = lr
        self.beta1 = beta1
        self.beta2 = beta2
        self.iter = 0
        self.m = None
        self.v = None
        
    def update(self, params, grads):
        if self.m is None:
            self.m, self.v = {}, {}
            for key, val in params.items():
                self.m[key] = np.zeros_like(val)
                self.v[key] = np.zeros_like(val)
        self.iter += 1
        lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)         
        for key in params.keys():
            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])
            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])
            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)


class TwoLayerNet:
    def __init__(self, input_size, hidden_size1, hidden_size2, output_size1, output_size2, weight_init_std=0.01):
        self.params = {}
        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size1)
        self.params['b1'] = np.zeros(hidden_size1)
        self.params['W2'] = weight_init_std * np.random.randn(hidden_size1, output_size1)
        self.params['b2'] = np.zeros(output_size1)
        self.params['W3'] = weight_init_std * np.random.randn(hidden_size2, output_size2)
        self.params['b3'] = np.zeros(output_size2)
        self.layers = OrderedDict()
        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])
        self.layers['Relu1'] = Relu()
        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])
        self.layers['Relu2'] = Relu()
        self.layers['Affine3'] = Affine(self.params['W3'], self.params['b3'])
        self.lastLayer = SoftmaxWithLoss()

    def predict(self, x):
        for layer in self.layers.values():
            x = layer.forward(x)
        return x

    def loss(self, x, t):
        y = self.predict(x)
        return self.lastLayer.forward(y, t)


    def accuracy(self, x, t):
        y = self.predict(x)
        y = np.argmax(y, axis=1)
        if t.ndim != 1: t = np.argmax(t, axis=1)
        accuracy = np.sum(y == t) / float(x.shape[0])
        return accuracy

    def numerical_gradient(self, x, t):
        loss_W = lambda W: self.loss(x, t)
        grads = {}
        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])
        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])
        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])
        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])
        grads['W3'] = numerical_gradient(loss_W, self.params['W3'])
        grads['b3'] = numerical_gradient(loss_W, self.params['b3'])
        return grads


    def gradient(self, x, t):
        self.loss(x, t)
        dout = 1
        dout = self.lastLayer.backward(dout)
        layers = list(self.layers.values())
        layers.reverse()
        for layer in layers:
            dout = layer.backward(dout)
        grads = {}
        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db
        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db
        grads['W3'], grads['b3'] = self.layers['Affine3'].dW, self.layers['Affine3'].db
        return grads
===================================================================================================





문제 187. (점심시간 문제) 어제 만든 오차역전파 3층 신경망에 SGD 경사감소를 입히고 학습시켜 보시오 !
	  191페이지의 SGD 클래스를 만들고 객체화시켜서 수행

	(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)
	network = TwoLayerNet(input_size=784, hidden_size1=50, hidden_size2=100, output_size1 = 100,  \
	output_size2=10)
	iters_num = 10000
	train_size = x_train.shape[0] # 60000 개
	batch_size = 100
	learning_rate = 0.1
	train_loss_list = []
	train_acc_list = []
	test_acc_list = []
	
	iter_per_epoch = max(train_size / batch_size, 1)
	
	optimizer = SGD()
	
	for i in range(iters_num):
	    batch_mask = np.random.choice(train_size, batch_size)
	    x_batch = x_train[batch_mask]
	    t_batch = t_train[batch_mask]
	    grad = network.gradient(x_batch, t_batch)
	    for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):
	        grads = network.gradient(x_batch, t_batch)
	        params = network.params
	        optimizer.update(params,grads)
	    loss = network.loss(x_batch, t_batch)
	    train_loss_list.append(loss)
	    if i % iter_per_epoch == 0:
	        train_acc = network.accuracy(x_train, t_train)
	        test_acc = network.accuracy(x_test, t_test)
	        train_acc_list.append(train_acc)
	        test_acc_list.append(test_acc)
	        print("train acc, test acc | " + str(train_acc) + ", " + str(test_acc))
	
	markers = {'train': 'o', 'test': 's'}
	x = np.arange(len(train_acc_list))
	plt.plot(x, train_acc_list, label='train acc')
	plt.plot(x, test_acc_list, label='test acc', linestyle='--')
	plt.xlabel("epochs")
	plt.ylabel("accuracy")
	plt.ylim(0, 1.0)
	plt.legend(loc='lower right')
	plt.show()

	train acc, test acc | 0.97295, 0.9646




문제 188. 경사감소법의 종류를 Momentum으로 변경해서 위의 3층 신경망을 학습시키고 SGD와 속도와 정확도를 
	  비교하시오 ! (p 195 페이지 참고)
	(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)
	network = TwoLayerNet(input_size=784, hidden_size1=50, hidden_size2=100, output_size1 = 100,\
	  output_size2=10)
	iters_num = 10000
	train_size = x_train.shape[0] # 60000 개
	batch_size = 100
	learning_rate = 0.1
	train_loss_list = []
	train_acc_list = []
	test_acc_list = []
	
	iter_per_epoch = max(train_size / batch_size, 1)
	
	optimizer = Momentum()
	
	for i in range(iters_num):
	    batch_mask = np.random.choice(train_size, batch_size)
	    x_batch = x_train[batch_mask]
	    t_batch = t_train[batch_mask]
	    grad = network.gradient(x_batch, t_batch)
	    for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):
	        grads = network.gradient(x_batch, t_batch)
	        params = network.params
	        optimizer.update(params,grads)
	    loss = network.loss(x_batch, t_batch)
	    train_loss_list.append(loss)
	    if i % iter_per_epoch == 0:
	        train_acc = network.accuracy(x_train, t_train)
	        test_acc = network.accuracy(x_test, t_test)
	        train_acc_list.append(train_acc)
	        test_acc_list.append(test_acc)
	        print("train acc, test acc | " + str(train_acc) + ", " + str(test_acc))
	
	markers = {'train': 'o', 'test': 's'}
	x = np.arange(len(train_acc_list))
	plt.plot(x, train_acc_list, label='train acc')
	plt.plot(x, test_acc_list, label='test acc', linestyle='--')
	plt.xlabel("epochs")
	plt.ylabel("accuracy")
	plt.ylim(0, 1.0)
	plt.legend(loc='lower right')
	plt.show()

	train acc, test acc | 0.994783333333, 0.9746




문제 189.  Adagrade 경사감소법으로 위의 3층 신경망을 학습시키시오 (p 197 Adagrade 클래스 참고)
	(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)
	network = TwoLayerNet(input_size=784, hidden_size1=50, hidden_size2=100, output_size1 = 100, \
	 output_size2=10)
	iters_num = 10000
	train_size = x_train.shape[0] # 60000 개
	batch_size = 100
	learning_rate = 0.1
	train_loss_list = []
	train_acc_list = []
	test_acc_list = []
	
	iter_per_epoch = max(train_size / batch_size, 1)
	
	optimizer = Adamgrade()
	
	for i in range(iters_num):
	    batch_mask = np.random.choice(train_size, batch_size)
	    x_batch = x_train[batch_mask]
	    t_batch = t_train[batch_mask]
	    grad = network.gradient(x_batch, t_batch)
	    for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):
	        grads = network.gradient(x_batch, t_batch)
	        params = network.params
	        optimizer.update(params,grads)
	    loss = network.loss(x_batch, t_batch)
	    train_loss_list.append(loss)
	    if i % iter_per_epoch == 0:
	        train_acc = network.accuracy(x_train, t_train)
	        test_acc = network.accuracy(x_test, t_test)
	        train_acc_list.append(train_acc)
	        test_acc_list.append(test_acc)
	        print("train acc, test acc | " + str(train_acc) + ", " + str(test_acc))
	
	markers = {'train': 'o', 'test': 's'}
	x = np.arange(len(train_acc_list))
	plt.plot(x, train_acc_list, label='train acc')
	plt.plot(x, test_acc_list, label='test acc', linestyle='--')
	plt.xlabel("epochs")
	plt.ylabel("accuracy")
	plt.ylim(0, 1.0)
	plt.legend(loc='lower right')
	plt.show()

	train acc, test acc | 0.98205, 0.9682









문제 190. Adam 경사감소법을 사용한 위의 3층 신경망을 구현하시오 !

	(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)
	network = TwoLayerNet(input_size=784, hidden_size1=50, hidden_size2=100, output_size1 = 100, \
	 output_size2=10)
	iters_num = 10000
	train_size = x_train.shape[0] # 60000 개
	batch_size = 100
	learning_rate = 0.1
	train_loss_list = []
	train_acc_list = []
	test_acc_list = []
	
	iter_per_epoch = max(train_size / batch_size, 1)
	
	optimizer = Adam()
	
	for i in range(iters_num):
	    batch_mask = np.random.choice(train_size, batch_size)
	    x_batch = x_train[batch_mask]
	    t_batch = t_train[batch_mask]
	    grad = network.gradient(x_batch, t_batch)
	    for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):
	        grads = network.gradient(x_batch, t_batch)
	        params = network.params
	        optimizer.update(params,grads)
	    loss = network.loss(x_batch, t_batch)
	    train_loss_list.append(loss)
	    if i % iter_per_epoch == 0:
	        train_acc = network.accuracy(x_train, t_train)
	        test_acc = network.accuracy(x_test, t_test)
	        train_acc_list.append(train_acc)
	        test_acc_list.append(test_acc)
	        print("train acc, test acc | " + str(train_acc) + ", " + str(test_acc))
	
	markers = {'train': 'o', 'test': 's'}
	x = np.arange(len(train_acc_list))
	plt.plot(x, train_acc_list, label='train acc')
	plt.plot(x, test_acc_list, label='test acc', linestyle='--')
	plt.xlabel("epochs")
	plt.ylabel("accuracy")
	plt.ylim(0, 1.0)
	plt.legend(loc='lower right')
	plt.show()
	
	train acc, test acc | 0.9915, 0.9725










★ 2. 가중치 초기화 값 설정
	"가중치 초기값을 적절히 설정하면 각 층의 활성화 값의 분포가 적당히 퍼지는 효과가 발생한다. 적당히
	 펴지게 되면 학습이 잘되고 정확도가 높아진다.


	1. 표준편차가 작을수록 data가 평균에 가깝게 분포
	   (시험문제 쉬우면 학생들 점수가 평균에 가깝다)
		구현 예 : 0.01 * np.random.randn(10, 100)
	
		그림 6-_-7



	2. 표준편차가 클수록 data가 더 많이 흩어져 있다.
	   (시험문제가 어려워지면 아주 잘하는 학생들과 아주 못하는 학생들로 나누어 진다.)	
		구현 예 : 1 * np.random.randn(10, 100)
	
		그림 6-_-8
	
		그림 6-13이 바람직한 가중치 초기화 값의 구성이다.
	
		바람직한 가중치 초기화값 구성을 위한 방법 2가지
	
			1. Xavier(사비에르) 초기값 선정
				표준편차가 √1/n 인 정규분포로 초기화 한다
				( n은 앞층의 노드수 )
				구현 예 : 1/np.sqrt(50) * np.random.randn(10, 100)

			2. He 초기값 선정
				표준편차가 √2/n 인 정규분포로 초기화 한다.
				( n은 앞층의 노드수 )
				구현 예 : np.sqrt(2/50) * np.random.randn(10, 100)





문제 191. 우리가 가지고 있는 3층 신경망에 가중치 초기화를 He를 써서 구현하고 정확도를 확인하시오 !

	기존 가중치 초기화값 : weight_init_std = 0.01
	변경된 가중치 초기화값 : weight_init_std = np.sqrt(2/50)
	
	class TwoLayerNet:
	    def __init__(self, input_size, hidden_size1, hidden_size2, output_size1, output_size2, \
	weight_init_std=np.sqrt(2/50)):
	        self.params = {}
	        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size1)
	        self.params['b1'] = np.zeros(hidden_size1)
	        self.params['W2'] = weight_init_std * np.random.randn(hidden_size1, output_size1)
	        self.params['b2'] = np.zeros(output_size1)
	        self.params['W3'] = weight_init_std * np.random.randn(hidden_size2, output_size2)
	        self.params['b3'] = np.zeros(output_size2)
	        self.layers = OrderedDict()
	        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])
	        self.layers['Relu1'] = Relu()
	        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])
	        self.layers['Relu2'] = Relu()
	        self.layers['Affine3'] = Affine(self.params['W3'], self.params['b3'])
	        self.lastLayer = SoftmaxWithLoss()
	
	    def predict(self, x):
	        for layer in self.layers.values():
	            x = layer.forward(x)
	        return x
	
	    def loss(self, x, t):
	        y = self.predict(x)
	        return self.lastLayer.forward(y, t)
	
	
	    def accuracy(self, x, t):
	        y = self.predict(x)
	        y = np.argmax(y, axis=1)
	        if t.ndim != 1: t = np.argmax(t, axis=1)
	        accuracy = np.sum(y == t) / float(x.shape[0])
	        return accuracy
	
	    def numerical_gradient(self, x, t):
	        loss_W = lambda W: self.loss(x, t)
	        grads = {}
	        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])
	        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])
	        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])
	        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])
	        grads['W3'] = numerical_gradient(loss_W, self.params['W3'])
	        grads['b3'] = numerical_gradient(loss_W, self.params['b3'])
	        return grads
	
	
	    def gradient(self, x, t):
	        self.loss(x, t)
	        dout = 1
	        dout = self.lastLayer.backward(dout)
	        layers = list(self.layers.values())
	        layers.reverse()
	        for layer in layers:
	            dout = layer.backward(dout)
	        grads = {}
	        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db
	        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db
	        grads['W3'], grads['b3'] = self.layers['Affine3'].dW, self.layers['Affine3'].db
	        return grads
	    
	
	(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)
	network = TwoLayerNet(input_size=784, hidden_size1=50, hidden_size2=100, output_size1 = 100, \
	 output_size2=10)
	iters_num = 10000
	train_size = x_train.shape[0] # 60000 개
	batch_size = 100
	learning_rate = 0.1
	train_loss_list = []
	train_acc_list = []
	test_acc_list = []
	
	iter_per_epoch = max(train_size / batch_size, 1)
	
	optimizer = Adam()
	
	for i in range(iters_num):
	    batch_mask = np.random.choice(train_size, batch_size)
	    x_batch = x_train[batch_mask]
	    t_batch = t_train[batch_mask]
	    grad = network.gradient(x_batch, t_batch)
	    for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):
	        grads = network.gradient(x_batch, t_batch)
	        params = network.params
	        optimizer.update(params,grads)
	    loss = network.loss(x_batch, t_batch)
	    train_loss_list.append(loss)
	    if i % iter_per_epoch == 0:
	        train_acc = network.accuracy(x_train, t_train)
	        test_acc = network.accuracy(x_test, t_test)
	        train_acc_list.append(train_acc)
	        test_acc_list.append(test_acc)
	        print("train acc, test acc | " + str(train_acc) + ", " + str(test_acc))
	
	markers = {'train': 'o', 'test': 's'}
	x = np.arange(len(train_acc_list))
	plt.plot(x, train_acc_list, label='train acc')
	plt.plot(x, test_acc_list, label='test acc', linestyle='--')
	plt.xlabel("epochs")
	plt.ylabel("accuracy")
	plt.ylim(0, 1.0)
	plt.legend(loc='lower right')
	plt.show()

	train acc, test acc | 0.989066666667, 0.973







★ 오버피팅을 억제하는 방법 2가지

	1. 드롭아웃 (dropout)
	2. 가중치 감소 (Weight Decay)



☆ 드롭아웃(dropout) (p 219)

	"오버피팅을 억제하기위해서 뉴런을 임의로 삭제하면서 학습시키는 방법"


	 * Dropout class 구현 코드  
	
		class Dropout:
		    """
		    http://arxiv.org/abs/1207.0580
		    """
		    def __init__(self, dropout_ratio=0.15):
		        self.dropout_ratio = dropout_ratio
		        self.mask = None
		
		    def forward(self, x, train_flg=True):
		        if train_flg:
		            self.mask = np.random.rand(*x.shape) > self.dropout_ratio
		            return x * self.mask
		        else:
		            return x * (1.0 - self.dropout_ratio)
		
		    def backward(self, dout):
		        return dout * self.mask
	
	   * dropout 클래스의 키가 되는 부분 
	
	   self.mask = np.random.rand(*x.shape) > self.dropout_ratio
	                                                    ↓
	                                                   0.15  
		100% 중에 85%의 노드만 남겨두고 15%노드를 삭제하겠다.
		사진의 일부를 없애면서 학습을 시킨다.
		너무 많이 삭제하면 무슨 문제가 생길까?

			"정확도가 떨어진다."

--------------------------------------------------------------------------------------------
class TwoLayerNet:
    def __init__(self, input_size, hidden_size1, hidden_size2, output_size1, output_size2, \
                 weight_init_std=np.sqrt(2/50)):
        self.params = {}
        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size1)
        self.params['b1'] = np.zeros(hidden_size1)
        self.params['W2'] = weight_init_std * np.random.randn(hidden_size1, output_size1)
        self.params['b2'] = np.zeros(output_size1)
        self.params['W3'] = weight_init_std * np.random.randn(hidden_size2, output_size2)
        self.params['b3'] = np.zeros(output_size2)
        self.layers = OrderedDict()
        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])
        self.layers['Relu1'] = Relu()
        self.layers['dropout1'] = Dropout()
        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])
        self.layers['Relu2'] = Relu()
        self.layers['dropout1'] = Dropout()
        self.layers['Affine3'] = Affine(self.params['W3'], self.params['b3'])
        self.lastLayer = SoftmaxWithLoss()

    def predict(self, x):
        for layer in self.layers.values():
            x = layer.forward(x)
        return x

    def loss(self, x, t):
        y = self.predict(x)
        return self.lastLayer.forward(y, t)


    def accuracy(self, x, t):
        y = self.predict(x)
        y = np.argmax(y, axis=1)
        if t.ndim != 1: t = np.argmax(t, axis=1)
        accuracy = np.sum(y == t) / float(x.shape[0])
        return accuracy

    def numerical_gradient(self, x, t):
        loss_W = lambda W: self.loss(x, t)
        grads = {}
        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])
        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])
        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])
        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])
        grads['W3'] = numerical_gradient(loss_W, self.params['W3'])
        grads['b3'] = numerical_gradient(loss_W, self.params['b3'])
        return grads


    def gradient(self, x, t):
        self.loss(x, t)
        dout = 1
        dout = self.lastLayer.backward(dout)
        layers = list(self.layers.values())
        layers.reverse()
        for layer in layers:
            dout = layer.backward(dout)
        grads = {}
        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db
        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db
        grads['W3'], grads['b3'] = self.layers['Affine3'].dW, self.layers['Affine3'].db
        return grads
    

(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)
network = TwoLayerNet(input_size=784, hidden_size1=50, hidden_size2=100, output_size1 = 100, \
 output_size2=10)
iters_num = 10000
train_size = x_train.shape[0] # 60000 개
batch_size = 100
learning_rate = 0.1
train_loss_list = []
train_acc_list = []
test_acc_list = []

iter_per_epoch = max(train_size / batch_size, 1)

optimizer = Adam()

for i in range(iters_num):
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]
    grad = network.gradient(x_batch, t_batch)
    for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):
        grads = network.gradient(x_batch, t_batch)
        params = network.params
        optimizer.update(params,grads)
    loss = network.loss(x_batch, t_batch)
    train_loss_list.append(loss)
    if i % iter_per_epoch == 0:
        train_acc = network.accuracy(x_train, t_train)
        test_acc = network.accuracy(x_test, t_test)
        train_acc_list.append(train_acc)
        test_acc_list.append(test_acc)
        print("train acc, test acc | " + str(train_acc) + ", " + str(test_acc))

markers = {'train': 'o', 'test': 's'}
x = np.arange(len(train_acc_list))
plt.plot(x, train_acc_list, label='train acc')
plt.plot(x, test_acc_list, label='test acc', linestyle='--')
plt.xlabel("epochs")
plt.ylabel("accuracy")
plt.ylim(0, 1.0)
plt.legend(loc='lower right')
plt.show()

train acc, test acc | 0.985533333333, 0.9654
--------------------------------------------------------------------------------------------



	2. 가중치 감소 (Weight Decay)

		"학습 과정에서 큰 가중치에 대해서는 그에 상응하는 큰 패널티를 부여하여 오버피팅을
		 억제하는 방법"

	   예 : 고양이와 개사진을 구분하는 신경망

		입력층			은닉층
		  ○  ──────▶  귀가 있어요   	     ──────▶  가중치 출력
	고양이    ○  ──────▶  꼬리가 있어요 	     ──────▶  아주 큰 가중치 출력
		  ○  ──────▶  집게발을 가지고 있어요 ──────▶  가중치 출력
		  ○  ──────▶  장난스럽게 보여요      ──────▶  가중치 출력

		cf) 위와 같이 학습이 되면 꼬리가 없는 고양이 사진을 입력하면 고양이가 아니라고 판단을
		    한다.
			↓
			그래서 이를 해결하는 방법이 ?

		큰 패널티는 오차함수가 준다. 오차함수가 오차를 역전파 시킬 때 패널티를 부여하게끔 
		하는데 오차함수 코드가 아래와 같다.






☆ 가중치 decay 코드 분석 
                                    1
	 "모든 가중치의 각각의 손실 함수에 --- x 람다 x w^2  을 더한다. 
	                                    2
	 
	 1. 오차 함수의 코드에서 수정해야 할 부분 
	
	  weight_decay += 0.5 * self.weight_decay_lambda * np.sum(W**2)
	
	  return  self.lastLayer.forward(y,t) + wight_decay  
	                      ↓
	                     오차 
	 
	 2. gradient  Descent 업데이트 과정에서 그동안 오차 역전파법에
	    따른 결과에 정규화 항을 미분한 값에 람다 * 가중치를 더한다. 
	
	 코드예제:
	
	           기존 코드  : grad['W1'] = 미분값 
	         변경된 코드  : grad['W1'] = 미분값 + 람다 * 현재의 가중치




문제 192. 우리가 가지고 있는 신경망에 가중치 decay를 적용해서 오버피팅이 덜 발생하는지 확인하시오 !

	class TwoLayerNet:
	    def __init__(self, input_size, hidden_size1, hidden_size2, output_size1, output_size2, \
	                 weight_init_std=np.sqrt(2/50)):
	        self.params = {}
	        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size1)
	        self.params['b1'] = np.zeros(hidden_size1)
	        self.params['W2'] = weight_init_std * np.random.randn(hidden_size1, output_size1)
	        self.params['b2'] = np.zeros(output_size1)
	        self.params['W3'] = weight_init_std * np.random.randn(hidden_size2, output_size2)
	        self.params['b3'] = np.zeros(output_size2)
	        self.layers = OrderedDict()
	        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])
	        self.layers['Relu1'] = Relu()
	        self.layers['dropout1'] = Dropout()
	        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])
	        self.layers['Relu2'] = Relu()
	        self.layers['dropout1'] = Dropout()
	        self.layers['Affine3'] = Affine(self.params['W3'], self.params['b3'])
	        self.lastLayer = SoftmaxWithLoss()
	        self.weight_decay_lambda = 0.001
	
	    def predict(self, x):
	        for layer in self.layers.values():
	            x = layer.forward(x)
	        return x
	
	    def loss(self, x, t):
	        y = self.predict(x)
	        weight_decay = 0
	        for idx in range(1, 4):
	            W = self.params['W' + str(idx)]
	            weight_decay += 0.5 * self.weight_decay_lambda * np.sum(W ** 2)
		return self.lastLayer.forward(y, t) + weight_decay 
	

	    def accuracy(self, x, t):
	        y = self.predict(x)
	        y = np.argmax(y, axis=1)
	        if t.ndim != 1: t = np.argmax(t, axis=1)
	        accuracy = np.sum(y == t) / float(x.shape[0])
	        return accuracy
	
	    def numerical_gradient(self, x, t):
	        loss_W = lambda W: self.loss(x, t)
	        grads = {}
	        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])
	        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])
	        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])
	        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])
	        grads['W3'] = numerical_gradient(loss_W, self.params['W3'])
	        grads['b3'] = numerical_gradient(loss_W, self.params['b3'])
	        return grads
	
	
	    def gradient(self, x, t):
	        self.loss(x, t)
	        dout = 1
	        dout = self.lastLayer.backward(dout)
	        layers = list(self.layers.values())
	        layers.reverse()
	        for layer in layers:
	            dout = layer.backward(dout)
	        grads = {}
	        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db
	        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db
	        grads['W3'], grads['b3'] = self.layers['Affine3'].dW, self.layers['Affine3'].db
	        return grads
	    
	
	(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)
	network = TwoLayerNet(input_size=784, hidden_size1=50, hidden_size2=100, output_size1 = 100, \
	 output_size2=10)
	iters_num = 10000
	train_size = x_train.shape[0] # 60000 개
	batch_size = 100
	learning_rate = 0.1
	train_loss_list = []
	train_acc_list = []
	test_acc_list = []
	
	iter_per_epoch = max(train_size / batch_size, 1)
	
	optimizer = Adam()
	
	for i in range(iters_num):
	    batch_mask = np.random.choice(train_size, batch_size)
	    x_batch = x_train[batch_mask]
	    t_batch = t_train[batch_mask]
	    grad = network.gradient(x_batch, t_batch)
	    for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):
	        grads = network.gradient(x_batch, t_batch)
	        params = network.params
	        optimizer.update(params,grads)
	    loss = network.loss(x_batch, t_batch)
	    train_loss_list.append(loss)
	    if i % iter_per_epoch == 0:
	        train_acc = network.accuracy(x_train, t_train)
	        test_acc = network.accuracy(x_test, t_test)
	        train_acc_list.append(train_acc)
	        test_acc_list.append(test_acc)
	        print("train acc, test acc | " + str(train_acc) + ", " + str(test_acc))
	
	markers = {'train': 'o', 'test': 's'}
	x = np.arange(len(train_acc_list))
	plt.plot(x, train_acc_list, label='train acc')
	plt.plot(x, test_acc_list, label='test acc', linestyle='--')
	plt.xlabel("epochs")
	plt.ylabel("accuracy")
	plt.ylim(0, 1.0)
	plt.legend(loc='lower right')
	plt.show()
	
	train acc, test acc | 0.982716666667, 0.9649
