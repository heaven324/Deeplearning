3장 까지의 전체 코드 :

	import sys, os
	
	sys.path.append(os.pardir)
	
	import numpy as np
	from dataset.mnist import load_mnist
	from PIL import Image
	import  pickle
	import  numpy  as np
	
	
	# 신경망 함수들
	def sigmoid(num):
	    rst = (1 / (1 + np.exp(-num)))
	    return (rst)
	
	
	def identity_function(x):
	    return x
	
	
	def softmax(a):
	    c = np.max(a)
	    minus = a - c
	    exp_a = np.exp(minus)
	    sum_exp_a = np.sum(exp_a)
	    y = exp_a / sum_exp_a
	    return y
	
	def  init_network():
	    with open("sample_weight.pkl",'rb')  as  f:
	        network = pickle.load(f)
	    return  network
	
	
	def  predict(network, x):
	    W1, W2, W3 = network['W1'], network['W2'], network['W3']
	    b1, b2, b3 = network['b1'], network['b2'], network['b3']
	
	    a1 = np.dot(x,W1) + b1
	    z1 = sigmoid(a1)
	    a2 = np.dot(z1,W2) + b2
	    z2 = sigmoid(a2)
	    a3 = np.dot(z2,W3) + b3
	    y = softmax(a3)
	
	    return  y
	
	def  get_data():
	    (x_train, t_train) , (x_test, t_test) = \
	    load_mnist(normalize=True, flatten=True, one_hot_label=False)
	    return  x_test, t_test
	
	x, t = get_data()
	network = init_network()
	t_count = 0
	batch_size = 100
	cnt = 0
	for i in range(0,len(x), batch_size):
	    x_batch = x[i:i+batch_size]
	    y = predict(network,x_batch)
	    cnt += sum(np.argmax(y, axis=1) == t[i:i+batch_size])
	print('정확도 :', cnt/len(x))
	

■ 4장. 신경망 학습

	앞장에서는 저자가 만들어온 가중치값을 가지고 신경망을 구성했기 때문에 신경망을 학습시킬 필요가
	없었다. 

		필기체 7   ──────▶ 신경망  ──────▶ 예측값 == 실제값
					    ↑			   ↓
			      저자가 만들어온 가중치값 셋팅	  정확도

	4장에서는 우리가 직접 신경망을 학습을 시킬 것인데 학습을 시키기 위해서 알아야 할 내용 ?

		1. 오차함수
		2. 가중치를 갱신하는 방법 ( 수치 미분 ) 
		3. 미니배치 (mini batch)




★ 오차 함수

	"예상값과 실제값과의 오차를 신경망에 역전파 시켜주기 위해서 필요한 함수"

	오차가 최소화될 때 까지 신경망을 학습시키기 위해서 필요한 함수

		1. 평균제곱 오차함수      : 회귀분석을 할 때 사용
		2. 교차 엔트로피 오차함수 : 분류문제를 풀 때 사용

					 	 출력층 함수  	,     오차함수 
		회귀 문제를 해결할 때 ───▶     항등함수   	, 평균제곱 오차함수
		분류 문제를 해결할 때 ───▶ 소프트맥스 함수	, 교차 엔트로피 함수







★ 1. 평균제곱 오차 함수 (p 112)

	그림 4-_-1




문제 107. 그림 4-_-1 식을 가지고 평균제곱 오차함수를 파이썬으로 구현하시오 !

	import numpy as np
	
	def mean_squared_error(x,t):
	    return 0.5*np.sum((x-t)**2)
	
	y = np.array([0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0])
	t = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])
	
	print(mean_squared_error(y,t))
	
	0.0975 # 이 오차를 신경망으로 역전파 시켜서 가중치(W)를 갱신을 한다.




문제 108. 아래의 확률벡터를 평균제곱 오차 함수를 이용해서 target(실제값)과 예측값의 오차율이 어떻게 되는지 
	  for loop문으로 한번에 알아내시오 !

	import numpy as np
	
	def mean_squared_error(x,t):
	    return 0.5*np.sum((x-t)**2)
	
	t = [0,0,1,0,0,0,0,0,0,0]
	y1 = [0.1,0.05,0.1,0.0,0.05,0.1,0.0,0.1,0.0,0.0]  
	y2 = [0.1,0.05,0.2,0.0,0.05,0.1,0.0,0.6,0.0,0.0] 
	y3 = [0.0,0.05,0.3,0.0,0.05,0.1,0.0,0.6,0.0,0.0] 
	y4 = [0.0,0.05,0.4,0.0,0.05,0.0,0.0,0.5,0.0,0.0] 
	y5 = [0.0,0.05,0.5,0.0,0.05,0.0,0.0,0.4,0.0,0.0] 
	y6 = [0.0,0.05,0.6,0.0,0.05,0.0,0.0,0.3,0.0,0.0] 
	y7 = [0.0,0.05,0.7,0.0,0.05,0.0,0.0,0.2,0.0,0.0] 
	y8 = [0.0,0.1,0.8,0.0,0.1,0.0,0.0,0.2,0.0,0.0] 
	y9 = [0.0,0.05,0.9,0.0,0.05,0.0,0.0,0.0,0.0,0.0] 
	
	
	for i in range(1, 10):
	    print('y%d의 오차는 :'%i,mean_squared_error(np.array(eval('y'+str(i))), np.array(t)))

	y1의 오차율 : 0.4225
	y2의 오차율 : 0.5125
	y3의 오차율 : 0.4325
	y4의 오차율 : 0.3075
	y5의 오차율 : 0.2075
	y6의 오차율 : 0.1275
	y7의 오차율 : 0.0675
	y8의 오차율 : 0.05
	y9의 오차율 : 0.0075




★ 2. 교차 엔트로피 오차함수

	그림 4-_-2




문제 109. 확률 0.1과 확률 0.9를 교차 엔트로피 공식에 대입한 결과가 각각 어떻게 되는지 확인하시오 !

	import numpy as np
	
	print( -np.log(0.1), -np.log(0.9))

	2.30258509299 0.105360515658



문제 110. 교차 엔트로피 함수를 생성하시오 !

	import numpy as np
	
	def cross_entropy_error(x,t):
	    delta = 1e-7
	    return -np.sum(t*np.log(x+delta))
	
	t = [0,0,1,0,0,0,0,0,0,0]
	y1 = [0.1,0.05,0.1,0.0,0.05,0.1,0.0,0.1,0.0,0.0] 
	y9 = [0.0,0.05,0.9,0.0,0.05,0.0,0.0,0.0,0.0,0.0]
	
	print(cross_entropy_error(np.array(y1), np.array(t)))
	print(cross_entropy_error(np.array(y9), np.array(t)))
	
	2.30258409299
	0.105360404547



문제 111. 평균제곱 오차함수와 교차엔트로피 오차함수중에 어떤것이 더 큰 오차를 신경망에 역전파 
	  시켜주겠는지 테스트 해 보시오 !

	import numpy as np
	
	def mean_squared_error(x,t):
	    return 0.5*np.sum((x-t)**2)
	
	def cross_entropy_error(x,t):
	    delta = 1e-7
	    return -np.sum(t*np.log(x+delta))
	
	t = [0,0,1,0,0,0,0,0,0,0]
	y1 = [0.1,0.05,0.1,0.0,0.05,0.1,0.0,0.1,0.0,0.0]  
	y2 = [0.1,0.05,0.2,0.0,0.05,0.1,0.0,0.6,0.0,0.0] 
	y3 = [0.0,0.05,0.3,0.0,0.05,0.1,0.0,0.6,0.0,0.0] 
	y4 = [0.0,0.05,0.4,0.0,0.05,0.0,0.0,0.5,0.0,0.0] 
	y5 = [0.0,0.05,0.5,0.0,0.05,0.0,0.0,0.4,0.0,0.0] 
	y6 = [0.0,0.05,0.6,0.0,0.05,0.0,0.0,0.3,0.0,0.0] 
	y7 = [0.0,0.05,0.7,0.0,0.05,0.0,0.0,0.2,0.0,0.0] 
	y8 = [0.0,0.1 ,0.8,0.0,0.1 ,0.0,0.0,0.2,0.0,0.0] 
	y9 = [0.0,0.05,0.9,0.0,0.05,0.0,0.0,0.0,0.0,0.0]
	
	for i in range(1, 10):
	    print('y%d의 오차율(평제) :'%i,mean_squared_error(np.array(eval('y'+str(i))), np.array(t)),end = '    ')
	    print('y%d의 오차율(엔트) :'%i,cross_entropy_error(np.array(eval('y'+str(i))), np.array(t)),end = '\n')
	
	y1의 오차율(평제) : 0.4225    y1의 오차율(엔트) : 2.30258409299
	y2의 오차율(평제) : 0.5125    y2의 오차율(엔트) : 1.60943741243
	y3의 오차율(평제) : 0.4325    y3의 오차율(엔트) : 1.20397247099
	y4의 오차율(평제) : 0.3075    y4의 오차율(엔트) : 0.916290481874
	y5의 오차율(평제) : 0.2075    y5의 오차율(엔트) : 0.69314698056
	y6의 오차율(평제) : 0.1275    y6의 오차율(엔트) : 0.510825457099
	y7의 오차율(평제) : 0.0675    y7의 오차율(엔트) : 0.356674801082
	y8의 오차율(평제) : 0.05      y8의 오차율(엔트) : 0.223143426314
	y9의 오차율(평제) : 0.0075    y9의 오차율(엔트) : 0.105360404547
	