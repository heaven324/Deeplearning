★ 배치 정규화 (p210)

	" batch normalization "
	  batch 단위로 가중치의 값을 정규화 하겠다.



    통계 분석에서 중요한 2가지

	1. 표준화 : 평균을 기준으로 얼마나 떨어져 있는지 나타내는 값
		    2개 이상의 대상이 단위가 다를때 대상 데이터를 같은 기준으로 볼 수 있게 해준다.

	예: 키와 몸무게 -> 키와 몸무게는 단위가 다른데 우리반 학생들의 키를 0~1
			   사이의 숫자로 변경하고 몸무게도 0~1사이의 숫자로 변경하면
			   키와 몸무게 간의 어떤 관계가 있는지 분석을 해볼 수 있다.

		177(키)		84(몸무게)

	수식 : (요소값 - 평균) / 표준편차



2. 정규화(Normalization)

	  정규화는 전체 구간을 0~100 으로 설정하여 데이터를 관찰하는 방법이다.
	
	  이 방법은 데이터 군에서 특정 데이터가 가지는 위치를 볼 때 사용한다.
	
	  예 : 우리반에서 내 몸무게 77 kg 이면 0~100 사이에 몇에 해당 되는지 파악이 된다.

	  수식 : (요소값 - 최소값) / (최대값 - 최소값)




문제 193. 우리반의 이름과 나이 데이터를 이용해서 표준화를 시도해서 나의 나이가 0~1 사이중 어느 부분에 
	  해당 되는지 파이썬으로 확인하시오 !

		(요소값 - 평균) / 표준편차

	# -*- coding: utf-8 -*-
	
	import csv
	import  numpy  as np
	
	file = open('D:\\emp81.csv','r', encoding='UTF-8')
	emp2_csv = csv.reader(file)
	x2 =[]
	for emp2_list in emp2_csv:
	    x2.append(int(emp2_list[1]))
	    x3 = np.array(x2)
	
	print(x3)
	print( (x3-x3.mean())**2 / x3.std())
	




문제 194. 우리반 나이 데이터를 가지고 정규화(normalization) 을 해서 나이를 0 ~ 100 사이의 숫자로 
	  나타내시오 !
	
	수식 : (요소 - 최소값) / (최대값 - 최소값)
	
	# -*- coding: utf-8 -*-
	
	import csv
	import  numpy  as np
	
	file = open('D:\\emp81.csv','r', encoding='UTF-8')
	emp2_csv = csv.reader(file)
	x2 =[]
	for emp2_list in emp2_csv:
	    x2.append(int(emp2_list[1]))
	    x3 = np.array(x2)
	
	print( ( x3 -  x3.min()) / (x3.max() - x3.min()))









★ 배치 정규화란 ?

	앞에서는 가중치 초기화 값을 적절히 설정하면 각 층의 활성화 값의 분포가 적당히 퍼지는 효과를 보았다.

	 그림 6-_-9

	아주 공부를 잘하거나 아주 공부를 못하는 학생들이 있으면 별로 바람직하지 못해서 신경망 학습이 
	잘 안된다.

	그림 6-_-10

	또 평균에 다 몰려있어도 시험문제가 너무 쉬운 것이기 때문에 바람직 하지 못하다.

	그림 6-_-11

	그래서 가중치 초기화 할때 Xavier 나 He 를 써서 가중치 값들을 0~1 사이의 숫자로 적절히 분포시키는 
	작업을 했는데
	그런데 문제는 처음에는 데이터가 적절히 분포가 되었다가 점점 학습이 진행되면서 다시 데이터의 분포가 
	평균에 몰리거나 아니면 아주 잘하거나 아주 못하는 현상이 발생하게 된다.

	즉 학습이 진행될수록 분포가 틀어지는 현상이 생긴다.

	그래서 배치 정규화를 통해서 각 층의 활성화 값이 적당히 분포되도록 강제로 조정할 필요가 있는데 
	그게 배치 정규화이다.

	배치 정규화를 신경망에 적절히 삽입해주면 된다.
	배치 정규화는 활성화 함수 앞이나 뒤쪽에 입력할 수 있지만 대개는 활성화 함수 앞에 사용한다.








★ 배치 정규화 공식

p 211. 그림 6-_-12

	배치 정규화는 기본적으로 데이터 분포가 평균 0, 분산이 1 이 되도록 정규화한다.
	사이즈가 m 인 미니배치에 대하여 평균과 분산을 구한다.
	그리고 이 입력 데이터를 평균이 0, 분산이 1이 되도록 정규화를 한다.









★ 배치 정규화의 장점 (p 210 )

	1. 학습을 빨리 진행할 수 있다.		---> underfitting 해결
	2. 초기값에 크게 의존하지 않아도 된다. ---> underfitting 해결
	3. 오버피팅을 억제한다. ---> overfitting 해결

	이 입력 데이터를 평균이 0, 분산이 1 이 되도록 정규화를 하는데 그런데 신경망은 비선형을 가지고 
	있어야 표현력이 커져서 복잡한 함수를 표현할 수 있는데 위와 같이 평균이 0, 분산을 1로 고정시키는 
	것은 활성화 함수의 비선형을 없애버릴 수 있다.

	그래서 아래와 같이 감마와 베타를 이용해서 새로운 값을 내놓는다.

	그림 6-_-13

	감마가 데이터의 확대(분포) 를 조정하는 것이고 베타가 데이터의 이동을 조정하는 것인데 학습이 
	되면서 감마와 베타가 자동으로 신경망에 맞게 조정이 된다.






★ 배치 정규화의 계산 그래프

	μ(평균)
	σ(분산)

그림 6-_-14




문제 195. 위의 배치 정규화 계산 그래프 forward 함수를 생성하시오 !

	def batch_norm_forward(x, gamma = 1, beta = 0, eps):
	
	코드 구현
	
	print( batch_norm_forward(x,1,0,0.01) )







★ 배치 정규화의 역전파 

https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html
참고





문제 196. 아래의 코드를 이용해서 우리가 가지고 있는 3층 신경망에 배치 정규화 코드를 추가하시오 !

	class TwoLayerNet:
	    def __init__(self, input_size, hidden_size1, hidden_size2, output_size1, output_size2, \
	                 weight_init_std=np.sqrt(2/50)):
	        self.params = {}
	        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size1)
	        self.params['b1'] = np.zeros(hidden_size1)
	        self.params['W2'] = weight_init_std * np.random.randn(hidden_size1, output_size1)
	        self.params['b2'] = np.zeros(output_size1)
	        self.params['W3'] = weight_init_std * np.random.randn(hidden_size2, output_size2)
	        self.params['b3'] = np.zeros(output_size2)
	        self.layers = OrderedDict()
	        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])
	        self.layers['BatchNorm1'] = BatchNormalization(gamma=1.0, beta=0.)
	        self.layers['Relu1'] = Relu()
	#         self.layers['dropout1'] = Dropout()
	        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])
	        self.layers['BatchNorm2'] = BatchNormalization(gamma=1.0, beta=0.)
	        self.layers['Relu2'] = Relu()
	#         self.layers['dropout1'] = Dropout()
	        self.layers['Affine3'] = Affine(self.params['W3'], self.params['b3'])
	        self.lastLayer = SoftmaxWithLoss()
	        self.weight_decay_lambda = 0.001
	
	    def predict(self, x):
	        for layer in self.layers.values():
	            x = layer.forward(x)
	        return x
	
	    def loss(self, x, t):
	        y = self.predict(x)
	        weight_decay = 0
	        for idx in range(1, 4):
	             W = self.params['W' + str(idx)]
	             weight_decay += 0.5 * self.weight_decay_lambda * np.sum(W ** 2)
	        return self.lastLayer.forward(y, t) + weight_decay 
	
	
	    def accuracy(self, x, t):
	        y = self.predict(x)
	        y = np.argmax(y, axis=1)
	        if t.ndim != 1: t = np.argmax(t, axis=1)
	        accuracy = np.sum(y == t) / float(x.shape[0])
	        return accuracy
	
	    def numerical_gradient(self, x, t):
	        loss_W = lambda W: self.loss(x, t)
	        grads = {}
	        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])
	        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])
	        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])
	        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])
	        grads['W3'] = numerical_gradient(loss_W, self.params['W3'])
	        grads['b3'] = numerical_gradient(loss_W, self.params['b3'])
	        return grads
	
	
	    def gradient(self, x, t):
	        self.loss(x, t)
	        dout = 1
	        dout = self.lastLayer.backward(dout)
	        layers = list(self.layers.values())
	        layers.reverse()
	        for layer in layers:
	            dout = layer.backward(dout)
	        grads = {}
	        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db
	        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db
	        grads['W3'], grads['b3'] = self.layers['Affine3'].dW, self.layers['Affine3'].db
	        return grads
	
	(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)
	network = TwoLayerNet(input_size=784, hidden_size1=50, hidden_size2=100, output_size1 = 100, \
	 output_size2=10)
	iters_num = 10000
	train_size = x_train.shape[0] # 60000 개
	batch_size = 100
	learning_rate = 0.1
	train_loss_list = []
	train_acc_list = []
	test_acc_list = []
	
	iter_per_epoch = max(train_size / batch_size, 1)
	
	optimizer = Adam()
	
	for i in range(iters_num):
	    batch_mask = np.random.choice(train_size, batch_size)
	    x_batch = x_train[batch_mask]
	    t_batch = t_train[batch_mask]
	    grad = network.gradient(x_batch, t_batch)
	    for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):
	        grads = network.gradient(x_batch, t_batch)
	        params = network.params
	        optimizer.update(params,grads)
	    loss = network.loss(x_batch, t_batch)
	    train_loss_list.append(loss)
	    if i % iter_per_epoch == 0:
	        train_acc = network.accuracy(x_train, t_train)
	        test_acc = network.accuracy(x_test, t_test)
	        train_acc_list.append(train_acc)
	        test_acc_list.append(test_acc)
	        print("train acc, test acc | " + str(train_acc) + ", " + str(test_acc))
	
	markers = {'train': 'o', 'test': 's'}
	x = np.arange(len(train_acc_list))
	plt.plot(x, train_acc_list, label='train acc')
	plt.plot(x, test_acc_list, label='test acc', linestyle='--')
	plt.xlabel("epochs")
	plt.ylabel("accuracy")
	plt.ylim(0, 1.0)
	plt.legend(loc='lower right')
	plt.show()













■ 7장. CNN (Convolution Neural Network)
	         합성      곱    신경망

	합성 곱 신경망 ?

		" Convolution층과 pooling층을 포함하는 신경망 "


	기존 신경망과의 차이 ?
		- 기존 방법 : Affine ──▶ Relu
		- CNN 	    : Conv   ──▶ Relu ──▶ Pooling ──▶ 완전 연결 계층






★ 기존에 구현했던 완전 연결 계층의 문제점

	"데이터의 형상이 무시된다"

	그림으로 설명 (카페 확인 : http://cafe.daum.net/oracleoracle/SY3L/175 )


	* CNN을 이용하지 않은 기존 층의 문제점
		필기체 ──▶ 28 x 28 = 784의 1차원 데이터로 변경해서 784개의 데이터를 첫 Affine계층에
			      입력한게 기존 방법이다.
					↓
			형상을 무시하고 무든 입력 데이터를 동등한 뉴런으로 취급하기 때문에 이미지가 갖는
			본질적인 패턴을 읽지 못한다.
					↓
			그래서 합성곱 계층이 필요하다.

		결국 원본 이미지에서 조금만 모양이 달라져도 같은 이미지로 인식하지 못하는 문제를 합성곱이
		해결해 줄 수 있다.

		어떻게 해결하는가?	원본이미지를 가지고 여러개의 feature map을 만들어서 완전연결계층에
					입력해서 분류를 한다.



	* 합성곱 예제 :
		  0  2  0  2  0
		  2  0  2  0  2        1  1  1
		  2  0  0  0  2    ◎  0  0  0
		  2  0  0  0  2        0  0  0  
		  0  2  2  2  0 
		  0  0  2  0  0 
		
		   하트 이미지        필터

	* 합성곱 연산을 컴퓨터로 구현하는 방법

		1 2 3 0
		0 1 2 3	     ◎    2 0 1    =   15  16
		3 0 1 2            0 1 2         6  15
		2 3 0 1            1 0 2

		입력 데이터 	   필터 	 결과
		(4, 4) 		   (3, 3) 	 (2, 2)

		순서 1 : 
			1 2 3        2 0 1		2 0 3
			0 1 2	◎   0 1 2	=       0 1 4    =   15
			3 0 1	     1 0 2		3 0 2
	
		순서 2 :
			2 3 0        2 0 1		4 0 0
			1 2 3	◎   0 1 2	=       0 2 6    =   16
			0 1 2	     1 0 2		0 0 4
	
		순서 3 : 
			0 1 2        2 0 1		0 0 2
			3 0 1	◎   0 1 2	=       0 0 2    =   6
			2 3 0	     1 0 2		2 0 0
	
		순서 4 :
			1 2 3        2 0 1		2 0 3
			0 1 2	◎   0 1 2	=       0 1 4    =   15
			3 0 1	     1 0 2		3 0 2




문제 197. 아래의 두 행렬을 만들고 합성곱 한 결과인 15을 파이썬으로 출력하시오 !
	 1 2 3        2 0 1        2 0 3
	 0 1 2   ◎   0 1 2   =    0 1 4    =   15
	 3 0 1        1 0 2        3 0 2
	
	import numpy as np
	
	x = np.array([[1, 2, 3], [0, 1, 2], [3, 0, 1]])
	y = np.array([[2, 0, 1], [0, 1, 2], [1, 0, 2]])

	print(np.sum(x*y))
	15




문제 198. 아래의 4 x 4 행렬에서 아래의 3x3행렬만 추출하시오 !

	 1 2 3 0
	 0 1 2 3  ------->  1 2 3
	 3 0 1 2            0 1 2
	 2 3 0 1            3 0 1
	
	import numpy as np
	
	x = np.array([[1, 2, 3, 0], [0, 1, 2, 3], [3, 0, 1, 2], [2, 3, 0, 1]])
	print(x[0:3,0:3])

	[[1 2 3]
	 [0 1 2]
	 [3 0 1]]




문제 199. 아래의 행렬에서 아래의 결과 행렬만 추출하시오 !

	 1 2 3 0
	 0 1 2 3  ------->  2 3 0
	 3 0 1 2            1 2 3
	 2 3 0 1            0 1 2
	
	import numpy as np
	x = np.array([[1, 2, 3, 0], [0, 1, 2, 3], [3, 0, 1, 2], [2, 3, 0, 1]])
	y = np.array([[2, 0, 1], [0, 1, 2], [1, 0, 2]])
	
	print(x[0:y.shape[0],1:y.shape[1]+1])




문제 200. 아래의 행렬에서 for loop문을 이용해서 아래의 결과를 출력하시오 !

	import numpy as np
	x = np.array([[1, 2, 3, 0], [0, 1, 2, 3], [3, 0, 1, 2], [2, 3, 0, 1]])
	y = np.array([[2, 0, 1], [0, 1, 2], [1, 0, 2]])
	
	for i in range(x.shape[0]-y.shape[0]+1):
	    for j in range(x.shape[1] - y.shape[1]+1):
	        print(x[i:y.shape[0]+i,j:y.shape[1]+j])
	        print('-----------')
	
	[[1 2 3]
	 [0 1 2]
	 [3 0 1]]
	-----------
	[[2 3 0]
	 [1 2 3]
	 [0 1 2]]
	-----------
	[[0 1 2]
	 [3 0 1]
	 [2 3 0]]
	-----------
	[[1 2 3]
	 [0 1 2]
	 [3 0 1]]
	-----------




문제 201. 아래의 합성곱을 파이썬으로 구현하시오 !

	 1 2 3 0
	 0 1 2 3      ◎    2 0 1    =   15  16
	 3 0 1 2            0 1 2         6  15
	 2 3 0 1            1 0 2
	
	import numpy as np
	x = np.array([[1, 2, 3, 0], [0, 1, 2, 3], [3, 0, 1, 2], [2, 3, 0, 1]])
	y = np.array([[2, 0, 1], [0, 1, 2], [1, 0, 2]])
	
	gop = []
	for i in range(x.shape[0]-y.shape[0]+1):
	    for j in range(x.shape[1] - y.shape[1]+1):
	        gop.append(np.sum(x[i:y.shape[0]+i,j:y.shape[1]+j]*y))
	print(gop)

	[15, 16, 6, 15]




문제 202. 위에서 출력한 1차원 배열인 [15, 16, 6, 15]결과를 2x2행렬로 변경하시오 !

	import numpy as np
	x = np.array([[1, 2, 3, 0], [0, 1, 2, 3], [3, 0, 1, 2], [2, 3, 0, 1]])
	y = np.array([[2, 0, 1], [0, 1, 2], [1, 0, 2]])
	
	gop = []
	for i in range(x.shape[0]-y.shape[0]+1):
	    for j in range(x.shape[1] - y.shape[1]+1):
	        gop.append(np.sum(x[i:y.shape[0]+i,j:y.shape[1]+j]*y))
	print(np.array(gop).reshape(i+1,j+1))

	[[15 16]
	 [ 6 15]]




문제 203. 아래의 그림의 convolution연산을 파이썬으로 구현하시오 !
	 그림 7-_-3
	
	import numpy as np
	x = np.array([[1, 2, 3, 0], [0, 1, 2, 3], [3, 0, 1, 2], [2, 3, 0, 1]])
	y = np.array([[2, 0, 1], [0, 1, 2], [1, 0, 2]])
	
	gop = []
	for i in range(x.shape[0]-y.shape[0]+1):
	    for j in range(x.shape[1] - y.shape[1]+1):
	        gop.append(np.sum(x[i:y.shape[0]+i,j:y.shape[1]+j]*y))
	
	res = np.array(gop).reshape(i+1,j+1)
	print(res+3)
	
	[[18 19]
	 [ 9 18]]






★ Padding ( p 232 )

	그림
	
	"합성곱 연산을 수행하기 전에 입력 데이터 주변을 특정 값으로 채워 늘리는 것을 말한다."

	- 패딩이 필요한 이유 ?
		패딩을 하지 않을 경우 data의 공간크기는 합성곱 계층이 지날때 마다 작아지게 되므로 가장
		자리 정보들이 사라지게 되는 문제가 발생하기 때문에 패딩을 사용한다.

		4x4  ◎ 3x3 = 2x2

		conv ──▶ polling ──▶ conv ──▶ polling




문제 204. 문제 255번에서 출력한 아래의 2x2행렬에 제로패딩 1을 수행하시오 !

	import numpy as np
	x = np.array([[1, 2, 3, 0], [0, 1, 2, 3], [3, 0, 1, 2], [2, 3, 0, 1]])
	y = np.array([[2, 0, 1], [0, 1, 2], [1, 0, 2]])
	b = np.array([3])
	
	gop = []
	for i in range(x.shape[0]-y.shape[0]+1):
	    for j in range(x.shape[1] - y.shape[1]+1):
	        gop.append(np.sum(x[i:y.shape[0]+i,j:y.shape[1]+j]*y))
	
	res = np.array(gop).reshape(i+1,j+1)
	res = res+b
	res_pad = np.pad(res, pad_width = 1, mode = 'constant', constant_values=0)
	print(res_pad)

	[[ 0  0  0  0]
	 [ 0 18 19  0]
	 [ 0  9 18  0]
	 [ 0  0  0  0]]





문제 205. 4x4 행렬에 3x3필터를 적용해서 결과로 4x4행렬이 출력되게 하려면 제로패딩을 몇을 해줘야 하는가 ?
	패딩 공식 : p234
	 P = ( (OH -1)*S - H+FH ) / 2
	입력크기를 (H,W), 필터크기를 (FH, FW), 패딩을 P, 스트라이드 S 라고 한다.
	




문제 206. 위의 패딩 공식을 구현하는 파이썬 함수를 생성하시오 !

	def padding(H,S,OH,FH):
	    return int(( (OH -1)*S - H+FH ) / 2)
	
	print(padding(4,1,4,3))

	1




문제 207. 아래의 결과가 무엇인지 출력하시오 !

                     0 0 0 0 0 0
  1  2  3  0         0 1 2 3 0 0                           
  0  1  2  3    ---> 0 0 1 2 3 0  ◎  2 0 1   =   ?  
  3  0  1  2         0 3 0 1 2 0      0 1 2
  2  3  0  1         0 2 3 0 1 0      1 0 2 
                     0 0 0 0 0 0

    (4 x 4)            (6X6)         (3x3)       ( 4 x 4 ) 

	import numpy as np
	def padding(H,S,OH,FH):
	    return int(( (OH -1)*S - H+FH ) / 2)
	
	x = np.array([[1, 2, 3, 0], [0, 1, 2, 3], [3, 0, 1, 2], [2, 3, 0, 1]])
	y = np.array([[2, 0, 1], [0, 1, 2], [1, 0, 2]])
	b = np.array([3])
	pad = padding(x.shape[0], 1, x.shape[0], y.shape[0])
	
	x_pad = np.pad(x, pad_width = pad, mode = 'constant', constant_values=0)
	
	gop = []
	for i in range(x_pad.shape[0]-y.shape[0]+1):
	    for j in range(x_pad.shape[1] - y.shape[1]+1):
	        gop.append(np.sum(x_pad[i:y.shape[0]+i,j:y.shape[1]+j]*y))
	
	res = np.array(gop).reshape(i+1,j+1)
	res = res+b
	print(res)

	[[10 15 13  5]
	 [ 7 18 19 13]
	 [13  9 18  9]
	 [11 13  7  6]]

























