■ convolution 클래스내에서 일어나는 일

	  1. 원본이미지를 im2col로 2차원 행렬로 변경한다.
	  2. filter 를 reshape 의 -1 옵션을 이용해서  2차원 행렬로 변경한다.
	  3. 두개의 2차원 행렬을 내적한다.
	  4. 내적한 결과 2차원 행렬을 다시 4차원으로 변경한다.

	convolution층의 역할?  이미지의 특징을 추출하는 feature map을 여러개 만든다.




문제 241. 어제 마지막 문제는 4차원 행렬 원본 이미지와 4차원 행렬 필터를 2차원으로 차원 축소하여 
	  내적한 결과이다. 이 2차원 결과를 다시 4차원으로 변경하시오!

	data = np.array(
	       [
	         [[2, 2, 1, 1, 0],
	          [0, 0, 1, 0, 0],
	          [0, 2, 0, 0, 1],
	          [1, 2, 1, 1, 1],    #  --->  Red 
	          [1, 0, 1, 0, 1]],
	         [[2, 0, 0, 0, 1],
	          [0, 2, 2, 0, 1],
	          [0, 0, 0, 0, 2],    # ---->  Green
	          [0, 1, 2, 0, 1],
	          [2, 0, 2, 2, 2]],
	         [[4, 2, 1, 2,2],
	          [0, 1, 0, 4,1],      # ---->  Blue
	          [3, 0, 6, 2,1],
	          [4, 2, 4, 5,4],
	          [0, 1, 2, 0, 1]]
	       ])
	Filter=np.array([[[1,1,-1,-1,0,0,1,1,0],   \
	                  [-1,-1,0,0,-1,1,0,-1,0], \
	                  [-1,1,1,-1,1,-1,0,0,-1]]]).reshape(3,3,3)
	
	data_im = im2col(data.reshape(1, 3, 5, 5), Filter.shape[2], Filter.shape[1], stride=1,pad=1)
	Filter2 = Filter.reshape(3,-1).reshape(-1).reshape(1,27)
	res = np.dot(data_im, Filter2.T).reshape(5,5)
	result =res.reshape(1, 5, 5, -1)
			    ↑ ↑ ↑ ↑
			    N  H  W  C
	print(result.shape)

	(1, 5, 5, 1)


		------------------------------
		-- 아이린 사진 한장 
		import  numpy  as  np
		
		x1 = np.random.rand(1,3,7,7)
				    N C H W
		print (x1)
		print (x1.shape)
		print (temp2)
		-------------------------------





문제 242. 문제 241번의 4차원 결과를 아이린 사진과 같은 4차원 결과로 출력되게 하시오 !

	data = np.array(
	       [
	         [[2, 2, 1, 1, 0],
	          [0, 0, 1, 0, 0],
	          [0, 2, 0, 0, 1],
	          [1, 2, 1, 1, 1],    #  --->  Red 
	          [1, 0, 1, 0, 1]],
	         [[2, 0, 0, 0, 1],
	          [0, 2, 2, 0, 1],
	          [0, 0, 0, 0, 2],    # ---->  Green
	          [0, 1, 2, 0, 1],
	          [2, 0, 2, 2, 2]],
	         [[4, 2, 1, 2,2],
	          [0, 1, 0, 4,1],      # ---->  Blue
	          [3, 0, 6, 2,1],
	          [4, 2, 4, 5,4],
	          [0, 1, 2, 0, 1]]
	       ])
	Filter=np.array([[[1,1,-1,-1,0,0,1,1,0],   \
	                  [-1,-1,0,0,-1,1,0,-1,0], \
	                  [-1,1,1,-1,1,-1,0,0,-1]]]).reshape(3,3,3)
	
	data_im = im2col(data.reshape(1, 3, 5, 5), Filter.shape[2], Filter.shape[1], stride=1,pad=1)
	Filter2 = Filter.reshape(3,-1).reshape(-1).reshape(1,27)
	res = np.dot(data_im, Filter2.T).reshape(5,5)
	result = res.reshape(1, 5, 5, -1)
	result1 = result.transpose(0, 3, 1, 2)
	print(result1)

	[[[[ -1.  -7. -10.  -1.  -3.]
	   [  5.  -3.  -4.   7.  -5.]
	   [  3. -13.  -2.  -1.  -6.]
	   [  2.  -2.   2.  -9.  -6.]
	   [  2.   3.   7.   0.  -1.]]]]




문제 244. 책의 그림중 아래의 그림 7-28의 앞의 convolution층을 통과한 결과의 feature map의 shape를 출력하시오 !

	x = np.arange(154587).reshape(1, 3, 227, 227)
	Filter = np.arange(34848).reshape(96, 3, 11, 11)
	b1 = 1
	conv1 = Convolution(Filter, b1, stride = 4)
	fe_map = conv1.forward(x)
	print('fe_map의 shape', fe_map.shape)

	fe_map의 shape (1, 96, 55, 55)










★ 풀링(pooling) 층의 역할

	convolution층이 이미지의 특징을 잡아내는 역할을 한다면 pooling층은 이미지를 선명하게 만드는 
	역할을 한다.
	"말 그대로 출력값에서 일부분만 취하는 기능"

	convolution층이 이렇게 저렇게 망쳐놓은 그림들을 가지고 feature map이미지의 각 부분에서 대표들을 
	뽑아 사이즈가 작은 이미지로 만드는 것이다.
	마치 사진을 축소하면 해상도가 좋아지는 듯한 효과와 비슷하다.


	* 풀링(pooling)의 종류 3가지
		1. 최대풀링 : 컨볼루션 데이터에서 가장 큰 값을 대표값으로 선정
			그림 7-_-21
		2. 평균풀링 : 컨볼루션 데이터에서 모든값의 평균값을 대표값으로 선정
		3. 확률적 풀링 : 컨볼루션 데이터에서 임의 확률로 한개를 선정

	

문제 245. 파이썬으로 아래의 행렬을 만들고 max pooling을 시도해서 아래의 결과를 출력하시오 !

	 21  8  8 12
	 12 19  9  7
	  8 10  4  3  ───▶  21  12
	 18 12  9 10            18  10
	
	x = np.array([[21, 8, 8, 12], [12, 19, 9, 7], [9, 10, 4, 3], [18, 12, 9, 10]])
		
	def max_pooling(x, stride = 2):
	    res = []
	    for i in range(0, len(x), stride):
	        for j in range(0, len(x[i]), stride):
	            res.append(np.max(x[i:i+stride, j:j+stride]))
	    res = np.array(res).reshape(i,j)
	    return res
	print(max_pooling(x))
	
	[[21 12]
	 [18 10]]





문제 246. 아래의 입력 이미지 행렬 (4x4)를 4차원으로 변경한 후 im2col함수에 입력해서 결과를 
	   출력하시오 !(stride=2, padding=0)

	x = np.array([[[21,8,8,12],[12,19,9,7],[8,10,4,3],[18,12,9,10]],
	              [[19,8,7,12],[1,19,9,7],[4,2,4,3],[4,12,9,10]],
	              [[2,8,8,12],[10,19,9,7],[5,6,4,3],[1,12,9,12]]])
	x2 = x.reshape(1,3,4,4)
	x3 = im2col(x2, 2, 2, stride=2)
	print(x3)

	[[ 21.   8.  12.  19.  19.   8.   1.  19.   2.   8.  10.  19.]
	 [  8.  12.   9.   7.   7.  12.   9.   7.   8.  12.   9.   7.]
	 [  8.  10.  18.  12.   4.   2.   4.  12.   5.   6.   1.  12.]
	 [  4.   3.   9.  10.   4.   3.   9.  10.   4.   3.   9.  12.]]





문제 247. 위의 결과를 4개씩 묶어서 출력하시오 !

	x = np.array([[[21,8,8,12],[12,19,9,7],[8,10,4,3],[18,12,9,10]],
	              [[19,8,7,12],[1,19,9,7],[4,2,4,3],[4,12,9,10]],
	              [[2,8,8,12],[10,19,9,7],[5,6,4,3],[1,12,9,12]]])
	x2 = x.reshape(1,3,4,4)
	x3 = im2col(x2, 2, 2, stride=2)
	x4 = x3.reshape(-1,4)
	print(x4)
	
	[[ 21.   8.  12.  19.]
	 [ 19.   8.   1.  19.]
	 [  2.   8.  10.  19.]
	 [  8.  12.   9.   7.]
	 [  7.  12.   9.   7.]
	 [  8.  12.   9.   7.]
	 [  8.  10.  18.  12.]
	 [  4.   2.   4.  12.]
	 [  5.   6.   1.  12.]
	 [  4.   3.   9.  10.]
	 [  4.   3.   9.  10.]
	 [  4.   3.   9.  12.]]





문제 248. (점심시간 문제) np.max를 이용해서 maxpooling한 결과를 출력하시오 !

	x = np.array([[[21,8,8,12],[12,19,9,7],[8,10,4,3],[18,12,9,10]],
	              [[19,8,7,12],[1,19,9,7],[4,2,4,3],[4,12,9,10]],
	              [[2,8,8,12],[10,19,9,7],[5,6,4,3],[1,12,9,12]]])
	x2 = x.reshape(1,3,4,4)
	x3 = im2col(x2, 2, 2, stride=2)
	x4 = x3.reshape(-1,4)
	x5 = np.max(x4, axis = 1)
	print(x5)

	[ 21.  19.  19.  12.  12.  12.  18.  12.  12.  10.  10.  12.]




문제 249. 칠판의 그림처럼 출력되게 하시오 !

그림 7-_-22
	x = np.array([[[21,8,8,12],[12,19,9,7],[8,10,4,3],[18,12,9,10]],
	              [[19,8,7,12],[1,19,9,7],[4,2,4,3],[4,12,9,10]],
	              [[2,8,8,12],[10,19,9,7],[5,6,4,3],[1,12,9,12]]])
	x2 = x.reshape(1,3,4,4)
	x3 = im2col(x2, 2, 2, stride=2)
	x4 = x3.reshape(-1,4)
	x5 = np.max(x4, axis = 1)
	x6 = x5.reshape(2,2,-1)
	
	print(x6)
	
	[[[ 21.  19.  19.]
	  [ 12.  12.  12.]]
	
	 [[ 18.  12.  12.]
	  [ 10.  10.  12.]]]




문제 250. 위의 결과로 아래의 결과를 출력하시오 !

	 [[[[ 21.  12.]
	    [ 18.  10.]]
	
	   [[ 19.  12.]
	    [ 12.  10.]]
	
	   [[ 19.  12.]
	    [ 12.  12.]]]]

그림 7-_-23

	x = np.array([[[21,8,8,12],[12,19,9,7],[8,10,4,3],[18,12,9,10]],
	              [[19,8,7,12],[1,19,9,7],[4,2,4,3],[4,12,9,10]],
	              [[2,8,8,12],[10,19,9,7],[5,6,4,3],[1,12,9,12]]])
	x2 = x.reshape(1,3,4,4)
	x3 = im2col(x2, 2, 2, stride=2)
	x4 = x3.reshape(-1,4)
	x5 = np.max(x4, axis = 1)
	x6 = x5.reshape(1,2,2,-1).transpose(0, 3, 1, 2)
	
	print(x6)

	[[[[ 21.  12.]
	   [ 18.  10.]]
	
	  [[ 19.  12.]
	   [ 12.  10.]]
	
	  [[ 19.  12.]
	   [ 12.  12.]]]]




문제 251. 위의 일련의 과정을 하나로 묶은것이 책 249페이지의 Pooling클래스입니다.Pooling클래스를
	  생성하세요 !

	class Max_Pooling:
	    def __init__(self, pool_h, pool_w, stride=1, pad=0):
	        self.pool_h = pool_h
	        self.pool_w = pool_w
	        self.stride = stride
	        self.pad = pad
	        
	    def forward(self, x):
	        N, C, H, W = x.shape
	        out_h = int(1 + (H - self.pool_h) / self.stride)
	        out_w = int(1 + (H - self.pool_w) / self.stride)
	        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)
	        col = col.reshape(-1, self.pool_h * self.pool_w)
	        out = np.max(col, axis = 1)
	        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)
	        return out




문제252. 위에서 만든 Pooling 클래스를 객체화 시켜서 아래의 원본 이미지의 maxpooling 한 결과를 출력하시오 !

	x = np.array([[[21,8,8,12],[12,19,9,7],[8,10,4,3],[18,12,9,10]],
	              [[19,8,7,12],[1,19,9,7],[4,2,4,3],[4,12,9,10]],
	              [[2,8,8,12],[10,19,9,7],[5,6,4,3],[1,12,9,12]]])
	
	x2 = x.reshape(1,3,4,4)
	
	pool = Max_Pooling(2, 2, stride = 2)
	print(pool.forward(x2))
	
	[[[[ 21.  12.]
	   [ 18.  10.]]
	
	  [[ 19.  12.]
	   [ 12.  10.]]
	
	  [[ 19.  12.]
	   [ 12.  12.]]]]




문제 253. 위의 코드를 이번에는 최대풀링이 아니라 평균풀링으로 구현하시오 !

	class Mean_Pooling:
	    def __init__(self, pool_h, pool_w, stride=1, pad=0):
	        self.pool_h = pool_h
	        self.pool_w = pool_w
	        self.stride = stride
	        self.pad = pad
	        
	    def forward(self, x):
	        N, C, H, W = x.shape
	        out_h = int(1 + (H - self.pool_h) / self.stride)
	        out_w = int(1 + (H - self.pool_w) / self.stride)
	        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)
	        col = col.reshape(-1, self.pool_h * self.pool_w)
	        out = np.mean(col, axis = 1)
	        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)
	        return out
	    
	x = np.array([[[21,8,8,12],[12,19,9,7],[8,10,4,3],[18,12,9,10]],
	              [[19,8,7,12],[1,19,9,7],[4,2,4,3],[4,12,9,10]],
	              [[2,8,8,12],[10,19,9,7],[5,6,4,3],[1,12,9,12]]])
	
	x2 = x.reshape(1,3,4,4)
	
	pool = Mean_Pooling(2, 2, stride = 2)
	print(pool.forward(x2))
	
	[[[[ 15.     9.  ]
	   [ 12.     6.5 ]]
	
	  [[ 11.75   8.75]
	   [  5.5    6.5 ]]
	
	  [[  9.75   9.  ]
	   [  6.     7.  ]]]]




문제 254. 위의 풀링을 이번에는 확률적 풀링으로 수행하시오 !

	class Statistical_Pooling:
	    def __init__(self, pool_h, pool_w, stride=1, pad=0):
	        self.pool_h = pool_h
	        self.pool_w = pool_w
	        self.stride = stride
	        self.pad = pad
	        
	    def forward(self, x):
	        N, C, H, W = x.shape
	        out_h = int(1 + (H - self.pool_h) / self.stride)
	        out_w = int(1 + (H - self.pool_w) / self.stride)
	        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)
	        col = col.reshape(-1, self.pool_h * self.pool_w)
	        out = np.array([np.random.choice(i) for i in col])
	        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)
	        return out
	    
	x = np.array([[[21,8,8,12],[12,19,9,7],[8,10,4,3],[18,12,9,10]],
	              [[19,8,7,12],[1,19,9,7],[4,2,4,3],[4,12,9,10]],
	              [[2,8,8,12],[10,19,9,7],[5,6,4,3],[1,12,9,12]]])
	
	x2 = x.reshape(1,3,4,4)
	
	pool = Statistical_Pooling(2, 2, stride = 2)
	print(pool.forward(x2))

	[[[[ 19.   8.]
	   [ 10.   4.]]
	
	  [[  8.  12.]
	   [  2.  10.]]
	
	  [[  8.   7.]
	   [  6.   4.]]]]










★ CNN구현하기 (p 250)

그림 7-_-24

그림 7-_-25




문제 255. 아래의 그림을 노트에 구현해 보시오 !
그림 7-_-25

문제 255. 최종 코드
---------------------------------------------------------------------------------------------
# coding: utf-8
import sys, os

sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정
import pickle
import numpy as np
from collections import OrderedDict
from common.layers import *
from common.gradient import numerical_gradient
import matplotlib.pyplot as plt
from dataset.mnist import load_mnist
from common.trainer import Trainer


class SimpleConvNet:
    def __init__(self, input_dim=(1, 28, 28),
                 conv_param1={'filter_num': 32, 'filter_size': 5, 'pad': 2, 'stride': 1},
                 conv_param2={'filter_num': 64, 'filter_size': 5, 'pad': 2, 'stride': 1},
                 output_size=10, weight_init_std=np.sqrt(2/50)):

        self.params = {}

        self.layers = OrderedDict()
        
        filter_num1 = conv_param1['filter_num']
        filter_size1 = conv_param1['filter_size']
        filter_pad1 = conv_param1['pad']
        filter_stride1 = conv_param1['stride']
        input_size1 = input_dim[1]
        conv_output_size1 = (input_size1 - filter_size1 + 2 * filter_pad1) / filter_stride1 + 1
        pool_output_size1 = int(filter_num1 * (conv_output_size1 / 2) * (conv_output_size1 / 2))
        self.params['W1'] = weight_init_std * \
                            np.random.randn(filter_num1, input_dim[0], filter_size1, filter_size1)
        self.params['b1'] = np.zeros(filter_num1)
        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'], conv_param1['stride'], conv_param1['pad'])
        self.layers['Relu1'] = Relu()
        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)

        filter_num2 = conv_param2['filter_num']
        filter_size2 = conv_param2['filter_size']
        filter_pad2 = conv_param2['pad']
        filter_stride2 = conv_param2['stride']
        input_size2 = 14
        conv_output_size2 = (input_size2 - filter_size2 + 2 * filter_pad2) / filter_stride2 + 1
        pool_output_size2 = int(filter_num2 * (conv_output_size2 / 2) * (conv_output_size2 / 2))
        self.params['W2'] = weight_init_std * \
                            np.random.randn(filter_num2, 32, filter_size2, filter_size2)
        self.params['b2'] = np.zeros(filter_num2)
        self.layers['Conv2'] = Convolution(self.params['W2'], self.params['b2'], conv_param2['stride'], conv_param2['pad'])
        self.layers['Relu2'] = Relu()
        self.layers['Pool2'] = Pooling(pool_h=2, pool_w=2, stride=2)
        
        self.params['W3'] = weight_init_std * \
                            np.random.randn(pool_output_size2, output_size)
        self.params['b3'] = np.zeros(output_size)
        self.layers['Affine1'] = Affine(self.params['W3'], self.params['b3'])
        self.last_layer = SoftmaxWithLoss()

    def predict(self, x):
        for layer in self.layers.values():
            x = layer.forward(x)
        return x

    def loss(self, x, t):
        y = self.predict(x)
        return self.last_layer.forward(y, t)

    def accuracy(self, x, t, batch_size=100):
        if t.ndim != 1: t = np.argmax(t, axis=1)
        acc = 0.0
        for i in range(int(x.shape[0] / batch_size)):
            tx = x[i * batch_size:(i + 1) * batch_size]
            tt = t[i * batch_size:(i + 1) * batch_size]
            y = self.predict(tx)
            y = np.argmax(y, axis=1)
            acc += np.sum(y == tt)
        return acc / x.shape[0]

    def numerical_gradient(self, x, t):
        loss_w = lambda w: self.loss(x, t)
        grads = {}
        for idx in (1, 2, 3):
            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])
            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])
        return grads

    def gradient(self, x, t):
        self.loss(x, t)
        dout = 1
        dout = self.last_layer.backward(dout)
        layers = list(self.layers.values())
        layers.reverse()
        for layer in layers:
            dout = layer.backward(dout)
        grads = {}
        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db
        grads['W2'], grads['b2'] = self.layers['Conv2'].dW, self.layers['Conv2'].db
        grads['W3'], grads['b3'] = self.layers['Affine1'].dW, self.layers['Affine1'].db
        return grads

    def save_params(self, file_name="params.pkl"):
        params = {}
        for key, val in self.params.items():
            params[key] = val
        with open(file_name, 'wb') as f:
            pickle.dump(params, f)

    def load_params(self, file_name="params.pkl"):
        with open(file_name, 'rb') as f:
            params = pickle.load(f)
        for key, val in params.items():
            self.params[key] = val
        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):
            self.layers[key].W = self.params['W' + str(i + 1)]
            self.layers[key].b = self.params['b' + str(i + 1)]





(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)
max_epochs = 20
network = SimpleConvNet(input_dim=(1, 28, 28),
                        conv_param1={'filter_num': 32, 'filter_size': 5, 'pad': 2, 'stride': 1},
                        conv_param2={'filter_num': 64, 'filter_size': 5, 'pad': 2, 'stride': 1},
                        output_size=10, weight_init_std=0.01)



network.save_params("params.pkl")
print("Saved Network Parameters!")


iters_num = 10000
train_size = x_train.shape[0]
batch_size = 100
learning_rate = 0.1
train_loss_list = []
train_acc_list = []
test_acc_list = []



iter_per_epoch = max(train_size / batch_size, 1)
print(iter_per_epoch)

for i in range(iters_num):
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]

    grad = network.gradient(x_batch, t_batch)

    for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):
        network.params[key] -= learning_rate * grad[key]

    loss = network.loss(x_batch, t_batch)
    train_loss_list.append(loss)

    if i % iter_per_epoch == 0:
        print(x_train.shape)
        train_acc = network.accuracy(x_train, t_train)
        test_acc = network.accuracy(x_test, t_test)
        train_acc_list.append(train_acc)
        test_acc_list.append(test_acc)
        print("train acc, test acc | " + str(train_acc) + ", " + str(test_acc))

markers = {'train': 'o', 'test': 's'}
x = np.arange(len(train_acc_list))
plt.plot(x, train_acc_list, label='train acc')
plt.plot(x, test_acc_list, label='test acc', linestyle='--')
plt.xlabel("epochs")
plt.ylabel("accuracy")
plt.ylim(0, 1.0)
plt.legend(loc='lower right')
plt.show()
--------------------------------------------------------------------------------------------------


